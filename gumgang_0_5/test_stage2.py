#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê¸ˆê°• 0.5 â†’ 1.0 ë¦¬íŒ©í† ë§: 2ë‹¨ê³„ ê¸°ì–µ-ì¶”ë¡  ê· í˜• í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import json
from datetime import datetime

# ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

def test_hybrid_search():
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    try:
        from app.graph import HybridSearchSystem

        search_system = HybridSearchSystem()

        test_queries = [
            "FastAPIì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "ê¸ˆê°•ì˜ ë©”ëª¨ë¦¬ êµ¬ì¡°ëŠ”?",
            "LangGraph ì‚¬ìš©ë²•",
            "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"
        ]

        for query in test_queries:
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: {query}")
            results, info = search_system.search_memories(query, verbose=True)

            print(f"   ğŸ“Š ê²°ê³¼ ìˆ˜: {len(results)}")
            print(f"   ğŸ¯ í’ˆì§ˆ ì ìˆ˜: {info.get('quality', {}).get('relevance_score', 0):.2f}")
            print(f"   ğŸ”§ ì‚¬ìš© ë°©ë²•: {info.get('methods_used', [])}")

            if results:
                print(f"   ğŸ“ ì²« ë²ˆì§¸ ê²°ê³¼: {results[0][:100]}...")

    except Exception as e:
        print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def test_response_quality():
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š ì‘ë‹µ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    try:
        from app.graph import ResponseQualityEvaluator, _evaluate_response_quality

        evaluator = ResponseQualityEvaluator()

        test_cases = [
            {
                "query": "Pythonì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "memory_results": [
                    "Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤",
                    "Pythonì€ ê°„ë‹¨í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤",
                    "Pythonì€ ë°ì´í„° ê³¼í•™ì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤"
                ]
            },
            {
                "query": "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜",
                "memory_results": [
                    "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë°©ë²•",
                    "ì›¹ ê°œë°œ í”„ë ˆì„ì›Œí¬"
                ]
            },
            {
                "query": "FastAPI ì‚¬ìš©ë²•",
                "memory_results": []
            }
        ]

        for i, case in enumerate(test_cases, 1):
            print(f"\nğŸ§ª ì¼€ì´ìŠ¤ {i}: {case['query']}")

            # ë©”ëª¨ë¦¬ ê´€ë ¨ì„± í‰ê°€
            memory_quality = evaluator.evaluate_memory_relevance(
                case['query'],
                case['memory_results']
            )

            print(f"   ğŸ¯ ë©”ëª¨ë¦¬ ê´€ë ¨ì„±: {memory_quality['relevance_score']:.2f}")
            print(f"   ğŸ“ ì‹ ë¢°ë„: {memory_quality['confidence']:.2f}")
            print(f"   ğŸ’¡ ì´ìœ : {memory_quality['reasons']}")

            # ê°€ìƒ ì‘ë‹µ í’ˆì§ˆ í‰ê°€
            sample_response = f"ì´ê²ƒì€ {case['query']}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤. ìƒì„¸í•œ ì„¤ëª…ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
            response_quality = _evaluate_response_quality(case['query'], sample_response)
            print(f"   ğŸ“Š ì‘ë‹µ í’ˆì§ˆ: {response_quality:.2f}")

    except Exception as e:
        print(f"âŒ ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def test_contextual_generation():
    """ì˜ë„ë³„ ë§ì¶¤í˜• ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ì˜ë„ë³„ ë§ì¶¤í˜• ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    try:
        from app.graph import _generate_contextual_response

        test_intents = [
            {"query": "ì•ˆë…•í•˜ì„¸ìš”!", "intent": "casual", "confidence": 0.9},
            {"query": "ë‚˜ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?", "intent": "identity", "confidence": 0.8},
            {"query": "Pythonì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”", "intent": "knowledge", "confidence": 0.7},
            {"query": "ì½”ë“œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”", "intent": "action", "confidence": 0.6},
            {"query": "ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ”?", "intent": "meta", "confidence": 0.5}
        ]

        for case in test_intents:
            print(f"\nğŸ§ª ì˜ë„: {case['intent']} (ì‹ ë¢°ë„: {case['confidence']:.1f})")
            print(f"   ì§ˆë¬¸: {case['query']}")

            response = _generate_contextual_response(
                case['query'],
                case['intent'],
                case['confidence'],
                verbose=True
            )

            print(f"   ì‘ë‹µ: {response[:200]}...")

    except Exception as e:
        print(f"âŒ ë§ì¶¤í˜• ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def test_integrated_flow():
    """í†µí•© í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    print("\nğŸš€ í†µí•© í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    try:
        from app.graph import route_node, recall_chatgpt_node, no_memory_generate_node

        test_scenarios = [
            {
                "query": "ê¸ˆê°•ì˜ í˜„ì¬ ìƒíƒœëŠ”?",
                "expected_route": "status_report",
                "description": "META ì˜ë„ â†’ ìƒíƒœ ë³´ê³ "
            },
            {
                "query": "Pythonì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "expected_route": "no_memory_generate_node",
                "description": "KNOWLEDGE ì˜ë„ â†’ ë©”ëª¨ë¦¬ ì—†ìŒ ì‹œ GPT ìƒì„±"
            },
            {
                "query": "ì•ˆë…•í•˜ì„¸ìš”!",
                "expected_route": "no_memory_generate_node",
                "description": "CASUAL ì˜ë„ â†’ ì§ì ‘ ìƒì„±"
            }
        ]

        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\nğŸ§ª ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['description']}")
            print(f"   ì§ˆë¬¸: {scenario['query']}")

            # 1ë‹¨ê³„: ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸
            state = {"output": scenario['query'], "memory": None}
            routed_state = route_node(state, verbose=True)

            actual_route = routed_state.get("router_decision")
            print(f"   ğŸ”€ ë¼ìš°íŒ… ê²°ê³¼: {actual_route}")
            print(f"   âœ… ì˜ˆìƒ ì¼ì¹˜: {actual_route == scenario['expected_route']}")

            # 2ë‹¨ê³„: í•´ë‹¹ ë…¸ë“œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
            if actual_route == "recall_chatgpt":
                result_state = recall_chatgpt_node(routed_state, verbose=True)
            elif actual_route == "no_memory_generate_node":
                result_state = no_memory_generate_node(routed_state, verbose=True)
            else:
                result_state = routed_state
                print(f"   â­ï¸  {actual_route} ë…¸ë“œëŠ” ê°œë³„ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ")
                continue

            # ê²°ê³¼ ë¶„ì„
            response_quality = result_state.get("response_quality", {})
            print(f"   ğŸ“Š ì‘ë‹µ í’ˆì§ˆ: {json.dumps(response_quality, indent=6, ensure_ascii=False)}")

            final_response = result_state.get("output") or result_state.get("memory")
            if final_response:
                print(f"   ğŸ’¬ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {str(final_response)[:150]}...")

    except Exception as e:
        print(f"âŒ í†µí•© í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def test_performance_comparison():
    """ê¸°ì¡´ vs ë¦¬íŒ©í† ë§ ì„±ëŠ¥ ë¹„êµ"""
    print("\nâš¡ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    test_queries = [
        "ê¸ˆê°•ì˜ ìƒíƒœê°€ ì–´ë•Œ?",
        "Python í”„ë¡œê·¸ë˜ë° ì„¤ëª…",
        "ë‚˜ëŠ” ëˆ„êµ¬ì•¼?",
        "ì½”ë“œ ë¦¬íŒ©í† ë§ ë°©ë²•",
        "ì•ˆë…•í•˜ì„¸ìš”!"
    ]

    print("ğŸ¯ ë¦¬íŒ©í† ë§ëœ ì‹œìŠ¤í…œ ì„±ëŠ¥:")

    for query in test_queries:
        start_time = datetime.now()

        try:
            from app.graph import route_node
            state = {"output": query, "memory": None}
            result = route_node(state, verbose=False)

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            intent_info = result.get("intent_analysis", {})
            primary_intent = intent_info.get("primary_intent", "unknown")
            confidence = result.get("confidence_score", 0.0)

            print(f"   ğŸ“ '{query}'")
            print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {duration:.3f}ì´ˆ")
            print(f"   ğŸ¯ ì˜ë„: {primary_intent} (ì‹ ë¢°ë„: {confidence:.2f})")
            print(f"   ğŸ”€ ë¼ìš°íŒ…: {result.get('router_decision')}")
            print()

        except Exception as e:
            print(f"   âŒ '{query}' í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def generate_test_report():
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
    print("\nğŸ“‹ 2ë‹¨ê³„ ë¦¬íŒ©í† ë§ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ")
    print("=" * 60)

    report = {
        "test_timestamp": datetime.now().isoformat(),
        "stage": "2ë‹¨ê³„: ê¸°ì–µ-ì¶”ë¡  ê· í˜• ì¡°ì •",
        "improvements": [
            "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ (ë²¡í„° + í‚¤ì›Œë“œ)",
            "ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ",
            "ì˜ë„ë³„ ë§ì¶¤í˜• ì‘ë‹µ ìƒì„±",
            "ì ì‘í˜• GPT ëª¨ë¸ ì„ íƒ",
            "ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ê¸°ë°˜ ì‘ë‹µ ì „ëµ"
        ],
        "test_coverage": [
            "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ëŠ¥",
            "í’ˆì§ˆ í‰ê°€ ì•Œê³ ë¦¬ì¦˜",
            "ì˜ë„ë³„ ì‘ë‹µ ìƒì„±",
            "í†µí•© í”Œë¡œìš°",
            "ì„±ëŠ¥ ì¸¡ì •"
        ]
    }

    print(json.dumps(report, indent=2, ensure_ascii=False))

    return report

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¯ ê¸ˆê°• 2ë‹¨ê³„ ë¦¬íŒ©í† ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_hybrid_search()
    test_response_quality()
    test_contextual_generation()
    test_integrated_flow()
    test_performance_comparison()

    # ìµœì¢… ë³´ê³ ì„œ
    generate_test_report()

    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"â° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
