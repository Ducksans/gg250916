#!/usr/bin/env python3
"""
Semantic Task ID System v1.0
ê·œì¹™ ê¸°ë°˜ ì‘ì—… ë¶„ë¥˜ ë° ìœ„í—˜ë„ í‰ê°€ ì‹œìŠ¤í…œ
LLM ì—†ì´ 100% ë¡œì»¬ ì‹¤í–‰
"""

import json
import re
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import hashlib

@dataclass
class TaskMetadata:
    """ì‘ì—… ë©”íƒ€ë°ì´í„°"""
    id: str
    date: str
    category: str
    subcategory: str
    risk: str
    description: str
    created_at: str
    keywords: List[str]
    estimated_impact: str
    auto_generated: bool

class SemanticTaskID:
    """ì‹œë§¨í‹± ì‘ì—… ID ìƒì„± ë° ê´€ë¦¬"""

    # ì¹´í…Œê³ ë¦¬ ì •ì˜
    CATEGORIES = {
        'UI': 'User Interface / Frontend',
        'BE': 'Backend / Server',
        'AI': 'AI / Machine Learning',
        'GIT': 'Git / Version Control',
        'TERM': 'Terminal / CLI',
        'SEC': 'Security / Safety',
        'BUILD': 'Build / Deploy',
        'TEST': 'Testing / QA',
        'DB': 'Database',
        'API': 'API / Integration',
        'DOC': 'Documentation',
        'CONFIG': 'Configuration',
        'PERF': 'Performance',
        'FIX': 'Bug Fix',
        'EMG': 'Emergency'
    }

    # ìœ„í—˜ë„ ë ˆë²¨
    RISK_LEVELS = {
        'S': {'name': 'Safe', 'color': 'green', 'emoji': 'âœ…'},
        'C': {'name': 'Caution', 'color': 'yellow', 'emoji': 'âš ï¸'},
        'D': {'name': 'Dangerous', 'color': 'red', 'emoji': 'ğŸš¨'}
    }

    def __init__(self, db_path: str = None):
        """
        ì´ˆê¸°í™”

        Args:
            db_path: SQLite DB ê²½ë¡œ
        """
        self.db_path = db_path or Path.home() / '.semantic_task_history.db'
        self._init_database()
        self._load_patterns()

    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS tasks (
                id TEXT PRIMARY KEY,
                date TEXT,
                category TEXT,
                subcategory TEXT,
                risk TEXT,
                description TEXT,
                created_at TEXT,
                keywords TEXT,
                estimated_impact TEXT,
                auto_generated INTEGER,
                metadata TEXT
            )
        ''')
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS patterns (
                pattern TEXT,
                category TEXT,
                subcategory TEXT,
                weight REAL
            )
        ''')
        self.conn.commit()

    def _load_patterns(self):
        """íŒ¨í„´ ê·œì¹™ ë¡œë“œ"""
        self.patterns = {
            'UI': {
                'keywords': ['component', 'ui', 'view', 'page', 'widget', 'button',
                           'form', 'modal', 'dialog', 'layout', 'style', 'css',
                           'tsx', 'jsx', 'react', 'vue', 'angular'],
                'file_patterns': [r'\.tsx?$', r'\.jsx?$', r'\.vue$', r'\.css$', r'\.scss$'],
                'subcategories': {
                    'component': ['component', 'widget', 'element'],
                    'style': ['css', 'style', 'theme', 'design'],
                    'layout': ['layout', 'grid', 'flex', 'position'],
                    'interaction': ['click', 'hover', 'drag', 'drop', 'scroll']
                }
            },
            'BE': {
                'keywords': ['backend', 'server', 'api', 'endpoint', 'route', 'database',
                           'auth', 'middleware', 'controller', 'model', 'service'],
                'file_patterns': [r'\.py$', r'\.rs$', r'\.go$', r'\.java$', r'server\.'],
                'subcategories': {
                    'api': ['api', 'endpoint', 'route', 'rest', 'graphql'],
                    'auth': ['auth', 'login', 'jwt', 'session', 'oauth'],
                    'db': ['database', 'query', 'migration', 'schema'],
                    'service': ['service', 'business', 'logic', 'process']
                }
            },
            'AI': {
                'keywords': ['ai', 'ml', 'model', 'train', 'predict', 'neural', 'llm',
                           'gpt', 'embedding', 'vector', 'assistant', 'prompt'],
                'file_patterns': [r'\.ipynb$', r'model\.', r'train\.', r'predict\.'],
                'subcategories': {
                    'model': ['model', 'network', 'architecture'],
                    'training': ['train', 'fit', 'optimize', 'loss'],
                    'inference': ['predict', 'infer', 'generate', 'complete'],
                    'assistant': ['assistant', 'chat', 'conversation', 'prompt']
                }
            },
            'GIT': {
                'keywords': ['git', 'commit', 'branch', 'merge', 'rebase', 'push', 'pull',
                           'checkout', 'stash', 'tag', 'release'],
                'file_patterns': [r'\.git', r'\.gitignore'],
                'subcategories': {
                    'commit': ['commit', 'add', 'stage'],
                    'branch': ['branch', 'checkout', 'merge'],
                    'remote': ['push', 'pull', 'fetch', 'remote'],
                    'history': ['log', 'diff', 'blame', 'revert']
                }
            },
            'SEC': {
                'keywords': ['security', 'auth', 'encrypt', 'decrypt', 'password', 'token',
                           'permission', 'role', 'access', 'firewall', 'ssl'],
                'file_patterns': [r'auth\.', r'security\.', r'\.pem$', r'\.key$'],
                'subcategories': {
                    'auth': ['auth', 'login', 'logout', 'session'],
                    'crypto': ['encrypt', 'decrypt', 'hash', 'sign'],
                    'access': ['permission', 'role', 'acl', 'rbac'],
                    'network': ['firewall', 'ssl', 'tls', 'https']
                }
            }
        }

        # ìœ„í—˜ë„ í‰ê°€ ê·œì¹™
        self.risk_rules = {
            'D': {  # Dangerous
                'keywords': ['delete', 'drop', 'remove', 'destroy', 'format', 'wipe',
                           'rm -rf', 'sudo', 'admin', 'root', 'password', 'secret',
                           'production', 'live', 'master', 'main'],
                'patterns': [r'DELETE\s+FROM', r'DROP\s+TABLE', r'rm\s+-rf'],
                'file_patterns': [r'\.env$', r'\.key$', r'\.pem$'],
                'categories': ['SEC', 'DB', 'BUILD']
            },
            'C': {  # Caution
                'keywords': ['update', 'modify', 'change', 'alter', 'migrate', 'config',
                           'setting', 'api', 'integration', 'external'],
                'patterns': [r'UPDATE\s+SET', r'ALTER\s+TABLE'],
                'file_patterns': [r'config\.', r'settings\.'],
                'categories': ['API', 'CONFIG', 'BE']
            },
            'S': {  # Safe
                'keywords': ['test', 'doc', 'comment', 'readme', 'style', 'ui', 'view',
                           'component', 'mock', 'example', 'sample'],
                'patterns': [r'test_', r'spec\.', r'\.test\.'],
                'file_patterns': [r'\.md$', r'\.txt$', r'\.css$', r'test\.'],
                'categories': ['UI', 'DOC', 'TEST']
            }
        }

    def generate(self,
                description: str,
                category: str = None,
                subcategory: str = None,
                risk: str = None,
                auto_detect: bool = True) -> str:
        """
        ì‹œë§¨í‹± ì‘ì—… ID ìƒì„±

        Args:
            description: ì‘ì—… ì„¤ëª…
            category: ì¹´í…Œê³ ë¦¬ (ì˜µì…˜)
            subcategory: ì„œë¸Œì¹´í…Œê³ ë¦¬ (ì˜µì…˜)
            risk: ìœ„í—˜ë„ (ì˜µì…˜)
            auto_detect: ìë™ ê°ì§€ í™œì„±í™”

        Returns:
            ìƒì„±ëœ ì‘ì—… ID
        """
        # ë‚ ì§œ
        date = datetime.now().strftime('%Y%m%d')

        # ìë™ ê°ì§€
        if auto_detect:
            if not category:
                category = self._detect_category(description)
            if not subcategory:
                subcategory = self._detect_subcategory(description, category)
            if not risk:
                risk = self._assess_risk(description, category)

        # ê¸°ë³¸ê°’
        category = category or 'GENERAL'
        subcategory = subcategory or 'task'
        risk = risk or 'S'

        # ì„¤ëª… ì •ê·œí™”
        normalized_desc = self._normalize_description(description)

        # ID ìƒì„±
        task_id = f"GG-{date}-{category}.{subcategory}.{normalized_desc}-{risk}"

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = TaskMetadata(
            id=task_id,
            date=date,
            category=category,
            subcategory=subcategory,
            risk=risk,
            description=description,
            created_at=datetime.now().isoformat(),
            keywords=self._extract_keywords(description),
            estimated_impact=self._estimate_impact(risk, category),
            auto_generated=auto_detect
        )

        self._save_task(metadata)

        return task_id

    def _detect_category(self, text: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ìë™ ê°ì§€"""
        text_lower = text.lower()
        scores = {}

        # í‚¤ì›Œë“œ ë§¤ì¹­
        for cat, data in self.patterns.items():
            score = 0
            for keyword in data['keywords']:
                if keyword in text_lower:
                    score += 2

            # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­
            for pattern in data['file_patterns']:
                if re.search(pattern, text, re.IGNORECASE):
                    score += 3

            scores[cat] = score

        # ìµœê³  ì ìˆ˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if scores:
            best_cat = max(scores, key=scores.get)
            if scores[best_cat] > 0:
                return best_cat

        return 'GENERAL'

    def _detect_subcategory(self, text: str, category: str) -> str:
        """ì„œë¸Œì¹´í…Œê³ ë¦¬ ìë™ ê°ì§€"""
        if category not in self.patterns:
            return 'task'

        text_lower = text.lower()
        cat_data = self.patterns[category]

        if 'subcategories' in cat_data:
            for subcat, keywords in cat_data['subcategories'].items():
                for keyword in keywords:
                    if keyword in text_lower:
                        return subcat

        return 'general'

    def _assess_risk(self, text: str, category: str) -> str:
        """ìœ„í—˜ë„ í‰ê°€"""
        text_lower = text.lower()

        # ìœ„í—˜ë„ë³„ ì ìˆ˜ ê³„ì‚°
        scores = {'D': 0, 'C': 0, 'S': 0}

        for risk_level, rules in self.risk_rules.items():
            # í‚¤ì›Œë“œ ì²´í¬
            for keyword in rules['keywords']:
                if keyword.lower() in text_lower:
                    scores[risk_level] += 2

            # íŒ¨í„´ ì²´í¬
            for pattern in rules['patterns']:
                if re.search(pattern, text, re.IGNORECASE):
                    scores[risk_level] += 3

            # ì¹´í…Œê³ ë¦¬ ì²´í¬
            if category in rules['categories']:
                scores[risk_level] += 1

        # ìµœê³  ì ìˆ˜ ìœ„í—˜ë„ ë°˜í™˜
        max_score = max(scores.values())
        if max_score > 0:
            for risk, score in scores.items():
                if score == max_score:
                    return risk

        # ê¸°ë³¸ê°’: Safe
        return 'S'

    def _normalize_description(self, description: str) -> str:
        """ì„¤ëª… ì •ê·œí™”"""
        # ì†Œë¬¸ì ë³€í™˜
        normalized = description.lower()

        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = re.sub(r'[^a-z0-9\s-]', '', normalized)

        # ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ
        normalized = re.sub(r'\s+', '-', normalized)

        # ì¤‘ë³µ í•˜ì´í”ˆ ì œê±°
        normalized = re.sub(r'-+', '-', normalized)

        # ì•ë’¤ í•˜ì´í”ˆ ì œê±°
        normalized = normalized.strip('-')

        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        if len(normalized) > 30:
            normalized = normalized[:30]

        return normalized or 'task'

    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ ìœ„ì£¼)
        words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())

        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'the', 'and', 'for', 'with', 'from', 'this', 'that', 'will', 'can'}
        keywords = [w for w in words if w not in stopwords]

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return sorted(list(set(keywords)))[:10]

    def _estimate_impact(self, risk: str, category: str) -> str:
        """ì˜í–¥ë„ ì¶”ì •"""
        impact_matrix = {
            ('D', 'SEC'): 'CRITICAL',
            ('D', 'DB'): 'CRITICAL',
            ('D', 'BUILD'): 'HIGH',
            ('C', 'API'): 'MEDIUM',
            ('C', 'BE'): 'MEDIUM',
            ('S', 'UI'): 'LOW',
            ('S', 'DOC'): 'MINIMAL'
        }

        return impact_matrix.get((risk, category), 'MEDIUM')

    def _save_task(self, metadata: TaskMetadata):
        """ì‘ì—… ë©”íƒ€ë°ì´í„° ì €ì¥"""
        self.conn.execute('''
            INSERT OR REPLACE INTO tasks VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            metadata.id,
            metadata.date,
            metadata.category,
            metadata.subcategory,
            metadata.risk,
            metadata.description,
            metadata.created_at,
            json.dumps(metadata.keywords),
            metadata.estimated_impact,
            int(metadata.auto_generated),
            json.dumps(asdict(metadata))
        ))
        self.conn.commit()

    def parse(self, task_id: str) -> Optional[TaskMetadata]:
        """ì‘ì—… ID íŒŒì‹±"""
        pattern = r'GG-(\d{8})-([A-Z]+)\.([a-z]+)\.([a-z-]+)-([SCD])'
        match = re.match(pattern, task_id)

        if match:
            date, category, subcategory, description, risk = match.groups()

            # DBì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            cursor = self.conn.execute(
                'SELECT metadata FROM tasks WHERE id = ?', (task_id,)
            )
            row = cursor.fetchone()

            if row:
                return TaskMetadata(**json.loads(row[0]))
            else:
                # DBì— ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ìƒì„±
                return TaskMetadata(
                    id=task_id,
                    date=date,
                    category=category,
                    subcategory=subcategory,
                    risk=risk,
                    description=description.replace('-', ' '),
                    created_at=datetime.now().isoformat(),
                    keywords=[],
                    estimated_impact=self._estimate_impact(risk, category),
                    auto_generated=False
                )
        return None

    def search(self,
              query: str = None,
              category: str = None,
              risk: str = None,
              date_from: str = None,
              date_to: str = None,
              limit: int = 50) -> List[TaskMetadata]:
        """ì‘ì—… ê²€ìƒ‰"""
        sql = 'SELECT metadata FROM tasks WHERE 1=1'
        params = []

        if query:
            sql += ' AND (description LIKE ? OR keywords LIKE ?)'
            params.extend([f'%{query}%', f'%{query}%'])

        if category:
            sql += ' AND category = ?'
            params.append(category)

        if risk:
            sql += ' AND risk = ?'
            params.append(risk)

        if date_from:
            sql += ' AND date >= ?'
            params.append(date_from)

        if date_to:
            sql += ' AND date <= ?'
            params.append(date_to)

        sql += ' ORDER BY created_at DESC LIMIT ?'
        params.append(limit)

        cursor = self.conn.execute(sql, params)

        results = []
        for row in cursor:
            results.append(TaskMetadata(**json.loads(row[0])))

        return results

    def get_statistics(self) -> Dict:
        """í†µê³„ ì •ë³´"""
        stats = {
            'total_tasks': 0,
            'by_category': {},
            'by_risk': {},
            'by_date': {},
            'recent_tasks': []
        }

        # ì „ì²´ ì‘ì—… ìˆ˜
        cursor = self.conn.execute('SELECT COUNT(*) FROM tasks')
        stats['total_tasks'] = cursor.fetchone()[0]

        # ì¹´í…Œê³ ë¦¬ë³„
        cursor = self.conn.execute(
            'SELECT category, COUNT(*) FROM tasks GROUP BY category'
        )
        stats['by_category'] = dict(cursor.fetchall())

        # ìœ„í—˜ë„ë³„
        cursor = self.conn.execute(
            'SELECT risk, COUNT(*) FROM tasks GROUP BY risk'
        )
        stats['by_risk'] = dict(cursor.fetchall())

        # ë‚ ì§œë³„
        cursor = self.conn.execute(
            'SELECT date, COUNT(*) FROM tasks GROUP BY date ORDER BY date DESC LIMIT 30'
        )
        stats['by_date'] = dict(cursor.fetchall())

        # ìµœê·¼ ì‘ì—…
        stats['recent_tasks'] = self.search(limit=10)

        return stats

    def suggest_next_task(self, current_task_id: str) -> Optional[str]:
        """ë‹¤ìŒ ì‘ì—… ì œì•ˆ"""
        current = self.parse(current_task_id)
        if not current:
            return None

        # ê´€ë ¨ ì‘ì—… ë§¤í•‘
        next_task_map = {
            ('UI', 'component'): ('TEST', 'unit', 'ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸'),
            ('BE', 'api'): ('DOC', 'api', 'API ë¬¸ì„œí™”'),
            ('AI', 'model'): ('TEST', 'performance', 'ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸'),
            ('GIT', 'commit'): ('GIT', 'push', 'ì›ê²© ì €ì¥ì†Œ í‘¸ì‹œ'),
            ('SEC', 'auth'): ('TEST', 'security', 'ë³´ì•ˆ í…ŒìŠ¤íŠ¸'),
            ('BUILD', 'compile'): ('BUILD', 'deploy', 'ë°°í¬ ì¤€ë¹„')
        }

        key = (current.category, current.subcategory)
        if key in next_task_map:
            next_cat, next_sub, next_desc = next_task_map[key]
            return self.generate(
                description=next_desc,
                category=next_cat,
                subcategory=next_sub,
                risk='S',
                auto_detect=False
            )

        return None


def main():
    """CLI ì¸í„°í˜ì´ìŠ¤"""
    import argparse

    parser = argparse.ArgumentParser(description='Semantic Task ID System')
    parser.add_argument('--generate', type=str, help='ì‘ì—… ID ìƒì„±')
    parser.add_argument('--category', type=str, help='ì¹´í…Œê³ ë¦¬ ì§€ì •')
    parser.add_argument('--risk', type=str, help='ìœ„í—˜ë„ ì§€ì • (S/C/D)')
    parser.add_argument('--parse', type=str, help='ì‘ì—… ID íŒŒì‹±')
    parser.add_argument('--search', type=str, help='ì‘ì—… ê²€ìƒ‰')
    parser.add_argument('--stats', action='store_true', help='í†µê³„ ë³´ê¸°')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    parser.add_argument('--suggest', type=str, help='ë‹¤ìŒ ì‘ì—… ì œì•ˆ')

    args = parser.parse_args()

    # Semantic Task ID ì¸ìŠ¤í„´ìŠ¤
    stid = SemanticTaskID()

    if args.generate:
        task_id = stid.generate(
            description=args.generate,
            category=args.category,
            risk=args.risk
        )
        print(f"âœ… ìƒì„±ëœ ì‘ì—… ID: {task_id}")

        # ë©”íƒ€ë°ì´í„° ì¶œë ¥
        metadata = stid.parse(task_id)
        if metadata:
            print(f"  ğŸ“ ì¹´í…Œê³ ë¦¬: {metadata.category}.{metadata.subcategory}")
            print(f"  âš¡ ìœ„í—˜ë„: {stid.RISK_LEVELS[metadata.risk]['emoji']} {metadata.risk}")
            print(f"  ğŸ“Š ì˜í–¥ë„: {metadata.estimated_impact}")
            print(f"  ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(metadata.keywords)}")

    elif args.parse:
        metadata = stid.parse(args.parse)
        if metadata:
            print(json.dumps(asdict(metadata), indent=2, ensure_ascii=False))
        else:
            print(f"âŒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì‘ì—… ID: {args.parse}")

    elif args.search:
        results = stid.search(query=args.search)
        print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
        for task in results:
            risk_emoji = stid.RISK_LEVELS[task.risk]['emoji']
            print(f"  {risk_emoji} {task.id}")
            print(f"     {task.description}")

    elif args.stats:
        stats = stid.get_statistics()
        print("\nğŸ“Š ì‘ì—… í†µê³„:")
        print(f"  ì´ ì‘ì—… ìˆ˜: {stats['total_tasks']}")
        print("\n  ì¹´í…Œê³ ë¦¬ë³„:")
        for cat, count in stats['by_category'].items():
            print(f"    {cat}: {count}ê°œ")
        print("\n  ìœ„í—˜ë„ë³„:")
        for risk, count in stats['by_risk'].items():
            emoji = stid.RISK_LEVELS.get(risk, {}).get('emoji', 'â“')
            print(f"    {emoji} {risk}: {count}ê°œ")

    elif args.test:
        print("ğŸ§ª Semantic Task ID í…ŒìŠ¤íŠ¸\n")

        test_cases = [
            "AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ UI êµ¬í˜„",
            "ë°±ì—”ë“œ API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •",
            "ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì‚­ì œ",
            "Git ë¸Œëœì¹˜ ë³‘í•©",
            "ë³´ì•ˆ ì¸ì¦ ì‹œìŠ¤í…œ êµ¬í˜„",
            "í”„ë¡ íŠ¸ì—”ë“œ ì»´í¬ë„ŒíŠ¸ ìŠ¤íƒ€ì¼ ìˆ˜ì •",
            "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±",
            "í”„ë¡œë•ì…˜ ì„œë²„ ë°°í¬"
        ]

        for desc in test_cases:
            task_id = stid.generate(desc)
            metadata = stid.parse(task_id)
            risk_emoji = stid.RISK_LEVELS[metadata.risk]['emoji']
            print(f"{risk_emoji} {task_id}")
            print(f"   â†’ {desc}\n")

    elif args.suggest:
        next_task = stid.suggest_next_task(args.suggest)
        if next_task:
            print(f"ğŸ’¡ ë‹¤ìŒ ì¶”ì²œ ì‘ì—…: {next_task}")
        else:
            print("â„¹ï¸ ì¶”ì²œí•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
