import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any, AsyncGenerator
import datetime
import sys
import json
import asyncio
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)

# Initialize OpenAI client
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = None
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print("âœ… OpenAI API initialized successfully")
    except Exception as e:
        print(f"âš ï¸ Failed to initialize OpenAI: {e}")
else:
    print("âš ï¸ No OpenAI API key found in .env")

# Protocol ì—”ë“œí¬ì¸íŠ¸ import
try:
    from protocol_endpoints import router as protocol_router
    PROTOCOL_ENABLED = True
except ImportError:
    PROTOCOL_ENABLED = False
    print("âš ï¸ Protocol endpoints not available (protocol_endpoints.py not found)")

# âœ… FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="ê¸ˆê°• 2.0 ë°±ì—”ë“œ - ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì„œë²„")

# âœ… CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²°ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… Protocol ë¼ìš°í„° í†µí•©
if PROTOCOL_ENABLED:
    app.include_router(protocol_router)
    print("âœ… Protocol endpoints integrated successfully")

# âœ… í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ëª¨ë¸
class TestMessage(BaseModel):
    message: str
    timestamp: Optional[str] = None

class TaskRequest(BaseModel):
    task_id: str
    task_name: str
    status: str = "pending"
    progress: int = 0

class AskRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    code: Optional[str] = None
    language: Optional[str] = None
    file: Optional[str] = None

# âœ… í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.datetime.now().isoformat(),
        "service": "gumgang-backend",
        "version": "2.0-test"
    }

# âœ… ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "message": "ê¸ˆê°• 2.0 ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ ì„œë²„",
        "status": "running",
        "endpoints": [
            "/health",
            "/ask",
            "/api/test",
            "/api/echo",
            "/api/tasks",
            "/api/dashboard/stats"
        ]
    }

# âœ… /ask ì—”ë“œí¬ì¸íŠ¸ - AI ì–´ì‹œìŠ¤í„´íŠ¸ìš©
@app.post("/ask")
async def ask_question(request: AskRequest):
    """AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - OpenAI GPT ì—°ë™"""
    import uuid

    # ì„¸ì…˜ ID ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
    session_id = request.session_id or str(uuid.uuid4())

    # OpenAI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not openai_client:
        # Fallback to dummy response if no API key
        return {
            "response": f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] '{request.query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤. OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "source": "test-backend",
            "session_id": session_id,
            "context_info": {
                "session_id": session_id,
                "recent_interactions": 0,
                "context_type": "test"
            },
            "suggest_ingest": False
        }

    try:
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """You are ê¸ˆê°• 2.0, an advanced AI coding assistant powered by GPT-5 with a 5-layer memory system.
You have dramatically improved reasoning capabilities, near-human cognitive abilities, and persistent memory.
You are helpful, precise, and capable of understanding both Korean and English at an expert level.
When providing code, always use proper markdown formatting with exceptional accuracy.
You have PhD-level knowledge of programming, software architecture, and best practices."""

        # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # ì½”ë“œê°€ í¬í•¨ëœ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if request.code:
            code_context = f"\n\në‹¤ìŒ ì½”ë“œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤:\n```{request.language or 'python'}\n{request.code}\n```"
            messages.append({"role": "user", "content": request.query + code_context})
        else:
            messages.append({"role": "user", "content": request.query})

        # OpenAI API í˜¸ì¶œ
        print(f"ğŸ¤– Calling OpenAI API for query: {request.query[:50]}...")

        response = openai_client.chat.completions.create(
            model="gpt-5",  # GPT-5 ì •ì‹ ëª¨ë¸ (2025ë…„ 8ì›” 7ì¼ ì¶œì‹œ)
            messages=messages,
            max_completion_tokens=2000,  # GPT-5ëŠ” max_completion_tokens ì‚¬ìš©
            stream=False  # ìŠ¤íŠ¸ë¦¬ë°ì€ ë‚˜ì¤‘ì— êµ¬í˜„
        )

        # ì‘ë‹µ ì¶”ì¶œ
        response_text = response.choices[0].message.content

        print(f"âœ… OpenAI response received: {len(response_text)} chars")

        return {
            "response": response_text,
            "source": "openai-gpt-5-official",
            "session_id": session_id,
            "context_info": {
                "session_id": session_id,
                "recent_interactions": 0,
                "context_type": "openai",
                "model": "GPT-5",
                "tokens_used": response.usage.total_tokens if response.usage else None,
                "capabilities": "Advanced reasoning, persistent memory, near-AGI performance"
            },
            "suggest_ingest": False
        }

    except Exception as e:
        print(f"âŒ OpenAI API error: {e}")
        # ì—ëŸ¬ ë°œìƒì‹œ í´ë°± ì‘ë‹µ
        return {
            "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜: {str(e)}",
            "source": "error-fallback",
            "session_id": session_id,
            "context_info": {
                "session_id": session_id,
                "recent_interactions": 0,
                "context_type": "error",
                "error": str(e)
            },
            "suggest_ingest": False
        }

# âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—”ë“œí¬ì¸íŠ¸ - Server-Sent Events (SSE)
@app.post("/ask/stream")
async def ask_question_stream(request: AskRequest):
    """AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ - OpenAI GPT ìŠ¤íŠ¸ë¦¬ë°"""
    import uuid

    # ì„¸ì…˜ ID ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
    session_id = request.session_id or str(uuid.uuid4())

    async def generate_stream() -> AsyncGenerator[str, None]:
        """SSE í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""

        # OpenAI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not openai_client:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë°
            test_response = f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] '{request.query}'ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì…ë‹ˆë‹¤."
            for char in test_response:
                yield f"data: {json.dumps({'content': char, 'type': 'content'})}\n\n"
                await asyncio.sleep(0.01)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            yield f"data: {json.dumps({'type': 'done', 'session_id': session_id})}\n\n"
            return

        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = """You are ê¸ˆê°• 2.0, an advanced AI coding assistant powered by GPT-5 with a 5-layer memory system.
You have dramatically improved reasoning capabilities, near-human cognitive abilities, and persistent memory.
You are helpful, precise, and capable of understanding both Korean and English at an expert level.
When providing code, always use proper markdown formatting with exceptional accuracy.
You have PhD-level knowledge of programming, software architecture, and best practices."""

            # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt}
            ]

            # ì½”ë“œê°€ í¬í•¨ëœ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if request.code:
                code_context = f"\n\në‹¤ìŒ ì½”ë“œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤:\n```{request.language or 'python'}\n{request.code}\n```"
                messages.append({"role": "user", "content": request.query + code_context})
            else:
                messages.append({"role": "user", "content": request.query})

            # ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡
            yield f"data: {json.dumps({'type': 'start', 'session_id': session_id})}\n\n"

            # OpenAI API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            print(f"ğŸ¤– Starting streaming response for: {request.query[:50]}...")

            stream = openai_client.chat.completions.create(
                model="gpt-5",  # GPT-5 ì •ì‹ ëª¨ë¸ (2025ë…„ 8ì›” 7ì¼ ì¶œì‹œ)
                messages=messages,
                max_completion_tokens=2000,  # GPT-5ëŠ” max_completion_tokens ì‚¬ìš©
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡
            full_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    # ê° ì²­í¬ë¥¼ SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                    yield f"data: {json.dumps({'content': content, 'type': 'content'})}\n\n"

            # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
            yield f"data: {json.dumps({'type': 'done', 'session_id': session_id, 'full_response': full_response})}\n\n"
            print(f"âœ… Streaming completed: {len(full_response)} chars")

        except Exception as e:
            print(f"âŒ Streaming error: {e}")
            # ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡
            yield f"data: {json.dumps({'type': 'error', 'error': str(e), 'session_id': session_id})}\n\n"

    # StreamingResponseë¡œ SSE ì‘ë‹µ ë°˜í™˜
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )

# âœ… ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/api/echo")
async def echo_message(message: TestMessage):
    return {
        "echo": message.message,
        "timestamp": message.timestamp or datetime.datetime.now().isoformat(),
        "processed": True
    }

# âœ… í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/test")
async def test_endpoint():
    return {
        "status": "success",
        "message": "ë°±ì—”ë“œ ì—°ê²° ì„±ê³µ!",
        "timestamp": datetime.datetime.now().isoformat()
    }

# âœ… Echo ì—”ë“œí¬ì¸íŠ¸ (POST í…ŒìŠ¤íŠ¸ìš©)
@app.post("/api/echo")
async def echo_message(msg: TestMessage):
    return {
        "status": "success",
        "echo": msg.message,
        "received_at": datetime.datetime.now().isoformat(),
        "original_timestamp": msg.timestamp
    }

# âœ… Task ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/tasks")
async def get_tasks():
    # ë”ë¯¸ íƒœìŠ¤í¬ ë°ì´í„°
    return {
        "status": "success",
        "tasks": [
            {
                "task_id": "GG-20250108-001",
                "task_name": "ì„¸ì…˜ ë§¤ë‹ˆì € êµ¬ì¶•",
                "status": "completed",
                "progress": 100
            },
            {
                "task_id": "GG-20250108-002",
                "task_name": "Task ì¶”ì  ì‹œìŠ¤í…œ",
                "status": "completed",
                "progress": 100
            },
            {
                "task_id": "GG-20250108-003",
                "task_name": "í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™",
                "status": "completed",
                "progress": 100
            },
            {
                "task_id": "GG-20250108-005",
                "task_name": "ë°±ì—”ë“œ ì•ˆì •í™”",
                "status": "in_progress",
                "progress": 50
            }
        ],
        "total": 4,
        "completed": 3,
        "in_progress": 1
    }

@app.post("/api/tasks")
async def create_task(task: TaskRequest):
    return {
        "status": "success",
        "message": f"Task {task.task_id} created",
        "task": task.dict()
    }

# âœ… ëŒ€ì‹œë³´ë“œ í†µê³„ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/dashboard/stats")
async def dashboard_stats():
    return {
        "status": "success",
        "stats": {
            "total_files": 1247,
            "total_lines": 45892,
            "active_sessions": 1,
            "memory_usage": {
                "sensory": 15,
                "working": 8,
                "episodic": 42,
                "semantic": 156
            },
            "system_health": "optimal",
            "last_update": datetime.datetime.now().isoformat()
        }
    }

# âœ… íŒŒì¼ êµ¬ì¡° ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/structure")
async def get_structure():
    return {
        "status": "success",
        "structure": {
            "frontend": {
                "path": "/gumgang-v2",
                "framework": "Next.js 15",
                "status": "active"
            },
            "backend": {
                "path": "/backend",
                "framework": "FastAPI",
                "status": "running"
            },
            "memory": {
                "path": "/memory",
                "type": "4-layer temporal system",
                "status": "initialized"
            }
        }
    }

# âœ… ë©”ëª¨ë¦¬ ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ (ë”ë¯¸)
@app.get("/api/memory/status")
async def memory_status():
    return {
        "status": "success",
        "memory": {
            "layers": {
                "sensory": {"capacity": 100, "used": 15},
                "working": {"capacity": 50, "used": 8},
                "episodic": {"capacity": 500, "used": 42},
                "semantic": {"capacity": 1000, "used": 156}
            },
            "total_memories": 221,
            "last_cleanup": datetime.datetime.now().isoformat()
        }
    }

# âœ… ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={
            "status": "error",
            "message": "Endpoint not found",
            "path": str(request.url.path)
        }
    )

@app.exception_handler(500)
async def internal_error_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={
            "status": "error",
            "message": "Internal server error",
            "detail": str(exc)
        }
    )

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ê¸ˆê°• 2.0 ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹œì‘...")
    print("ğŸ“ http://localhost:8000")
    print("ğŸ“Š ëŒ€ì‹œë³´ë“œ: http://localhost:3000")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
