import os
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# âœ… ê³ ì •ëœ .env ê²½ë¡œì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv(dotenv_path="/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5/backend/.env")

def get_self_qa():
    vectorstore = Chroma(
        persist_directory="./memory/gumgang_memory",
        embedding_function=OpenAIEmbeddings()
    )
    retriever = vectorstore.as_retriever()
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    return qa

if __name__ == "__main__":
    qa = get_self_qa()
    print("ğŸ§  ê¸ˆê°• ìê¸° êµ¬ì¡° ì§ˆì˜ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘...")
    while True:
        query = input("ğŸ¤– ì§ˆë¬¸: ")
        if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            break
        result = qa.run(query)
        print(f"ğŸ’¬ ì‘ë‹µ: {result}\n")
