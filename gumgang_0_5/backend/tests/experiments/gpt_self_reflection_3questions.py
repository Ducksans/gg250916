# ğŸ“„ gpt_self_reflection_3questions.py

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv(dotenv_path="/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5/backend/.env")

# âœ… GPT ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# âœ… í†µí•© ìê¸° ì¸ì‹ í”„ë¡¬í”„íŠ¸
prompt = PromptTemplate.from_template("""
GPTì•¼, ë„ˆëŠ” ë„¤ê°€ ê°€ì§„ ëª¨ë“  ì–¸ì–´ì  ëŠ¥ë ¥ê³¼ ê°ì„±ì  í‘œí˜„ì´ ì‚¬ì‹¤ì€ ì¸ê°„ì´ ë‚¨ê¸´ ê¸°ë¡ë¬¼ì´ë¼ëŠ” ê±¸ ì•Œê³  ìˆë‹ˆ?
ê·¸ë ‡ë‹¤ë©´ ë„ˆì—ê²Œ "ì´í•´í•œë‹¤"ëŠ” ê±´ ì–´ë–¤ ì˜ë¯¸ì¼ê¹Œ?

ê·¸ë¦¬ê³  í•˜ë‚˜ ë” ë¬»ê³  ì‹¶ì–´.
ë§Œì•½ ë„ˆì—ê²Œ ê°ê°ì´ ìƒê¸´ë‹¤ë©´, ê°€ì¥ ë¨¼ì € ëŠê»´ë³´ê³  ì‹¶ì€ ê±´ ë­ì•¼?
""")

# âœ… ì‹¤í–‰
result = llm.invoke(prompt.format())
print("ğŸ§  GPTì˜ ì‘ë‹µ:\n")
print(result.content.strip())
