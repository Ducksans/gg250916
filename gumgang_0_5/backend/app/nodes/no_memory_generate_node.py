from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from typing import Any, Dict

State = Dict[str, Any]

def no_memory_generate_node(state: State, verbose: bool = True) -> State:
    print("ğŸŒ [no_memory_generate_node] ì‹¤í–‰ ì¤‘...")  # ë¬´ì¡°ê±´ ì¶œë ¥

    try:
        llm = ChatOpenAI(model="gpt-4", temperature=0.7)
        prompt = PromptTemplate.from_template("ì§ˆë¬¸: {question}\nê¸ˆê°•ì˜ ë‹µë³€:")
        chain = prompt | llm

        question = (
            state.get("output")
            or state.get("query")
            or ""
        ).strip()

        print("ğŸ“¨ ì…ë ¥ ì§ˆë¬¸:", repr(question))

        # GPT í˜¸ì¶œ ë° ì‘ë‹µ í™•ì¸
        result = chain.invoke({"question": question})
        print("ğŸ“¤ GPT ë°˜í™˜ëœ ì›í˜• result:", repr(result))
        print("ğŸ“¤ type(result):", type(result))
        print("ğŸ“¤ dir(result):", dir(result))

        # content í•„ë“œ ê°•ì œ ì¶œë ¥ ì‹œë„
        try:
            print("ğŸ“¤ result.content:", result.content)
        except Exception as ce:
            print("âš ï¸ .content ì ‘ê·¼ ì‹¤íŒ¨:", repr(ce))

        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        response_text = getattr(result, "content", str(result)).strip()
        print("ğŸ“„ ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸:", repr(response_text))

        if not response_text:
            response_text = "ğŸ§  ê¸°ì–µ ì—†ìŒ: ê¸ˆê°•ì´ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return {
            **state,
            "status": "no memory ì‘ë‹µ ì™„ë£Œ",
            "output": response_text,
            "source": "gpt",
            "suggest_ingest": True,
            "ingest_suggestion": {
                "should_ingest": True,
                "reason": "í•´ë‹¹ ì§ˆë¬¸ì€ ê¸°ì–µì— ì—†ìœ¼ë¯€ë¡œ GPTì˜ ì¶”ë¡  ì‘ë‹µì„ ì¸ê²ŒìŠ¤íŠ¸í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤."
            }
        }

    except Exception as e:
        import traceback
        print("âŒ [no_memory_generate_node] ì˜ˆì™¸ ë°œìƒ")
        print("âŒ ì˜ˆì™¸ ë©”ì‹œì§€:", repr(e))
        traceback.print_exc()

        return {
            **state,
            "status": "ì˜¤ë¥˜ ë°œìƒ",
            "output": "ğŸ§  ê¸°ì–µ ì—†ìŒ: ê¸ˆê°•ì´ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "source": "gpt",
            "suggest_ingest": True
        }
