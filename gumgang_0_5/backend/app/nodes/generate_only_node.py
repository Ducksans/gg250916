# âœ… generate_only_node.py
# ìˆœìˆ˜ GPT ì‘ë‹µë§Œ ë°˜í™˜í•˜ëŠ” LangGraph ë…¸ë“œ

from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, HumanMessage

# ê¸°ë³¸ LLM ì„¸íŒ… (ì˜µì…˜: temperature ë‚®ê²Œ ì¡°ì • ê°€ëŠ¥)
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# ì…ë ¥ì€ dict: {"query": "..."}
def generate_only_fn(inputs: dict) -> dict:
    query = inputs.get("query", "")
    if not query.strip():
        return {"response": "âŒ ë¹ˆ ì…ë ¥ì´ì—ìš”. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}

    response = llm.invoke([HumanMessage(content=query)])
    return {"response": response.content}

# LangGraphì—ì„œ ì‚¬ìš©í•  ë…¸ë“œ
generate_only_node = RunnableLambda(generate_only_fn)

# ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
# from generate_only_node import generate_only_node
# output = generate_only_node.invoke({"query": "ê¸ˆê°•ì•„, ë„ˆ ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?"})
# print(output["response"])
