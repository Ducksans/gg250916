from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional, Dict, List, Tuple
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.retrievers.ensemble import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os
import re
import json
import uuid
from enum import Enum
from datetime import datetime

# âœ… ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ (4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•©)
from app.context_manager import (
    ConversationTurn,
    get_conversation_memory,
    get_session_manager,
    get_response_enhancer
)

# âœ… 4ê³„ì¸µ ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
from app.core.memory.temporal import (
    get_temporal_memory_system,
    MemoryType,
    MemoryPriority
)

# âœ… ë©”íƒ€ ì¸ì§€ ì‹œìŠ¤í…œ (Claude 4.1 Think Engine Enhanced)
from app.core.cognition.meta import (
    get_metacognitive_system,
    CognitiveState,
    MetaCognitiveInsight
)

# âœ… ì™¸ë¶€ ë…¸ë“œ
from app.nodes.identity_reflect_node import identity_reflect_node
from app.nodes.status_report import status_report as status_report_node
from app.nodes.status_formatter import format_for_chat
from app.nodes.generate_only_node import generate_only_node
from app.nodes.reflect_router_node import build_reflect_router_node

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env"))

# âœ… ìƒíƒœ ì •ì˜
class IntentType(Enum):
    IDENTITY = "identity"      # ìê¸° ì •ì²´ì„± ê´€ë ¨
    META = "meta"             # ì‹œìŠ¤í…œ ìƒíƒœ/êµ¬ì¡° ê´€ë ¨
    KNOWLEDGE = "knowledge"   # ì§€ì‹ ê²€ìƒ‰ ê´€ë ¨
    ACTION = "action"         # ì‹¤í–‰/ì‘ì—… ê´€ë ¨
    CASUAL = "casual"         # ì¼ìƒ ëŒ€í™” ê´€ë ¨

class State(TypedDict):
    status: Optional[str]
    output: Optional[str]
    memory: Optional[str]
    router_decision: Optional[str]
    source: Optional[str]
    suggest_ingest: Optional[bool]
    intent_analysis: Optional[Dict]
    confidence_score: Optional[float]
    search_results: Optional[List[Dict]]
    response_quality: Optional[Dict]
    context_history: Optional[List[str]]
    session_id: Optional[str]
    conversation_context: Optional[Dict]
    enhanced_prompt: Optional[str]
    temporal_memory_context: Optional[List[Dict]]
    memory_enhanced_response: Optional[str]
    metacognitive_analysis: Optional[Dict]  # ë©”íƒ€ ì¸ì§€ ë¶„ì„ ê²°ê³¼
    cognitive_state: Optional[Dict]  # í˜„ì¬ ì¸ì§€ ìƒíƒœ
    reasoning_chain: Optional[List[Dict]]  # ì¶”ë¡  ì²´ì¸

# âœ… ì´ˆê¸° ì…ë ¥ ì €ì¥ ë…¸ë“œ (4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•©)
def reflect_node(state: State, verbose: bool = False) -> State:
    if verbose:
        print("ğŸ“¡ [reflect_node] 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œê³¼ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        print(f"ğŸ” ì…ë ¥: {state.get('output', '')}")

    # ì„¸ì…˜ ID ìƒì„± ë˜ëŠ” ê¸°ì¡´ ì„¸ì…˜ ì‚¬ìš©
    session_manager = get_session_manager()
    session_id = state.get("session_id")

    if not session_id:
        session_id = session_manager.create_session()
        if verbose:
            print(f"ğŸ†• ìƒˆ ì„¸ì…˜ ìƒì„±: {session_id}")
    else:
        if verbose:
            print(f"ğŸ”„ ê¸°ì¡´ ì„¸ì…˜ ì‚¬ìš©: {session_id}")

    # 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    temporal_memory = get_temporal_memory_system()

    # ë©”ëª¨ë¦¬ í†µê³„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    if verbose:
        memory_stats = temporal_memory.get_memory_stats()
        print(f"ğŸ§  ë©”ëª¨ë¦¬ ìƒíƒœ: {memory_stats['layers']}")

    return {
        **state,
        "status": "reflect ì™„ë£Œ (4ê³„ì¸µ ë©”ëª¨ë¦¬ í™œì„±í™”)",
        "output": state.get("output", ""),
        "memory": None,
        "session_id": session_id,
        "temporal_memory_context": [],
        "metacognitive_analysis": None,
        "cognitive_state": None,
        "reasoning_chain": []
    }

# âœ… ì˜ë„ ë¶„ì„ê¸° í´ë˜ìŠ¤
class IntentAnalyzer:
    def __init__(self):
        self.patterns = {
            IntentType.IDENTITY: [
                r'(ë‚˜ëŠ”|ë‚´ê°€|ê¸ˆê°•.*ëˆ„êµ¬|ì •ì²´ì„±|ìê¸°.*ì†Œê°œ)',
                r'(ë‚´.*ì—­í• |ë¬´ì—‡.*í•˜ëŠ”|ì–´ë–¤.*ì¡´ì¬)',
                r'(ìê¸°.*ì„¤ëª…|ìŠ¤ìŠ¤ë¡œ.*ì„¤ëª…)'
            ],
            IntentType.META: [
                r'(ìƒíƒœ|êµ¬ì¡°|ì‹œìŠ¤í…œ|ì•„í‚¤í…ì²˜|í˜„ì¬.*ìƒí™©)',
                r'(ë¡œë“œë§µ|ê³„íš|ê°œë°œ.*í˜„í™©|ë²„ì „)',
                r'(ê¸°ëŠ¥.*ëª©ë¡|ëŠ¥ë ¥.*ëª©ë¡|í• .*ìˆ˜.*ìˆ)',
                r'(ì „ì²´.*êµ¬ì¡°|ë‚´ë¶€.*êµ¬ì¡°|ì‘ë™.*ë°©ì‹)',
                r'(ë©”ëª¨ë¦¬.*êµ¬ì¡°|ë°ì´í„°ë² ì´ìŠ¤|ë²¡í„°)'
            ],
            IntentType.ACTION: [
                r'(í•´ì¤˜|ì‹¤í–‰|ì²˜ë¦¬|ì‘ì—…|ìˆ˜í–‰)',
                r'(ë§Œë“¤ì–´|ìƒì„±|ì‘ì„±|ê°œë°œ)',
                r'(ìˆ˜ì •|ë³€ê²½|ì—…ë°ì´íŠ¸|ë¦¬íŒ©í† ë§)',
                r'(ë¶„ì„|ê²€í† |í™•ì¸|ì ê²€)'
            ],
            IntentType.KNOWLEDGE: [
                r'(.*ì—.*ëŒ€í•´|.*ê´€ë ¨|.*ì •ë³´)',
                r'(.*ì„¤ëª…|.*ëœ»|.*ì˜ë¯¸|.*ê°œë…)',
                r'(.*ë°©ë²•|.*ì–´ë–»ê²Œ|.*ì™œ|.*ì–¸ì œ)',
                r'(ê¸°ì–µ.*ìˆ|ì•Œê³ .*ìˆ|í•™ìŠµ.*í–ˆ)'
            ],
            IntentType.CASUAL: [
                r'(ì•ˆë…•|hello|hi|ë°˜ê°€)',
                r'(ì–´ë–»ê²Œ.*ì§€ë‚´|ì˜.*ìˆ|ê´œì°®)',
                r'(ì¢‹.*ë‚ ì”¨|ì˜¤ëŠ˜.*ì–´ë•Œ|ê¸°ë¶„.*ì–´ë•Œ)',
                r'(ê³ ë§ˆì›Œ|ê°ì‚¬|ìˆ˜ê³ )'
            ]
        }

    def analyze_intent(self, query: str) -> Tuple[IntentType, float, Dict]:
        """ì˜ë„ ë¶„ì„ ë° ì‹ ë¢°ë„ ê³„ì‚°"""
        query_lower = query.lower()
        scores = {}
        details = {}

        for intent_type, patterns in self.patterns.items():
            score = 0
            matched_patterns = []

            for pattern in patterns:
                matches = re.findall(pattern, query_lower)
                if matches:
                    score += len(matches) * 10  # íŒ¨í„´ë‹¹ 10ì 
                    matched_patterns.append(pattern)

            scores[intent_type] = score
            details[intent_type.value] = {
                "score": score,
                "matched_patterns": matched_patterns
            }

        # ìµœê³  ì ìˆ˜ ì˜ë„ ì„ íƒ
        if scores:
            primary_intent = max(scores, key=scores.get)
            confidence = min(scores[primary_intent] / 10.0, 1.0)  # 0-1 ì •ê·œí™”
        else:
            primary_intent = IntentType.KNOWLEDGE  # ê¸°ë³¸ê°’
            confidence = 0.1

        return primary_intent, confidence, details

# âœ… ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ê¸°
class ContextAnalyzer:
    @staticmethod
    def analyze_context(query: str, memory: Optional[str] = None) -> Dict:
        """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context = {
            "has_memory": bool(memory),
            "query_length": len(query),
            "question_type": "unknown",
            "urgency": "normal",
            "specificity": "medium"
        }

        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        if any(marker in query for marker in ["?", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ"]):
            context["question_type"] = "interrogative"
        elif any(marker in query for marker in ["í•´ì¤˜", "ì‹¤í–‰", "ë§Œë“¤ì–´"]):
            context["question_type"] = "imperative"
        else:
            context["question_type"] = "declarative"

        # ê¸´ê¸‰ë„ ë¶„ì„
        if any(word in query for word in ["ê¸´ê¸‰", "ê¸‰í•´", "ë¹¨ë¦¬", "ì¦‰ì‹œ"]):
            context["urgency"] = "high"
        elif any(word in query for word in ["ì²œì²œíˆ", "ë‚˜ì¤‘ì—", "ì–¸ì  ê°€"]):
            context["urgency"] = "low"

        # êµ¬ì²´ì„± ë¶„ì„
        if len(query) > 100 or any(word in query for word in ["êµ¬ì²´ì ", "ìì„¸íˆ", "ì •í™•íˆ"]):
            context["specificity"] = "high"
        elif len(query) < 20:
            context["specificity"] = "low"

        return context

# âœ… ê³ ê¸‰ ë¼ìš°í„° ë…¸ë“œ (4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ì˜ë¯¸ë¡ ì  ë¶„ê¸°)
def route_node(state: dict, verbose: bool = False) -> dict:
    if verbose:
        print("ğŸ§­ [route_node] 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ê³ ê¸‰ ì˜ë¯¸ë¡ ì  ë¼ìš°í„° ì§„ì…")

    query = state.get("output", "")
    memory = state.get("memory")
    session_id = state.get("session_id", "default")

    if not query:
        return {**state, "router_decision": "no_memory_generate_node"}

    # 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    temporal_memory = get_temporal_memory_system()
    memory_context = temporal_memory.retrieve_memories(
        query=query,
        session_id=session_id,
        max_results=5,
        min_relevance=0.2
    )

    # ì˜ë„ ë¶„ì„
    analyzer = IntentAnalyzer()
    primary_intent, confidence, intent_details = analyzer.analyze_intent(query)

    # ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ (ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
    context = ContextAnalyzer.analyze_context(query, memory)

    # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê°•í™”
    if memory_context:
        context["has_memory_context"] = True
        context["memory_layers"] = [result['layer'] for result in memory_context]
        context["memory_relevance"] = sum(result['relevance'] for result in memory_context) / len(memory_context)
    else:
        context["has_memory_context"] = False
        context["memory_relevance"] = 0.0

    # ë¼ìš°íŒ… ê²°ì • ë¡œì§ (ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤)
    decision = _make_routing_decision(primary_intent, confidence, context, memory, memory_context, verbose)

    if verbose:
        print(f"ğŸ” ì˜ë„ ë¶„ì„: {primary_intent.value} (ì‹ ë¢°ë„: {confidence:.2f})")
        print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸: {context}")
        print(f"ğŸ§  ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸: {len(memory_context)}ê°œ ê²°ê³¼")
        print(f"ğŸ”€ ë¼ìš°íŒ… ê²°ì •: {decision}")

    return {
        **state,
        "router_decision": decision,
        "intent_analysis": {
            "primary_intent": primary_intent.value,
            "confidence": confidence,
            "details": intent_details,
            "context": context
        },
        "confidence_score": confidence,
        "temporal_memory_context": memory_context
    }

def _make_routing_decision(intent: IntentType, confidence: float, context: Dict,
                          memory: Optional[str], memory_context: List[Dict], verbose: bool = False) -> str:
    """4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì„ ê³ ë ¤í•œ ë¼ìš°íŒ… ê²°ì • ë¡œì§"""

    has_memory_context = context.get("has_memory_context", False)
    memory_relevance = context.get("memory_relevance", 0.0)

    # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê°•ë„ ê³„ì‚°
    memory_strength = len(memory_context) * memory_relevance if memory_context else 0

    # 1. IDENTITY ì˜ë„ â†’ identity_reflect (ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¡œ ê°•í™”)
    if intent == IntentType.IDENTITY and confidence > 0.3:
        return "identity_reflect"

    # 2. META ì˜ë„ â†’ status_report (ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‹œìŠ¤í…œ ì •ë³´ ê°•í™”)
    if intent == IntentType.META and confidence > 0.2:
        return "status_report"

    # 3. ACTION ì˜ë„ â†’ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²°ì •
    if intent == IntentType.ACTION:
        if has_memory_context and memory_relevance > 0.4:
            return "recall_chatgpt"  # ê´€ë ¨ ê¸°ì–µì´ ìˆìœ¼ë©´ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ
        elif memory or confidence > 0.7:
            return "status_report"
        else:
            return "no_memory_generate_node"

    # 4. KNOWLEDGE ì˜ë„ â†’ ê³„ì¸µì  ë©”ëª¨ë¦¬ ê²€ìƒ‰ ìš°ì„ 
    if intent == IntentType.KNOWLEDGE:
        if has_memory_context and memory_relevance > 0.3:
            return "recall_chatgpt"  # ê´€ë ¨ ê¸°ì–µ í™œìš©
        elif memory:
            return "recall_chatgpt"  # ê¸°ì¡´ ë©”ëª¨ë¦¬ í™œìš©
        elif confidence > 0.5:
            return "no_memory_generate_node"
        else:
            return "recall_chatgpt"  # ë¶ˆí™•ì‹¤í•˜ë©´ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì‹œë„

    # 5. CASUAL ì˜ë„ â†’ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ê°€ ê°•í•˜ë©´ ê°œì¸í™”ëœ ì‘ë‹µ
    if intent == IntentType.CASUAL:
        if has_memory_context and memory_relevance > 0.5:
            return "recall_chatgpt"  # ê°œì¸í™”ëœ ìºì£¼ì–¼ ì‘ë‹µ
        else:
            return "no_memory_generate_node"

    # 6. ê¸°ë³¸ê°’: ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§
    if has_memory_context and memory_relevance > 0.3:
        return "recall_chatgpt"
    elif memory:
        return "recall_chatgpt"
    else:
        return "no_memory_generate_node"

# âœ… ì‘ë‹µ í’ˆì§ˆ í‰ê°€ê¸°
class ResponseQualityEvaluator:
    @staticmethod
    def evaluate_memory_relevance(query: str, memory_results: List[str]) -> Dict:
        """ë©”ëª¨ë¦¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± í‰ê°€"""
        if not memory_results:
            return {"relevance_score": 0.0, "confidence": 0.0, "reasons": ["No memory results"]}

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜
        query_keywords = set(re.findall(r'\w+', query.lower()))
        total_score = 0

        for result in memory_results:
            result_keywords = set(re.findall(r'\w+', result.lower()))
            overlap = len(query_keywords.intersection(result_keywords))
            total_score += overlap / max(len(query_keywords), 1)

        avg_score = total_score / len(memory_results)

        return {
            "relevance_score": min(avg_score, 1.0),
            "confidence": 0.8 if avg_score > 0.3 else 0.4,
            "reasons": [f"í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜: {avg_score:.2f}"],
            "result_count": len(memory_results)
        }

# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
class HybridSearchSystem:
    def __init__(self):
        self.embedding = OpenAIEmbeddings()
        self.quality_evaluator = ResponseQualityEvaluator()

    def search_memories(self, query: str, verbose: bool = False) -> Tuple[List[str], Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í‚¤ì›Œë“œ + ì˜ë¯¸ë¡ ì """
        if verbose:
            print(f"ğŸ” [HybridSearch] ë‹¤ì¸µ ê²€ìƒ‰ ì‹œì‘: {query}")

        results = []
        search_info = {"methods_used": [], "result_counts": {}}

        try:
            # 1. ë²¡í„° ê²€ìƒ‰ (ê¸°ì¡´)
            vector_results = self._vector_search(query, verbose)
            results.extend(vector_results)
            search_info["methods_used"].append("vector")
            search_info["result_counts"]["vector"] = len(vector_results)

            # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œë„
            keyword_results = self._keyword_search(query, verbose)
            results.extend(keyword_results)
            search_info["methods_used"].append("keyword")
            search_info["result_counts"]["keyword"] = len(keyword_results)

            # 3. ì¤‘ë³µ ì œê±°
            unique_results = list(dict.fromkeys(results))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°

            # 4. í’ˆì§ˆ í‰ê°€
            quality = self.quality_evaluator.evaluate_memory_relevance(query, unique_results)

            if verbose:
                print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(unique_results)}ê°œ, í’ˆì§ˆ ì ìˆ˜: {quality['relevance_score']:.2f}")

            return unique_results, {**search_info, "quality": quality}

        except Exception as e:
            if verbose:
                print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], {"error": str(e), "methods_used": [], "result_counts": {}}

    def _vector_search(self, query: str, verbose: bool = False) -> List[str]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            retriever_chatgpt = Chroma(
                persist_directory="./memory/vectors/chatgpt_memory",
                embedding_function=self.embedding
            ).as_retriever(search_kwargs={"k": 3})

            retriever_gumgang = Chroma(
                persist_directory="./memory/gumgang_memory",
                embedding_function=self.embedding
            ).as_retriever(search_kwargs={"k": 3})

            # ê° ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
            docs_chatgpt = retriever_chatgpt.get_relevant_documents(query)
            docs_gumgang = retriever_gumgang.get_relevant_documents(query)

            results = []
            for doc in docs_chatgpt + docs_gumgang:
                if hasattr(doc, 'page_content'):
                    results.append(doc.page_content)
                else:
                    results.append(str(doc))

            return results[:5]  # ìƒìœ„ 5ê°œë§Œ

        except Exception as e:
            if verbose:
                print(f"âš ï¸ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _keyword_search(self, query: str, verbose: bool = False) -> List[str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25)"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ êµ¬í˜„ (BM25ëŠ” ë¬¸ì„œ ì»¬ë ‰ì…˜ì´ í•„ìš”)
            query_keywords = re.findall(r'\w+', query.lower())
            if not query_keywords:
                return []

            # ë©”ëª¨ë¦¬ íŒŒì¼ë“¤ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìƒ‰ì¸ëœ ë¬¸ì„œì—ì„œ ê²€ìƒ‰
            keyword_results = []

            # ê°„ë‹¨í•œ ì˜ˆì‹œ êµ¬í˜„
            return keyword_results

        except Exception as e:
            if verbose:
                print(f"âš ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

# âœ… 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ê¸°ì–µ ê²€ìƒ‰ ë…¸ë“œ
def recall_chatgpt_node(state: State, verbose: bool = False) -> State:
    if verbose:
        print(f"ğŸ§  [recall_chatgpt_node] 4ê³„ì¸µ ë©”ëª¨ë¦¬ + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ â†’ {state['output']}")

    query = state["output"]
    session_id = state.get("session_id", "default")
    temporal_memory_context = state.get("temporal_memory_context", [])

    # 1. 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì—ì„œ ìš°ì„  ê²€ìƒ‰
    temporal_memory = get_temporal_memory_system()
    memory_results = []

    if not temporal_memory_context:
        # ë¼ìš°í„°ì—ì„œ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆë‹¤ë©´ ì—¬ê¸°ì„œ ê²€ìƒ‰
        memory_results = temporal_memory.retrieve_memories(
            query=query,
            session_id=session_id,
            max_results=10,
            min_relevance=0.2
        )
    else:
        memory_results = temporal_memory_context

    # 2. ê¸°ì¡´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ë„ ë³‘í–‰
    search_system = HybridSearchSystem()
    legacy_results, search_info = search_system.search_memories(query, verbose)

    # 3. ê²°ê³¼ í†µí•© ë° í’ˆì§ˆ í‰ê°€
    all_results = []

    # 4ê³„ì¸µ ë©”ëª¨ë¦¬ ê²°ê³¼ ì²˜ë¦¬
    for result in memory_results:
        trace = result['trace']
        all_results.append({
            'content': trace.content,
            'relevance': result['relevance'],
            'source': f"temporal_memory_{result['layer']}",
            'timestamp': trace.timestamp,
            'priority': trace.priority.value,
            'memory_type': trace.memory_type.value
        })

    # ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
    for content in legacy_results:
        all_results.append({
            'content': content,
            'relevance': 0.5,  # ê¸°ë³¸ê°’
            'source': 'legacy_search',
            'timestamp': None,
            'priority': 0.5,
            'memory_type': 'unknown'
        })

    if not all_results:
        if verbose:
            print("ğŸ” ëª¨ë“  ê²€ìƒ‰ì—ì„œ ê²°ê³¼ ì—†ìŒ â†’ no_memory_generateë¡œ ì „í™˜ ì œì•ˆ")
        return {
            **state,
            "status": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (4ê³„ì¸µ + í•˜ì´ë¸Œë¦¬ë“œ)",
            "memory": None,
            "search_results": [],
            "response_quality": {"search_success": False, "reason": "No results found"},
            "suggest_ingest": True
        }

    # 4. í†µí•©ëœ ê²°ê³¼ë¡œ ì‘ë‹µ ìƒì„±
    response = _generate_temporal_memory_response(query, all_results, memory_results, verbose)

    # 5. ì‘ë‹µ í’ˆì§ˆ í‰ê°€
    quality_score = _evaluate_temporal_memory_response_quality(all_results, memory_results)

    return {
        **state,
        "status": "4ê³„ì¸µ ë©”ëª¨ë¦¬ + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ",
        "memory": response,
        "source": "temporal_memory_hybrid",
        "search_results": all_results,
        "response_quality": {
            "search_success": True,
            "relevance_score": quality_score,
            "result_count": len(all_results),
            "memory_layers": list(layer_context.keys()) if 'layer_context' in locals() else [],
            "temporal_memory_count": len(memory_results),
            "legacy_search_count": len(legacy_results)
        }
    }

# âœ… ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„± (í’ˆì§ˆ ë†’ìŒ)
def _generate_temporal_memory_response(query: str, all_results: List[Dict], memory_results: List[Dict], verbose: bool = False) -> str:
    """4ê³„ì¸µ ë©”ëª¨ë¦¬ ê²°ê³¼ë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±"""
    if verbose:
        print(f"ğŸ§  4ê³„ì¸µ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„±: {len(memory_results)}ê°œ ë©”ëª¨ë¦¬ + {len(all_results)}ê°œ ì „ì²´ ê²°ê³¼")

    # ë©”ëª¨ë¦¬ ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    layer_context = {}
    for result in memory_results:
        layer = result['layer']
        if layer not in layer_context:
            layer_context[layer] = []
        layer_context[layer].append(result['trace'].content)

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context_parts = []

    # ìµœì‹  ì»¨í…ìŠ¤íŠ¸ (ì´ˆë‹¨ê¸°/ë‹¨ê¸° ë©”ëª¨ë¦¬)
    if 'ultra_short' in layer_context or 'short_term' in layer_context:
        recent_context = []
        recent_context.extend(layer_context.get('ultra_short', []))
        recent_context.extend(layer_context.get('short_term', []))
        if recent_context:
            context_parts.append("ìµœê·¼ ëŒ€í™” ë§¥ë½:")
            context_parts.extend(recent_context[:3])

    # ê´€ë ¨ ê²½í—˜ (ì¤‘ì¥ê¸° ë©”ëª¨ë¦¬)
    if 'medium_term' in layer_context:
        context_parts.append("\nê´€ë ¨ ê²½í—˜:")
        context_parts.extend(layer_context['medium_term'][:2])

    # í•µì‹¬ ì§€ì‹ (ì¥ê¸° ë©”ëª¨ë¦¬)
    if 'long_term' in layer_context:
        context_parts.append("\ní•µì‹¬ ì§€ì‹:")
        context_parts.extend(layer_context['long_term'][:2])

    # ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼
    other_results = [r['content'] for r in all_results if 'temporal_memory' not in r['source']]
    if other_results:
        context_parts.append("\nì¶”ê°€ ì •ë³´:")
        context_parts.extend(other_results[:2])

    full_context = "\n".join(context_parts)

    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.6,  # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë¯€ë¡œ ì˜¨ë„ ë‚®ì¶¤
            max_tokens=1200   # ë” í’ë¶€í•œ ì‘ë‹µì„ ìœ„í•´ í† í° ì¦ê°€
        )

        prompt = f"""ë‹¤ìŒì€ 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:

{full_context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ì˜ ê³„ì¸µë³„ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°œì¸í™”ë˜ê³  ë§¥ë½ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ìµœê·¼ ëŒ€í™”ëŠ” ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼, í•µì‹¬ ì§€ì‹ì€ ê¸°ë°˜ ì •ë³´ë¡œ í™œìš©í•˜ì„¸ìš”."""

        response = llm.invoke(prompt)
        return response.content

    except Exception as e:
        if verbose:
            print(f"âŒ 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def _evaluate_temporal_memory_response_quality(all_results: List[Dict], memory_results: List[Dict]) -> float:
    """4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    if not all_results:
        return 0.0

    # ê¸°ë³¸ ì ìˆ˜
    base_score = 0.0

    # ë©”ëª¨ë¦¬ ê³„ì¸µë³„ ê°€ì¤‘ì¹˜
    layer_weights = {
        'ultra_short': 0.4,  # ìµœì‹ ì„± ë†’ìŒ
        'short_term': 0.3,   # ì„¸ì…˜ ì—°ê´€ì„±
        'medium_term': 0.2,  # íŒ¨í„´ ê¸°ë°˜
        'long_term': 0.1     # í•µì‹¬ ì§€ì‹
    }

    # ê³„ì¸µë³„ ì ìˆ˜ ê³„ì‚°
    for result in memory_results:
        layer = result['layer']
        relevance = result['relevance']
        weight = layer_weights.get(layer, 0.1)
        base_score += relevance * weight

    # ê²°ê³¼ ìˆ˜ ë³´ë„ˆìŠ¤ (ì ë‹¹í•œ ìˆ˜ì˜ ê²°ê³¼)
    result_count = len(all_results)
    if 3 <= result_count <= 7:
        base_score += 0.1
    elif result_count > 10:
        base_score -= 0.1  # ë„ˆë¬´ ë§ìœ¼ë©´ ë…¸ì´ì¦ˆ

    # ë‹¤ì¸µ ë©”ëª¨ë¦¬ ë³´ë„ˆìŠ¤
    unique_layers = set(result['layer'] for result in memory_results)
    if len(unique_layers) > 1:
        base_score += 0.1 * len(unique_layers)

    return min(1.0, base_score)

def _generate_memory_response(query: str, search_results: List[str], verbose: bool = False) -> str:
    """ê³ í’ˆì§ˆ ë©”ëª¨ë¦¬ ê²°ê³¼ë¡œ ì‘ë‹µ ìƒì„±"""
    if verbose:
        print("ğŸ§  [_generate_memory_response] ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„±")

    try:
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2)

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ í”„ë¡¬í”„íŠ¸
        memory_context = "\n".join([f"- {result}" for result in search_results[:3]])

        prompt = PromptTemplate.from_template("""
ê´€ë ¨ ê¸°ì–µ:
{memory_context}

ì§ˆë¬¸: {question}

ìœ„ì˜ ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. ê¸°ì–µì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
""")

        chain = prompt | llm
        response = chain.invoke({
            "question": query,
            "memory_context": memory_context
        })

        return response.content if hasattr(response, 'content') else str(response)

    except Exception as e:
        return f"ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# âœ… í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ìƒì„± (í’ˆì§ˆ ë‚®ìŒ)
def _generate_hybrid_response(query: str, search_results: List[str], verbose: bool = False) -> str:
    """ë‚®ì€ í’ˆì§ˆ ë©”ëª¨ë¦¬ë¥¼ GPT ì¶”ë¡ ìœ¼ë¡œ ë³´ì™„"""
    if verbose:
        print("ğŸ”§ [_generate_hybrid_response] í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ìƒì„±")

    try:
        llm = ChatOpenAI(model="gpt-4", temperature=0.5)

        memory_context = "\n".join([f"- {result}" for result in search_results[:2]])

        prompt = PromptTemplate.from_template("""
ë¶€ë¶„ì  ê¸°ì–µ:
{memory_context}

ì§ˆë¬¸: {question}

ìœ„ì˜ ê¸°ì–µì€ ì§ˆë¬¸ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê¸°ì–µì—ì„œ ìœ ìš©í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ í™œìš©í•˜ë˜, ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì—¬ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ë‹µë³€ ì‹œì‘ì— "ê¸°ì–µê³¼ ì¶”ë¡ ì„ ê²°í•©í•œ ë‹µë³€:"ì´ë¼ê³  ëª…ì‹œí•´ì£¼ì„¸ìš”.
""")

        chain = prompt | llm
        response = chain.invoke({
            "question": query,
            "memory_context": memory_context
        })

        return response.content if hasattr(response, 'content') else str(response)

    except Exception as e:
        return f"í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# âœ… 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ì ì‘í˜• GPT ìƒì„± ë…¸ë“œ
def no_memory_generate_node(state: State, verbose: bool = False) -> State:
    try:
        if verbose:
            print("ğŸŒ [no_memory_generate_node] 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ì ì‘í˜• ì‘ë‹µ ìƒì„± ì¤‘...")

        query = state["output"]
        intent_analysis = state.get("intent_analysis", {})
        primary_intent = intent_analysis.get("primary_intent", "knowledge")
        confidence = state.get("confidence_score", 0.5)
        session_id = state.get("session_id", "default")

        # 4ê³„ì¸µ ë©”ëª¨ë¦¬ì—ì„œ ì•½í•œ ê´€ë ¨ì„± ê²€ìƒ‰ (ë©”ëª¨ë¦¬ê°€ ì—†ì–´ë„ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•)
        temporal_memory = get_temporal_memory_system()
        weak_context = temporal_memory.retrieve_memories(
            query=query,
            session_id=session_id,
            max_results=3,
            min_relevance=0.1  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì•½í•œ ê´€ë ¨ì„±ë„ í¬í•¨
        )

        # ì˜ë„ë³„ ë§ì¶¤í˜• ì‘ë‹µ ìƒì„± (4ê³„ì¸µ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        response = _generate_contextual_response(query, primary_intent, confidence, session_id, weak_context, verbose)

        # ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤)
        quality_score = _evaluate_response_quality(query, response, weak_context)

        # ìƒì„±ëœ ì‘ë‹µì„ 4ê³„ì¸µ ë©”ëª¨ë¦¬ì— ì €ì¥
        _store_generated_response_to_memory(query, response, primary_intent, quality_score, session_id)

        return {
            **state,
            "status": "4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ì ì‘í˜• ì‘ë‹µ ì™„ë£Œ",
            "output": response,
            "source": "adaptive_gpt_memory_enhanced",
            "suggest_ingest": True,
            "temporal_memory_context": weak_context,
            "memory_enhanced_response": len(weak_context) > 0,
            "response_quality": {
                "generation_method": "adaptive_memory_enhanced",
                "intent_based": True,
                "quality_score": quality_score,
                "primary_intent": primary_intent,
                "context_boost": len(weak_context) > 0,
                "memory_context_count": len(weak_context)
            },
            "ingest_suggestion": {
                "should_ingest": quality_score > 0.7,
                "reason": f"í’ˆì§ˆ ì ìˆ˜ {quality_score:.2f}, ì˜ë„: {primary_intent}, ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸: {len(weak_context)}ê°œ"
            }
        }
    except Exception as e:
        import traceback
        print("âŒ [no_memory_generate_node] ì˜¤ë¥˜:", e)
        traceback.print_exc()
        return {
            **state,
            "status": "ì˜¤ë¥˜ ë°œìƒ",
            "output": f"[ì˜¤ë¥˜]: {str(e)}",
            "response_quality": {"error": True, "message": str(e)}
        }

def _generate_contextual_response(query: str, intent: str, confidence: float,
                                 session_id: str = None, memory_context: List[Dict] = None, verbose: bool = False) -> str:
    """4ê³„ì¸µ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ë§ì¶¤í˜• ì‘ë‹µ ìƒì„±"""

    # ì»¨í…ìŠ¤íŠ¸ ê°•í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
    if session_id:
        response_enhancer = get_response_enhancer()
        enhanced_prompt, context_info = response_enhancer.enhance_prompt(query, session_id, intent)

        if verbose:
            print(f"ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©: {context_info.get('flow_analysis', {}).get('context_type', 'unknown')}")
    else:
        enhanced_prompt = f"ì§ˆë¬¸: {query}\n\në„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."

    # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    memory_context_str = ""
    if memory_context and len(memory_context) > 0:
        memory_parts = []
        for result in memory_context:
            trace = result['trace']
            layer = result['layer']
            relevance = result['relevance']
            content_preview = trace.content[:100] + "..." if len(trace.content) > 100 else trace.content
            memory_parts.append(f"[{layer}, ê´€ë ¨ë„: {relevance:.2f}] {content_preview}")

        memory_context_str = f"\n\nê´€ë ¨ ê¸°ì–µ (ì•½í•œ ì—°ê´€ì„±):\n" + "\n".join(memory_parts)
        enhanced_prompt += memory_context_str

    # ì˜ë„ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (4ê³„ì¸µ ë©”ëª¨ë¦¬ ê°•í™”)
    intent_prompts = {
        "identity": f"""
ë‹¹ì‹ ì€ ê¸ˆê°•ì´ë¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì„ í†µí•´ ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
{enhanced_prompt}

ìì‹ ì˜ ì •ì²´ì„±ì— ëŒ€í•´ ì¼ê´€ë˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ë˜, ìœ„ì˜ ê´€ë ¨ ê¸°ì–µì´ ìˆë‹¤ë©´ ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ë” ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.
""",
        "meta": f"""
ë‹¹ì‹ ì€ ê¸ˆê°•ì´ë¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.
{enhanced_prompt}

ì‹œìŠ¤í…œì˜ êµ¬ì¡°, ê¸°ëŠ¥, í˜„ì¬ ìƒíƒœì— ëŒ€í•´ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ê´€ë ¨ ê¸°ì–µì´ ìˆë‹¤ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
""",
        "knowledge": f"""
ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§€ì‹ ê´€ë ¨ ì§ˆë¬¸ì…ë‹ˆë‹¤.
{enhanced_prompt}

ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ê´€ë ¨ ê¸°ì–µì´ ìˆë‹¤ë©´ ì´ë¥¼ í™œìš©í•˜ì—¬ ë” ê°œì¸í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ëª…ì‹œí•´ì£¼ì„¸ìš”.
""",
        "action": f"""
ë‹¹ì‹ ì€ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í–‰ë™ ê´€ë ¨ ìš”ì²­ì…ë‹ˆë‹¤.
{enhanced_prompt}

êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê´€ë ¨ ê¸°ì–µì´ ìˆë‹¤ë©´ ì‚¬ìš©ìì˜ ì´ì „ ê²½í—˜ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
""",
        "casual": f"""
ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì¼ìƒì ì¸ ëŒ€í™”ì…ë‹ˆë‹¤.
{enhanced_prompt}

ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”. ê´€ë ¨ ê¸°ì–µì´ ìˆë‹¤ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ê°œì¸ì ì´ê³  ë”°ëœ»í•œ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.
"""
    }

    # ì˜ë„ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ì„ íƒ
    selected_prompt = intent_prompts.get(intent, f"""
ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
{enhanced_prompt}

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ê´€ë ¨ ê¸°ì–µì´ ìˆë‹¤ë©´ ì´ë¥¼ í™œìš©í•˜ì—¬ ë” ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.
""")

    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=1000
        )

        response = llm.invoke(selected_prompt)
        return response.content if hasattr(response, 'content') else str(response)

    except Exception as e:
        if verbose:
            print(f"âŒ ì»¨í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def _store_generated_response_to_memory(query: str, response: str, intent: str, quality_score: float, session_id: str):
    """ìƒì„±ëœ ì‘ë‹µì„ 4ê³„ì¸µ ë©”ëª¨ë¦¬ì— ì €ì¥"""
    try:
        temporal_memory = get_temporal_memory_system()

        # ë©”ëª¨ë¦¬ íƒ€ì… ê²°ì •
        memory_type_map = {
            'identity': MemoryType.SEMANTIC,
            'knowledge': MemoryType.SEMANTIC,
            'action': MemoryType.PROCEDURAL,
            'meta': MemoryType.CONTEXTUAL,
            'casual': MemoryType.EPISODIC
        }
        memory_type = memory_type_map.get(intent, MemoryType.EPISODIC)

        # ìš°ì„ ìˆœìœ„ ê²°ì •
        if quality_score >= 0.8:
            priority = MemoryPriority.HIGH
        elif quality_score >= 0.6:
            priority = MemoryPriority.MEDIUM
        elif quality_score >= 0.4:
            priority = MemoryPriority.LOW
        else:
            priority = MemoryPriority.MINIMAL

        # íƒœê·¸ ìƒì„±
        tags = {intent, 'generated_response', f'quality_{int(quality_score * 10)}'}

        # ë©”ëª¨ë¦¬ ì €ì¥
        content = f"ì‚¬ìš©ì: {query}\nì‹œìŠ¤í…œ: {response}"
        temporal_memory.store_memory(
            content=content,
            memory_type=memory_type,
            priority=priority,
            session_id=session_id,
            emotional_valence=0.1,  # ì•½ê°„ ê¸ì •ì  (ë„ì›€ì„ ì œê³µí–ˆìœ¼ë¯€ë¡œ)
            tags=tags
        )

    except Exception as e:
        print(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def _evaluate_response_quality(query: str, response: str, memory_context: List[Dict] = None) -> float:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤)"""
    quality_score = 0.5  # ê¸°ë³¸ê°’

    # ì‘ë‹µ ê¸¸ì´ í‰ê°€
    if 50 <= len(response) <= 1000:
        quality_score += 0.1
    elif len(response) < 20:
        quality_score -= 0.2

    # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í™œìš© ë³´ë„ˆìŠ¤
    if memory_context and len(memory_context) > 0:
        quality_score += 0.1 * len(memory_context)

    # ì˜¤ë¥˜ ë©”ì‹œì§€ ê°ì 
    if "ì˜¤ë¥˜" in response or "ì£„ì†¡í•©ë‹ˆë‹¤" in response:
        quality_score -= 0.3

    # êµ¬ì²´ì„± í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    if any(word in response for word in ["êµ¬ì²´ì ìœ¼ë¡œ", "ì˜ˆë¥¼ ë“¤ì–´", "ë‹¨ê³„ì ìœ¼ë¡œ"]):
        quality_score += 0.1

    return min(1.0, max(0.0, quality_score))

# âœ… ìƒíƒœ ìš”ì•½ ë…¸ë“œ (4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© Chat ì „ìš© ìš”ì•½ ì¶œë ¥)
def status_report_wrapper(state: dict, verbose: bool = False) -> dict:
    if verbose:
        print("ğŸ“Š [status_report_wrapper] 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ìƒíƒœ ë³´ê³  ì‹¤í–‰ ì¤‘...")

    # 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ìƒíƒœ ì¶”ê°€
    temporal_memory = get_temporal_memory_system()
    memory_stats = temporal_memory.get_memory_stats()

    # ê¸°ì¡´ ìƒíƒœ ë³´ê³ ì— ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
    enhanced_state = {
        **state,
        "memory_stats": memory_stats,
        "temporal_memory_active": True
    }

    report = status_report_node(enhanced_state, verbose=verbose)
    response = format_for_chat(report)

    if verbose:
        print("ğŸ§  ë©”ëª¨ë¦¬ í†µê³„:", memory_stats['layers'])
        print("ğŸ§¾ report:", report)
        print("ğŸ—£ï¸ response (chatìš©):", response)

    return {
        **report,
        "status": "4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© ìƒíƒœ ë³´ê³  ì™„ë£Œ",
        "output": response or "âŒ ì¶œë ¥ ì—†ìŒ",
        "memory_stats": memory_stats
    }

# âœ… ë©”íƒ€ ì¸ì§€ ë…¸ë“œ (Think-Reflect-Create)
async def metacognitive_node(state: State, verbose: bool = False) -> State:
    """
    ë©”íƒ€ ì¸ì§€ ì‹œìŠ¤í…œ ë…¸ë“œ
    Claude 4.1 Think Engineì˜ í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„
    """
    if verbose:
        print("ğŸ§  [metacognitive_node] ë©”íƒ€ ì¸ì§€ ì‹œìŠ¤í…œ í™œì„±í™”...")

    # ë©”íƒ€ ì¸ì§€ ì‹œìŠ¤í…œ ê°€ì ¸ì˜¤ê¸°
    metacognitive_system = get_metacognitive_system()

    query = state.get("output", "")
    context = {
        "session_id": state.get("session_id"),
        "intent": state.get("intent_analysis"),
        "confidence": state.get("confidence_score", 0.5),
        "temporal_memory": state.get("temporal_memory_context", [])
    }

    # Think-Reflect-Create íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        import asyncio
        metacognitive_result = await metacognitive_system.think_reflect_create(
            query=query,
            context=context
        )

        if verbose:
            print(f"ğŸ¯ ë©”íƒ€ ì¸ì§€ í™•ì‹ ë„: {metacognitive_result.get('final_confidence', 0):.2f}")
            print(f"ğŸ’¡ ìƒì„±ëœ í†µì°°: {metacognitive_result.get('insights_generated', 0)}ê°œ")

        # í•™ìŠµ ì „ëµ ì¡°ì •
        new_strategy = await metacognitive_system.adapt_learning_strategy()
        if verbose:
            print(f"ğŸ“š í•™ìŠµ ì „ëµ: {new_strategy}")

        # ì‹ ê²½ í™œì„±í™” ëª¨ë‹ˆí„°ë§
        activation_analysis = await metacognitive_system.monitor_neural_activations()

        # ìê¸° ì¸ì‹ ë³´ê³ ì„œ ìƒì„± (ì£¼ê¸°ì ìœ¼ë¡œ)
        import random
        if random.random() < 0.1:  # 10% í™•ë¥ ë¡œ ìê¸° ì¸ì‹ ë³´ê³ 
            self_report = await metacognitive_system.self_awareness_report()
            if verbose:
                print(f"ğŸª ìê¸° ì¸ì‹: {self_report.get('self_description', '')}")

        return {
            **state,
            "metacognitive_analysis": metacognitive_result,
            "cognitive_state": metacognitive_result.get("cognitive_state"),
            "reasoning_chain": metacognitive_result.get("thinking", {}).get("reasoning_chain", []),
            "confidence_score": metacognitive_result.get("final_confidence", state.get("confidence_score", 0.5))
        }

    except Exception as e:
        if verbose:
            print(f"âš ï¸ ë©”íƒ€ ì¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        return {
            **state,
            "metacognitive_analysis": {"error": str(e)},
            "cognitive_state": None,
            "reasoning_chain": []
        }

# ë™ê¸° ë˜í¼ (LangGraphëŠ” ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©)
def metacognitive_node_sync(state: State, verbose: bool = False) -> State:
    """ë©”íƒ€ ì¸ì§€ ë…¸ë“œ ë™ê¸° ë˜í¼"""
    import asyncio

    # ì´ë²¤íŠ¸ ë£¨í”„ ì²˜ë¦¬
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    if loop.is_running():
        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ íƒœìŠ¤í¬ë¡œ ìŠ¤ì¼€ì¤„
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, metacognitive_node(state, verbose))
            return future.result()
    else:
        # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
        return loop.run_until_complete(metacognitive_node(state, verbose))

def _save_conversation_turn(state: State, response: str):
    """ëŒ€í™” í„´ì„ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìì— ì €ì¥ (4ê³„ì¸µ ë©”ëª¨ë¦¬ ìë™ ì—°ë™)"""
    try:
        query = state.get("output", "")
        session_id = state.get("session_id", "default")
        intent_analysis = state.get("intent_analysis", {})
        confidence = state.get("confidence_score", 0.5)
        source = state.get("source", "unknown")
        response_quality = state.get("response_quality", {})

        # ConversationTurn ìƒì„±
        turn = ConversationTurn(
            turn_id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            user_query=query,
            system_response=response,
            intent=intent_analysis.get("primary_intent", "unknown"),
            confidence=confidence,
            source=source,
            response_quality=response_quality.get("quality_score", 0.5),
            session_id=session_id,
            emotional_context=0.0,  # ê¸°ë³¸ê°’
            importance_score=confidence
        )

        # ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ì— ì €ì¥ (ìë™ìœ¼ë¡œ 4ê³„ì¸µ ë©”ëª¨ë¦¬ì—ë„ ì €ì¥ë¨)
        conversation_memory = get_conversation_memory()
        conversation_memory.add_turn(turn)

        # ì„¸ì…˜ ë§¤ë‹ˆì € ì—…ë°ì´íŠ¸
        session_manager = get_session_manager()
        session_manager.update_session(session_id, turn)

    except Exception as e:
        print(f"ëŒ€í™” í„´ ì €ì¥ ì‹¤íŒ¨: {e}")

# âœ… LangGraph ì‹¤í–‰ ì§„ì…ì  (4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í†µí•©)
def run_graph(user_input: str, session_id: str = None, verbose: bool = False) -> dict:
    if verbose:
        print(f"ğŸš€ 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µí•© LangGraph ì‹œì‘ â†’ '{user_input}'")

    # 4ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í™•ì¸
    temporal_memory = get_temporal_memory_system()
    if verbose:
        memory_stats = temporal_memory.get_memory_stats()
        print(f"ğŸ§  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í™œì„±í™”: {memory_stats['layers']}")

    workflow = StateGraph(State)

    workflow.add_node("reflect", lambda s: reflect_node(s, verbose=verbose))
    workflow.add_node("metacognitive", lambda s: metacognitive_node_sync(s, verbose=verbose))
    workflow.add_node("route", lambda s: route_node(s, verbose=verbose))
    workflow.add_node("identity_reflect", lambda s: identity_reflect_node(s, verbose=verbose))
    workflow.add_node("recall_chatgpt", lambda s: recall_chatgpt_node(s, verbose=verbose))
    workflow.add_node("generate_only_node", generate_only_node)
    workflow.add_node("status_report", lambda s: status_report_wrapper(s, verbose=verbose))
    workflow.add_node("no_memory_generate_node", lambda s: no_memory_generate_node(s, verbose=verbose))

    workflow.set_entry_point("reflect")

    workflow.add_edge("reflect", "metacognitive")
    workflow.add_edge("metacognitive", "route")
    workflow.add_conditional_edges(
        "route",
        lambda s: s["router_decision"],
        {
            "identity_reflect": "identity_reflect",
            "generate_only_node": "generate_only_node",
            "recall_chatgpt": "recall_chatgpt",
            "no_memory_generate_node": "no_memory_generate_node",
            "status_report": "status_report"
        }
    )

    workflow.add_edge("identity_reflect", "status_report")
    workflow.add_edge("generate_only_node", "status_report")
    workflow.add_edge("recall_chatgpt", "status_report")
    workflow.add_edge("no_memory_generate_node", "status_report")
    workflow.add_edge("status_report", END)

    app = workflow.compile()

    # ì„¸ì…˜ ID ì²˜ë¦¬
    if not session_id:
        session_manager = get_session_manager()
        session_id = session_manager.create_session()

    final_state = app.invoke({
        "status": None,
        "output": user_input,
        "memory": None,
        "session_id": session_id,
        "temporal_memory_context": []
    })

    print("ğŸ§ª ìµœì¢… ìƒíƒœ í™•ì¸:", final_state)

    # âœ… 3ë‹¨ê³„ ë¦¬íŒ©í† ë§ëœ í•µì‹¬ ë¦¬í„´ë¶€ (ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í¬í•¨)
    response_quality = final_state.get("response_quality", {})
    search_results = final_state.get("search_results", [])
    session_id = final_state.get("session_id")

    # ëŒ€í™” í„´ ê¸°ë¡ ì €ì¥ (4ê³„ì¸µ ë©”ëª¨ë¦¬ ìë™ ì—°ë™)
    response_text = final_state.get("output") or final_state.get("memory") or "ì‘ë‹µ ì—†ìŒ"
    if session_id:
        _save_conversation_turn(final_state, response_text)

    # 4ê³„ì¸µ ë©”ëª¨ë¦¬ í†µê³„ ì¶”ê°€
    temporal_memory_context = final_state.get("temporal_memory_context", [])
    memory_enhanced = final_state.get("memory_enhanced_response", False)

    return {
        "response": response_text,
        "source": final_state.get("source", "memory"),
        "suggest_ingest": final_state.get("suggest_ingest", False),
        "router_decision": final_state.get("router_decision", None),
        "status": final_state.get("status", None),
        "response_quality": response_quality,
        "search_results_count": len(search_results),
        "intent_analysis": final_state.get("intent_analysis", {}),
        "confidence_score": final_state.get("confidence_score", 0.0),
        "session_id": session_id,
        "conversation_context": final_state.get("conversation_context", {}),
        "temporal_memory_context": temporal_memory_context,
        "memory_enhanced": memory_enhanced,
        "memory_layers_used": [r['layer'] for r in temporal_memory_context] if temporal_memory_context else []
    }

# Function removed - duplicate version above is more complete
