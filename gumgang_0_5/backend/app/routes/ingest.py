# ğŸ“„ ingest.py

import os, json
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… ë²¡í„° ì €ì¥ì†Œ ìœ„ì¹˜ì™€ ì„ë² ë”© ì •ì˜
CHROMA_PATH = "./memory/gumgang_memory"
embedding = OpenAIEmbeddings()
db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding)

# âœ… íŒŒì¼ ë‚´ìš© ë¡œë“œ í•¨ìˆ˜
def load_text(path: str) -> str | None:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {path} â†’ {e}")
        return None

# âœ… íŒŒì¼ í˜•ì‹ë³„ íŒŒì‹±
def parse_file(filepath: str) -> str | None:
    ext = os.path.splitext(filepath)[-1].lower()
    text = load_text(filepath)
    if not text:
        return None

    if ext == ".html":
        soup = BeautifulSoup(text, "html.parser")
        return soup.get_text()
    elif ext in [".md", ".txt"]:
        return text
    elif ext == ".json":
        try:
            return json.dumps(json.loads(text), indent=2)
        except:
            return None
    return None

# âœ… í…ìŠ¤íŠ¸ â†’ ë¬¸ì„œ ì¡°ê° ë¶„í• 
def split_text_to_chunks(text: str, chunk_size=1000, chunk_overlap=200):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    return splitter.split_text(text)

# âœ… ë””ë ‰í† ë¦¬ ì „ì²´ ì¸ê²ŒìŠ¤íŠ¸
def ingest_directory(root_dir="~/newgumgang"):
    root_dir = os.path.expanduser(root_dir)
    added_chunks = 0
    for dirpath, _, filenames in os.walk(root_dir):
        for fname in filenames:
            if not fname.lower().endswith((".html", ".md", ".json", ".txt")):
                continue
            fpath = os.path.join(dirpath, fname)
            parsed = parse_file(fpath)
            if not parsed:
                continue

            chunks = split_text_to_chunks(parsed)
            docs = [Document(page_content=chunk) for chunk in chunks]

            try:
                db.add_documents(docs)
                print(f"âœ… {fname} â†’ {len(docs)} ì¡°ê° ì €ì¥ë¨")
                added_chunks += len(docs)
            except Exception as e:
                print(f"âŒ {fname} ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    db.persist()
    return {"ingested_chunks": added_chunks}

# âœ… ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¸ê²ŒìŠ¤íŠ¸ (APIì—ì„œ í˜¸ì¶œ)
def ingest_document(text: str):
    try:
        chunks = split_text_to_chunks(text)
        docs = [Document(page_content=chunk) for chunk in chunks]
        db.add_documents(docs)
        db.persist()
        return {"status": "success", "chunks": len(docs)}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# âœ… ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    print("ğŸ“¥ ê¸°ì–µ ì£¼ì… ì‹œì‘...")
    result = ingest_directory()
    print("âœ… ì™„ë£Œ:", result)
