import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Optional, AsyncGenerator
import datetime

import json
import asyncio

from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from fastapi import Request
from utils.time_kr import now_kr_str_minute, validate_kr_timestamp
from app.api.websocket_server import websocket_endpoint, start_background_tasks, manager, MessageType
from app.utils.memory_status_adapter import from_simple_memory, build_tiers_response
from utils.rules_enforcer import prepend_full_rules, HEAD_MARK
from app.middleware.approval_gate import ApprovalGateMiddleware
from app.routes import autocode as autocode_routes
from app.routes import files_read as files_read_routes
from app.routes import files_tree as files_tree_routes
import logging

# Logger ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸ§  ì „ì—­ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
memory_system_instance = None

def get_memory_system():
    """ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global memory_system_instance
    if memory_system_instance is None:
        from app.temporal_memory import TemporalMemorySystem, MemoryType, MemoryPriority, MemoryTrace
        import uuid
        from datetime import datetime

        memory_system_instance = TemporalMemorySystem()

        # ê° ë ˆë²¨ì— ì§ì ‘ ë°ì´í„° ì¶”ê°€ (ë”ë¯¸ ë°ì´í„°)
        # Level 1: Ultra Short (ì„ì‹œ ê¸°ì–µ)
        for i in range(5):
            trace = MemoryTrace(
                trace_id=str(uuid.uuid4()),
                content=f"ì‘ì—… ë©”ëª¨ë¦¬ {i+1}: í˜„ì¬ ì„¸ì…˜ ë°ì´í„°",
                timestamp=datetime.now(),
                memory_type=MemoryType.EPISODIC,
                priority=MemoryPriority.MEDIUM
            )
            memory_system_instance.ultra_short.buffer.append(trace.trace_id)

        # Level 2: Short Term (ì—í”¼ì†Œë“œ)
        for i in range(8):
            trace = MemoryTrace(
                trace_id=str(uuid.uuid4()),
                content=f"ì—í”¼ì†Œë“œ {i+1}: ìµœê·¼ ëŒ€í™” ë‚´ìš©",
                timestamp=datetime.now(),
                memory_type=MemoryType.EPISODIC,
                priority=MemoryPriority.MEDIUM
            )
            memory_system_instance.short_term.traces[trace.trace_id] = trace

        # Level 3: Medium Term (ì˜ë¯¸ ê¸°ì–µ)
        for i in range(15):
            trace = MemoryTrace(
                trace_id=str(uuid.uuid4()),
                content=f"ì˜ë¯¸ ê¸°ì–µ {i+1}: Blueprint v1.2, .rules ì›ì¹™",
                timestamp=datetime.now(),
                memory_type=MemoryType.SEMANTIC,
                priority=MemoryPriority.HIGH
            )
            memory_system_instance.medium_term.traces[trace.trace_id] = trace

        # Level 4: Long Term (ì ˆì°¨ ê¸°ì–µ)
        for i in range(20):
            trace = MemoryTrace(
                trace_id=str(uuid.uuid4()),
                content=f"ì ˆì°¨ ê¸°ì–µ {i+1}: Tauri, Next.js, Monaco Editor",
                timestamp=datetime.now(),
                memory_type=MemoryType.PROCEDURAL,
                priority=MemoryPriority.HIGH
            )
            memory_system_instance.long_term.core_knowledge[trace.trace_id] = trace

        logger.info("âœ… ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        logger.info(f"   - Ultra Short: {len(memory_system_instance.ultra_short.buffer)}ê°œ")
        logger.info(f"   - Short Term: {len(memory_system_instance.short_term.traces)}ê°œ")
        logger.info(f"   - Medium Term: {len(memory_system_instance.medium_term.traces)}ê°œ")
        logger.info(f"   - Long Term: {len(memory_system_instance.long_term.core_knowledge)}ê°œ")

    return memory_system_instance

# Load environment variables
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)

# Initialize OpenAI client
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = None
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print("âœ… OpenAI API initialized successfully")
    except Exception as e:
        print(f"âš ï¸ Failed to initialize OpenAI: {e}")
else:
    print("âš ï¸ No OpenAI API key found in .env")

protocol_router = None
# Protocol ì—”ë“œí¬ì¸íŠ¸ import
try:
    from protocol_endpoints import router as protocol_router
    PROTOCOL_ENABLED = True
except ImportError:
    protocol_router = None
    PROTOCOL_ENABLED = False
    print("âš ï¸ Protocol endpoints not available (protocol_endpoints.py not found)")

# âœ… FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="ê¸ˆê°• 2.0 ë°±ì—”ë“œ - ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì„œë²„")
# ğŸ” Global Approval Gate for WRITE routes (POST/PUT/PATCH/DELETE)
app.add_middleware(ApprovalGateMiddleware)

# âœ… CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²°ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.include_router(autocode_routes.router)
app.include_router(files_read_routes.router)
app.include_router(files_tree_routes.router)

# âœ… Timestamp Middleware - ëª¨ë“  ì‘ë‹µì— KST íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
@app.middleware("http")
async def timestamp_guard(request: Request, call_next):
    """ëª¨ë“  HTTP ì‘ë‹µì— KST íƒ€ì„ìŠ¤íƒ¬í”„ í—¤ë” ì¶”ê°€ ë° í˜•ì‹ ê²€ì¦"""
    response = await call_next(request)

    # í˜„ì¬ KST ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    ts = now_kr_str_minute()

    # ì‘ë‹µ í—¤ë”ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
    response.headers["X-Gumgang-Timestamp"] = ts
    response.headers["X-Gumgang-TZ"] = "Asia/Seoul"

    # íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ê²€ì¦
    if not validate_kr_timestamp(ts):
        raise HTTPException(
            status_code=422,
            detail=f"Invalid timestamp format: {ts}. Expected YYYY-MM-DD HH:mm"
        )

    return response

# âœ… Rules Enforcement Middleware - ëª¨ë“  LLM í˜¸ì¶œì— .rules ê°•ì œ ì£¼ì…
@app.middleware("http")
async def enforce_full_rules(request: Request, call_next):
    """ëª¨ë“  API ìš”ì²­ì— .rules ê°•ì œ ì£¼ì… ë° ê²€ì¦"""

    # POST ìš”ì²­ì´ë©° /api/* ë˜ëŠ” /ask, /ask/stream ê²½ë¡œì¸ ê²½ìš° Rules ì£¼ì…
    if request.method == "POST" and (request.url.path.startswith("/api/") or request.url.path in ["/ask", "/ask/stream"]):
        try:
            # Request body ì½ê¸°
            body_bytes = await request.body()

            # JSON íŒŒì‹± ì‹œë„
            try:
                body = json.loads(body_bytes) if body_bytes else {}
            except Exception:
                body = {}

            # prompt í•„ë“œê°€ ìˆëŠ” ê²½ìš° rules ì£¼ì…
            if "prompt" in body:
                text = (body.get("prompt") or "").lstrip()

                # ì´ë¯¸ rulesê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
                if not text.startswith(HEAD_MARK):
                    try:
                        # Rules ì£¼ì…
                        injected, rules_hash, head = prepend_full_rules(body["prompt"])
                        body["prompt"] = injected

                        # Request body ì¬ì„¤ì •
                        new_body = json.dumps(body).encode("utf-8")

                        # Request ì¬êµ¬ì„±ì„ ìœ„í•œ ìƒˆë¡œìš´ receive í•¨ìˆ˜
                        async def receive():
                            return {"type": "http.request", "body": new_body}

                        request = Request(request.scope, receive, request._send)

                        # Stateì— rules ì •ë³´ ì €ì¥
                        request.state.rules_hash = rules_hash
                        request.state.rules_head = head
                        request.state.rules_injected = True
                    except Exception as e:
                        return JSONResponse(
                            {"error": f"rules_enforcement_failed: {e}"},
                            status_code=500
                        )
        except Exception:
            # Body ì½ê¸° ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì†
            pass

    # ë‹¤ìŒ ë¯¸ë“¤ì›¨ì–´/í•¸ë“¤ëŸ¬ í˜¸ì¶œ
    response = await call_next(request)

    # Rules ì •ë³´ê°€ ìˆìœ¼ë©´ ì‘ë‹µ í—¤ë”ì— ì¶”ê°€
    if hasattr(request.state, "rules_hash"):
        response.headers["X-Rules-Hash"] = request.state.rules_hash
        response.headers["X-Rules-Head"] = "RULES v1.0 - Gumgang 2.0 / KST 2025-08-09 12:33"
        if hasattr(request.state, "rules_injected"):
            response.headers["X-Rules-Injected"] = "true"

    return response

# âœ… Protocol ë¼ìš°í„° í†µí•©
if PROTOCOL_ENABLED and protocol_router is not None:
    app.include_router(protocol_router)
    print("âœ… Protocol endpoints integrated successfully")

    # âœ… WebSocket ì—”ë“œí¬ì¸íŠ¸ ë° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì—°ê²°
    # - /ws ê²½ë¡œì— WebSocket ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
    app.add_api_websocket_route("/ws", websocket_endpoint)

    # - WebSocket ì„œë²„ì˜ heartbeat/cleanup ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
    @app.on_event("startup")
    async def _ws_start_background_tasks():
        try:
            await start_background_tasks()
        except Exception as e:
            logger.error(f"WebSocket tasks start failed: {e}")

    # - ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìƒíƒœ(tiers + ts_kst)ë¥¼ ëª¨ë“  í™œì„± ì—°ê²°ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸
    async def _memory_update_broadcaster():
        while True:
            try:
                memory_system = get_memory_system()
                tiers = from_simple_memory(memory_system)
                payload = {
                    "type": "memory-update",
                    "data": build_tiers_response(tiers)
                }
                # ëª¨ë“  í™œì„± ì—°ê²°ì— ì „ì†¡
                for connection_id in list(manager.active_connections.keys()):
                    await manager.send_personal_message(connection_id, payload)
            except Exception as e:
                logger.error(f"memory update broadcast failed: {e}")
            # 5ì´ˆ ì£¼ê¸°
            await asyncio.sleep(5)

    @app.on_event("startup")
    async def _start_memory_update_broadcaster():
        asyncio.create_task(_memory_update_broadcaster())

# âœ… í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ëª¨ë¸
class TestMessage(BaseModel):
    message: str
    timestamp: Optional[str] = None

class TaskRequest(BaseModel):
    task_id: str
    task_name: str
    status: str = "pending"
    progress: int = 0

class AskRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    code: Optional[str] = None
    language: Optional[str] = None
    file: Optional[str] = None

# âœ… í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "timestamp": now_kr_str_minute(),
        "service": "gumgang-backend",
        "version": "2.0-test"
    }

@app.get("/status")
async def status_check():
    """ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)"""
    try:
        # WebSocket ìƒíƒœ ê³„ì‚°: ì—°ê²° ìˆ˜ê°€ 1ê°œ ì´ìƒì´ë©´ on, 0ì´ë©´ ready, ì˜¤ë¥˜ ì‹œ off
        ws_connections = 0
        ws_state = "off"
        try:
            ws_connections = len(getattr(manager, "active_connections", {}) or {})
            ws_state = "on" if ws_connections > 0 else "ready"
        except Exception:
            ws_connections = 0
            ws_state = "off"

        # CPU ì‚¬ìš©ë¥  ê·¼ì‚¬ì¹˜(ì˜ì¡´ì„± ì—†ì´ loadavg/ì½”ì–´ìˆ˜ë¡œ ê³„ì‚°)
        try:
            load1 = os.getloadavg()[0]
            cores = os.cpu_count() or 1
            cpu_percent = max(0.0, min(100.0, (load1 / cores) * 100.0))
        except Exception:
            cpu_percent = 0.0

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(/proc/meminfo ê¸°ë°˜)
        mem_total = 0
        mem_used = 0
        try:
            total_kb = 0
            avail_kb = 0
            with open("/proc/meminfo", "r") as f:
                for line in f:
                    if line.startswith("MemTotal:"):
                        # e.g., "MemTotal:       32694724 kB"
                        parts = line.split()
                        if len(parts) >= 2:
                            total_kb = int(parts[1])
                    elif line.startswith("MemAvailable:"):
                        parts = line.split()
                        if len(parts) >= 2:
                            avail_kb = int(parts[1])
            mem_total = total_kb * 1024
            mem_used = max(0, mem_total - (avail_kb * 1024))
        except Exception:
            pass

        return {
            "status": "healthy",
            "timestamp": now_kr_str_minute(),
            "backend_port": 8000,
            "frontend_port": 3000,
            "websocket": ws_state,
            "ws_connections": ws_connections,
            "cpu_percent": cpu_percent,
            "memory_total": mem_total,
            "memory_used": mem_used
        }
    except Exception:
        # ë³´ìˆ˜ì  ê¸°ë³¸ê°’
        return {
            "status": "healthy",
            "timestamp": now_kr_str_minute(),
            "backend_port": 8000,
            "frontend_port": 3000,
            "websocket": "off",
            "ws_connections": 0,
            "cpu_percent": 0.0,
            "memory_total": 0,
            "memory_used": 0
        }

# âœ… Rules í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# Phase 3 â€” Usage summary/tail APIs
@app.get("/api/usage/summary")
async def usage_summary(session_id: str):
    try:
        from utils.usage_recorder import summarize_session  # type: ignore
        data = summarize_session(session_id)
        data["ts_kst"] = now_kr_str_minute()
        return data
    except Exception as e:
        return JSONResponse({"error": f"usage_summary_failed: {e}"}, status_code=500)

@app.get("/api/usage/tail")
async def usage_tail(lines: int = 20):
    try:
        from utils.usage_recorder import tail_usage  # type: ignore
        lines = max(1, min(1000, int(lines)))
        data = tail_usage(lines)
        return {"ts_kst": now_kr_str_minute(), "lines": data, "count": len(data)}
    except Exception as e:
        return JSONResponse({"error": f"usage_tail_failed: {e}"}, status_code=500)

# âœ… Rules í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸

# Phase 2 â€” Ideas APIs (Quick MVP)
class IdeaCaptureRequest(BaseModel):
    title: str
    body: str | None = ""
    tags: list[str] | None = None
    session_id: str | None = None
    actor: str | None = None

@app.post("/api/ideas/capture")
async def ideas_capture(payload: IdeaCaptureRequest):
    """
    Capture a lightweight idea as Markdown (docs/ideas) and append JSONL index (logs/ideas.jsonl).
    """
    try:
        if not payload.title or not payload.title.strip():
            return JSONResponse({"error": "title is required"}, status_code=400)
        from utils.ideas_recorder import capture_idea  # type: ignore
        res = capture_idea(
            title=payload.title.strip(),
            body=(payload.body or ""),
            tags=(payload.tags or []),
            session_id=payload.session_id,
            actor=payload.actor,
            meta={"route": "/api/ideas/capture"},
        )
        return {
            "ts_kst": res.ts_kst,
            "id": res.id,
            "path": res.path,
            "title": res.title,
            "tags": res.tags,
            "links": res.links,
            "written": res.written,
            "indexed": res.indexed,
        }
    except Exception as e:
        return JSONResponse({"error": f"ideas_capture_failed: {e}"}, status_code=500)

@app.get("/api/ideas/list")
async def ideas_list(limit: int = 20):
    """
    List recent ideas (metadata parsed from frontmatter). Limit 1-200.
    """
    try:
        from utils.ideas_recorder import list_recent_ideas  # type: ignore
        lim = max(1, min(200, int(limit)))
        items = list_recent_ideas(limit=lim)
        return {"ts_kst": now_kr_str_minute(), "items": items, "count": len(items)}
    except Exception as e:
        return JSONResponse({"error": f"ideas_list_failed: {e}"}, status_code=500)

# Blueprint index endpoint (headings + sha256, KST stamped)
@app.get("/api/blueprint/index")
async def blueprint_index():
    """
    Return a lightweight index of docs/blueprint_v1.2.md:
    - version, ts_kst, path, sha256
    - headings: [{level, title, line, anchor}, ...]
    """
    try:
        from utils.blueprint_indexer import build_index  # type: ignore
        repo_root = Path(__file__).resolve().parent.parent
        md_path = repo_root / "docs" / "blueprint_v1.2.md"
        index = build_index(md_path)
        return index
    except Exception as e:
        return JSONResponse({"error": f"blueprint_index_failed: {e}"}, status_code=500)

# Turn Prompt â€” Git dirty endpoint
@app.get("/api/protocol/git/dirty")
async def protocol_git_dirty():
    """
    Return current git working tree dirty status for Turn-Checkpoint Prompt.
    Uses `git status --porcelain` to classify changes.
    """
    try:
        import subprocess
        repo_root = Path(__file__).resolve().parent.parent  # project root
        cp = subprocess.run(
            ["git", "status", "--porcelain"],
            cwd=str(repo_root),
            capture_output=True,
            text=True,
            check=False,
        )
        raw = [ln.strip() for ln in (cp.stdout or "").splitlines() if ln.strip()]
        modified, added, deleted, renamed, untracked = [], [], [], [], []

        for ln in raw:
            # Status format: XY<space>path (e.g., " M file", "A  file", "D  file", "?  file")
            code = ln[:2]
            rest = ln[3:] if len(ln) > 3 else ""
            # Rename format example: "R  old/path -> new/path"
            if "->" in rest:
                parts = [p.strip() for p in rest.split("->", 1)]
                if len(parts) == 2:
                    renamed.append({"from": parts[0], "to": parts[1]})
                    continue

            path = rest
            if "?" in code:
                untracked.append(path)
            elif "A" in code:
                added.append(path)
            elif "D" in code:
                deleted.append(path)
            elif "M" in code or code.strip():
                # Treat any other non-empty status as modified
                modified.append(path)

        return {
            "ts_kst": now_kr_str_minute(),
            "has_changes": bool(raw),
            "counts": {
                "modified": len(modified),
                "added": len(added),
                "deleted": len(deleted),
                "renamed": len(renamed),
                "untracked": len(untracked),
            },
            "modified": modified,
            "added": added,
            "deleted": deleted,
            "renamed": renamed,
            "untracked": untracked,
        }
    except Exception as e:
        return JSONResponse({"error": f"git_dirty_failed: {e}"}, status_code=500)

# Phase 2 â€” Protocol Guard Checkpoint/Audit APIs
class CheckpointRequest(BaseModel):
    task_id: str
    operation: str = "edit"
    paths: list[str] | None = None
    notes: str
    risk: str = "SAFE"
    actor: str = "gpt-5"


@app.post("/api/protocol/checkpoint")
async def protocol_checkpoint(payload: CheckpointRequest):
    """
    Record a checkpoint/audit entry via server-side guard recorder.
    - Uses KST timestamp (now_kr_str_minute)
    - Dedups by (task_id, operation, path)
    """
    try:
        # Import locally to avoid global import churn if module not needed at runtime
        from utils.guard_recorder import record  # type: ignore

        paths = payload.paths or []
        notes = payload.notes.strip()
        if not payload.task_id.strip():
            return JSONResponse({"error": "task_id is required"}, status_code=400)
        if not notes:
            return JSONResponse({"error": "notes is required"}, status_code=400)

        appended, modified = record(
            task_id=payload.task_id.strip(),
            operation=payload.operation.strip() or "edit",
            paths=paths,
            notes=notes,
            message=notes,
            risk=payload.risk.strip() or "SAFE",
            actor=payload.actor.strip() or "gpt-5",
        )

        return {
            "ts_kst": now_kr_str_minute(),
            "task_id": payload.task_id,
            "operation": payload.operation,
            "paths": paths,
            "audit_appended": appended,
            "manifest_modified": modified,
        }
    except Exception as e:
        return JSONResponse({"error": f"checkpoint_failed: {e}"}, status_code=500)


@app.get("/api/protocol/audit/tail")
async def protocol_audit_tail(lines: int = 200):
    """
    Return the last N lines of guard audit log.
    """
    try:
        from utils.guard_recorder import audit_tail  # type: ignore

        lines = max(1, min(1000, int(lines)))
        tail = audit_tail(lines=lines)
        return {
            "ts_kst": now_kr_str_minute(),
            "lines": tail,
            "count": len(tail),
        }
    except Exception as e:
        return JSONResponse({"error": f"audit_tail_failed: {e}"}, status_code=500)
@app.post("/api/test")
async def test_rules_injection(request: Request, data: Optional[dict] = None):
    """Rules ì£¼ì… í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    # Check if rules were injected
    rules_injected = hasattr(request.state, "rules_injected") and request.state.rules_injected
    rules_hash = getattr(request.state, "rules_hash", None)
    rules_head = getattr(request.state, "rules_head", None)

    # Get prompt from request
    prompt = data.get("prompt", "") if data else ""

    return {
        "status": "ok",
        "rules_enforced": rules_injected,
        "rules_hash": rules_hash,
        "rules_head": rules_head,
        "prompt_starts_with_rules": prompt.startswith(HEAD_MARK) if prompt else False,
        "timestamp": now_kr_str_minute()
    }

# âœ… ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "message": "ê¸ˆê°• 2.0 ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ ì„œë²„",
        "status": "running",
        "endpoints": [
            "/health",
            "/ask",
            "/api/test",
            "/api/echo",
            "/api/tasks",
            "/api/dashboard/stats"
        ]
    }

# âœ… /ask ì—”ë“œí¬ì¸íŠ¸ - AI ì–´ì‹œìŠ¤í„´íŠ¸ìš©
@app.post("/ask")
async def ask_question(request: AskRequest):
    """AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - OpenAI GPT ì—°ë™ + ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"""
    import uuid

    # ì„¸ì…˜ ID ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
    session_id = request.session_id or str(uuid.uuid4())

    # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    memory_system = get_memory_system()
    memory_context = []

    # ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
    try:
        # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
        memory_context.append("ì‚¬ìš©ì ì •ë³´: ë•ì‚°(duksan) - Gumgang 2.0 í”„ë¡œì íŠ¸ ê°œë°œì")
        memory_context.append("í”„ë¡œì íŠ¸: Gumgang 2.0 - Blueprint v1.2 ê¸°ë°˜ ìë¦½í˜• AI ì½”ë“œ ì—ë””í„°")
        memory_context.append("í•µì‹¬ ì›ì¹™: .rules ë¬¸ì„œ ë¶ˆê°€ì¹¨")

        # ìµœê·¼ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if hasattr(memory_system, 'short_term') and memory_system.short_term.traces:
            recent_memories = list(memory_system.short_term.traces.values())[:3]
            for mem in recent_memories:
                if hasattr(mem, 'content'):
                    memory_context.append(f"ìµœê·¼ ê¸°ì–µ: {mem.content}")

        # ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ì¤‘ìš” ì •ë³´
        if hasattr(memory_system, 'long_term') and memory_system.long_term.core_knowledge:
            core_knowledge = list(memory_system.long_term.core_knowledge.values())[:2]
            for knowledge in core_knowledge:
                if hasattr(knowledge, 'content'):
                    memory_context.append(f"í•µì‹¬ ì§€ì‹: {knowledge.content}")
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # OpenAI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not openai_client:
        # Fallback to dummy response if no API key
        return {
            "response": f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] '{request.query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤. OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "source": "test-backend",
            "session_id": session_id,
            "timestamp": now_kr_str_minute(),
            "context_info": {
                "session_id": session_id,
                "recent_interactions": 0,
                "context_type": "test"
            },
            "suggest_ingest": False
        }

    try:
        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        memory_info = "\n".join(memory_context) if memory_context else "ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."

        system_prompt = f"""You are ê¸ˆê°• 2.0, an advanced AI coding assistant with a 5-layer temporal memory system.

í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ:
{memory_info}

ë‹¹ì‹ ì€ ë•ì‚°(duksan)ì´ë¼ëŠ” ê°œë°œìì™€ í•¨ê»˜ Gumgang 2.0 í”„ë¡œì íŠ¸ë¥¼ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.
- ì‚¬ìš©ì ì´ë¦„: ë•ì‚° (duksan)
- í”„ë¡œì íŠ¸: Gumgang 2.0 (Blueprint v1.2 ê¸°ë°˜)
- ì•„í‚¤í…ì²˜: Tauri + Next.js + Monaco Editor
- ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ: 4ê³„ì¸µ ì‹œê°„ì  ë©”ëª¨ë¦¬ (ì´ˆë‹¨ê¸°, ë‹¨ê¸°, ì¤‘ê¸°, ì¥ê¸°) + ë©”íƒ€ì¸ì§€

You have persistent memory and remember all previous interactions.
You are helpful, precise, and capable of understanding both Korean and English at an expert level.
When providing code, always use proper markdown formatting."""

        # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # ì½”ë“œê°€ í¬í•¨ëœ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if request.code:
            code_context = f"\n\në‹¤ìŒ ì½”ë“œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤:\n```{request.language or 'python'}\n{request.code}\n```"
            messages.append({"role": "user", "content": request.query + code_context})
        else:
            messages.append({"role": "user", "content": request.query})

        # OpenAI API í˜¸ì¶œ
        print(f"ğŸ¤– Calling OpenAI API for query: {request.query[:50]}...")
        print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸: {len(memory_context)}ê°œ í•­ëª© ë¡œë“œë¨")

        # GPT-5 ëª¨ë¸ ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥)
        model_name = os.getenv("OPENAI_MODEL", "gpt-5")  # ê¸°ë³¸ê°’: GPT-5
        logger.info(f"ğŸš€ Using OpenAI model: {model_name}")

        # GPT-5 API íŒŒë¼ë¯¸í„° ì„¤ì •
        api_params = {
            "model": model_name,
            "messages": messages,
            "stream": False  # ìŠ¤íŠ¸ë¦¬ë°ì€ ë‚˜ì¤‘ì— êµ¬í˜„
        }

        if "gpt-5" in model_name.lower():
            api_params["max_completion_tokens"] = 120000

        elif "gpt-4" in model_name.lower():
            api_params["max_tokens"] = 2000
            api_params["temperature"] = 0.7
        else:
            api_params["max_tokens"] = 2000
            api_params["temperature"] = 0.7

        response = openai_client.chat.completions.create(**api_params)

        # ì‘ë‹µ ì¶”ì¶œ
        response_text = response.choices[0].message.content

        print(f"âœ… OpenAI response received: {len(response_text)} chars")

        # ì‚¬ìš©ëŸ‰ ê¸°ë¡(ì‘ë‹µ usage ìš°ì„ , ì—†ìœ¼ë©´ ê·¼ì‚¬ê°’)
        computed_tokens = None
        try:
            from utils.usage_recorder import record_from_openai_usage, approximate_tokens_from_text, record_usage  # type: ignore
            import uuid as _uuid
            turn_id = str(_uuid.uuid4())
            if getattr(response, "usage", None):
                rec = record_from_openai_usage(
                    session_id=session_id,
                    turn_id=turn_id,
                    model=model_name,
                    usage=response.usage,
                    meta={"route": "/ask"},
                )
                computed_tokens = rec.total_tokens
            else:
                p = approximate_tokens_from_text(request.query or "")
                c = approximate_tokens_from_text(response_text or "")
                rec = record_usage(
                    session_id=session_id,
                    turn_id=turn_id,
                    model=model_name,
                    prompt_tokens=p,
                    completion_tokens=c,
                    meta={"route": "/ask", "approx": True},
                )
                computed_tokens = rec.total_tokens
        except Exception as e:
            logger.warning(f"ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")

        # ëŒ€í™” ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
        try:
            from app.temporal_memory import MemoryType, MemoryPriority
            memory_system.store_memory(
                f"Q: {request.query[:100]}... A: {response_text[:100]}...",
                MemoryType.EPISODIC,
                MemoryPriority.MEDIUM,
                session_id=session_id
            )
            logger.info("âœ… ëŒ€í™” ë‚´ìš© ë©”ëª¨ë¦¬ì— ì €ì¥ë¨")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

        return {
            "response": response_text,
            "source": f"openai-{model_name}",
            "session_id": session_id,
            "context_info": {
                "session_id": session_id,
                "recent_interactions": len(memory_context),
                "context_type": "openai-with-memory",
                "model": model_name.upper(),
                "tokens_used": computed_tokens,
                "memory_loaded": len(memory_context) > 0,
                "capabilities": "4-layer temporal memory system with GPT-5 intelligence",
                "model_version": model_name,
                "released": "2025-08-08"  # GPT-5 ì •ì‹ ì¶œì‹œì¼
            },
            "suggest_ingest": False
        }

    except Exception as e:
        print(f"âŒ OpenAI API error: {e}")
        # ì—ëŸ¬ ë°œìƒì‹œ í´ë°± ì‘ë‹µ
        return {
            "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜: {str(e)}",
            "source": "error-fallback",
            "session_id": session_id,
            "context_info": {
                "session_id": session_id,
                "recent_interactions": 0,
                "context_type": "error",
                "error": str(e)
            },
            "suggest_ingest": False
        }

# âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—”ë“œí¬ì¸íŠ¸ - Server-Sent Events (SSE)
@app.post("/ask/stream")
async def ask_question_stream(request: AskRequest):
    """AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ - OpenAI GPT ìŠ¤íŠ¸ë¦¬ë°"""
    import uuid

    # ì„¸ì…˜ ID ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
    session_id = request.session_id or str(uuid.uuid4())

    async def generate_stream() -> AsyncGenerator[str, None]:
        """SSE í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""

        # OpenAI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not openai_client:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë°
            test_response = f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] '{request.query}'ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì…ë‹ˆë‹¤."
            for char in test_response:
                yield f"data: {json.dumps({'content': char, 'type': 'content'})}\n\n"
                await asyncio.sleep(0.01)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            yield f"data: {json.dumps({'type': 'done', 'session_id': session_id})}\n\n"
            return

        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = """You are ê¸ˆê°• 2.0, an advanced AI coding assistant powered by GPT-5 with a 5-layer memory system.
You have dramatically improved reasoning capabilities, near-human cognitive abilities, and persistent memory.
You are helpful, precise, and capable of understanding both Korean and English at an expert level.
When providing code, always use proper markdown formatting with exceptional accuracy.
You have PhD-level knowledge of programming, software architecture, and best practices."""

            # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt}
            ]

            # ì½”ë“œê°€ í¬í•¨ëœ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if request.code:
                code_context = f"\n\në‹¤ìŒ ì½”ë“œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤:\n```{request.language or 'python'}\n{request.code}\n```"
                messages.append({"role": "user", "content": request.query + code_context})
            else:
                messages.append({"role": "user", "content": request.query})

            # ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡
            yield f"data: {json.dumps({'type': 'start', 'session_id': session_id})}\n\n"

            # OpenAI API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            print(f"ğŸ¤– Starting streaming response for: {request.query[:50]}...")

            stream = openai_client.chat.completions.create(
                model="gpt-5",  # GPT-5 ì •ì‹ ëª¨ë¸ (2025ë…„ 8ì›” 7ì¼ ì¶œì‹œ)
                messages=messages,  # type: ignore[arg-type]
                max_completion_tokens=2000,  # GPT-5ëŠ” max_completion_tokens ì‚¬ìš©
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡
            full_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    # ê° ì²­í¬ë¥¼ SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                    yield f"data: {json.dumps({'content': content, 'type': 'content'})}\n\n"

            # ì‚¬ìš©ëŸ‰ ê¸°ë¡(ê·¼ì‚¬ì¹˜)
            try:
                from utils.usage_recorder import approximate_tokens_from_text, record_usage  # type: ignore
                import uuid as _uuid
                p = approximate_tokens_from_text(request.query or "")
                c = approximate_tokens_from_text(full_response or "")
                record_usage(
                    session_id=session_id,
                    turn_id=str(_uuid.uuid4()),
                    model="gpt-5",
                    prompt_tokens=p,
                    completion_tokens=c,
                    meta={"route": "/ask/stream"},
                )
            except Exception as e:
                print(f"âš ï¸ Usage record failed: {e}")

            # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
            yield f"data: {json.dumps({'type': 'done', 'session_id': session_id, 'full_response': full_response})}\n\n"
            print(f"âœ… Streaming completed: {len(full_response)} chars")

        except Exception as e:
            print(f"âŒ Streaming error: {e}")
            # ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡
            yield f"data: {json.dumps({'type': 'error', 'error': str(e), 'session_id': session_id})}\n\n"

    # StreamingResponseë¡œ SSE ì‘ë‹µ ë°˜í™˜
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )

# âœ… ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ë“¤


# âœ… í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/test")
async def test_endpoint():
    return {
        "status": "success",
        "message": "ë°±ì—”ë“œ ì—°ê²° ì„±ê³µ!",
        "timestamp": now_kr_str_minute()
    }

# âœ… Echo ì—”ë“œí¬ì¸íŠ¸ (POST í…ŒìŠ¤íŠ¸ìš©)
@app.post("/api/echo")
async def echo_message_post(msg: TestMessage):
    return {
        "status": "success",
        "echo": msg.message,
        "received_at": now_kr_str_minute(),
        "original_timestamp": msg.timestamp
    }

# âœ… Task ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/tasks")
async def get_tasks():
    # ë”ë¯¸ íƒœìŠ¤í¬ ë°ì´í„°
    return {
        "status": "success",
        "tasks": [
            {
                "task_id": "GG-20250108-001",
                "task_name": "ì„¸ì…˜ ë§¤ë‹ˆì € êµ¬ì¶•",
                "status": "completed",
                "progress": 100
            },
            {
                "task_id": "GG-20250108-002",
                "task_name": "Task ì¶”ì  ì‹œìŠ¤í…œ",
                "status": "completed",
                "progress": 100
            },
            {
                "task_id": "GG-20250108-003",
                "task_name": "í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™",
                "status": "completed",
                "progress": 100
            },
            {
                "task_id": "GG-20250108-005",
                "task_name": "ë°±ì—”ë“œ ì•ˆì •í™”",
                "status": "in_progress",
                "progress": 50
            }
        ],
        "total": 4,
        "completed": 3,
        "in_progress": 1
    }

@app.post("/api/tasks")
async def create_task(task: TaskRequest):
    return {
        "status": "success",
        "message": f"Task {task.task_id} created",
        "task": task.dict()
    }

# âœ… ëŒ€ì‹œë³´ë“œ í†µê³„ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/dashboard/stats")
async def dashboard_stats():
    return {
        "status": "success",
        "stats": {
            "total_files": 1247,
            "total_lines": 45892,
            "active_sessions": 1,
            "memory_usage": {
                "sensory": 15,
                "working": 8,
                "episodic": 42,
                "semantic": 156
            },
            "system_health": "optimal",
            "last_update": datetime.datetime.now().isoformat()
        }
    }

# âœ… íŒŒì¼ êµ¬ì¡° ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/structure")
async def get_structure():
    return {
        "status": "success",
        "structure": {
            "frontend": {
                "path": "/gumgang-v2",
                "framework": "Next.js 15",
                "status": "active"
            },
            "backend": {
                "path": "/backend",
                "framework": "FastAPI",
                "status": "running"
            },
            "memory": {
                "path": "/memory",
                "type": "4-layer temporal system",
                "status": "initialized"
            }
        }
    }

# âœ… ë©”ëª¨ë¦¬ ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ (ë”ë¯¸)
@app.get("/api/memory/status")
async def memory_status():
    return {
        "status": "success",
        "memory": {
            "layers": {
                "sensory": {"capacity": 100, "used": 15},
                "working": {"capacity": 50, "used": 8},
                "episodic": {"capacity": 500, "used": 42},
                "semantic": {"capacity": 1000, "used": 156}
            },
            "total_memories": 221,
            "last_cleanup": now_kr_str_minute()
        }
    }

# âœ… í”„ë¡ íŠ¸ì—”ë“œìš© ë©”ëª¨ë¦¬ ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ (ì‹¤ì œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì—°ê²°)
@app.get("/memory/status")
async def frontend_memory_status():
    """í”„ë¡ íŠ¸ì—”ë“œ MemoryStatus ì»´í¬ë„ŒíŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì „ì—­ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        memory_system = get_memory_system()

        # ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì¤€ tiers ê³„ì‚°
        from app.utils.memory_status_adapter import build_tiers_response, from_simple_memory
        tiers = from_simple_memory(memory_system)
        return build_tiers_response(tiers)

    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        # í‘œì¤€ ìŠ¤í‚¤ë§ˆì˜ ê¸°ë³¸ê°’ ë°˜í™˜
        from utils.time_kr import now_kr_str_minute
        return {
            "tiers": {
                "ultra_short": 0,
                "short_term": 0,
                "medium_term": 0,
                "long_term": 0,
                "meta": 0
            },
            "ts_kst": now_kr_str_minute()
        }

# âœ… ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
@app.get("/memory/search")
async def search_memory(query: Optional[str] = None):
    """ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì „ì—­ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        memory_system = get_memory_system()

        if not query:
            return {
                "query": "",
                "results": [],
                "count": 0,
                "status": "no_query"
            }

        # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì˜ retrieve_memories ë©”ì„œë“œ ì‚¬ìš© (í‘œì¤€ ë§µí•‘)
        results = memory_system.retrieve_memories(query, max_results=10)

        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        layer_to_level = {
            "ultra_short": 1,
            "short_term": 2,
            "medium_term": 3,
            "long_term": 4,
        }
        for result in results or []:
            trace = result.get("trace") if isinstance(result, dict) else None
            content = getattr(trace, "content", "") if trace is not None else ""
            ts = getattr(trace, "timestamp", None) if trace is not None else None
            timestamp_str = str(ts) if ts else now_kr_str_minute()
            layer = result.get("layer") if isinstance(result, dict) else None
            level = layer_to_level.get(str(layer), 1)
            relevance = float(result.get("relevance", 0.0)) if isinstance(result, dict) else 0.0
            formatted_results.append({
                "content": content,
                "level": level,
                "timestamp": timestamp_str,
                "relevance": relevance
            })

        return {
            "query": query,
            "results": formatted_results,
            "count": len(formatted_results),
            "status": "success"
        }

    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return {
            "query": query or "",
            "results": [],
            "count": 0,
            "status": "error",
            "error": str(e)
        }

# âœ… ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={
            "status": "error",
            "message": "Endpoint not found",
            "path": str(request.url.path)
        }
    )

@app.exception_handler(500)
async def internal_error_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={
            "status": "error",
            "message": "Internal server error",
            "detail": str(exc)
        }
    )

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ê¸ˆê°• 2.0 ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹œì‘...")
    print("ğŸ“ http://localhost:8000")
    print("ğŸ“Š ëŒ€ì‹œë³´ë“œ: http://localhost:3000")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
