# ğŸ“ backend/scripts/ingest_chat_ui_log.py

import os
import json
from datetime import datetime
from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../.env"))

# âœ… ê²½ë¡œ ì •ì˜
CHAT_LOG_DIR = "./memory/chat_logs"
VECTOR_DB_DIR = "./memory/vectors/chatgpt_memory"

def load_chat_records():
    documents = []
    total = 0
    skipped = 0

    for filename in os.listdir(CHAT_LOG_DIR):
        if filename.endswith(".json"):
            filepath = os.path.join(CHAT_LOG_DIR, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                try:
                    records = json.load(f)
                    for record in records:
                        question = record.get("question", "").strip()
                        answer = record.get("answer", "").strip()
                        timestamp = record.get("timestamp", "")
                        if not question or not answer:
                            skipped += 1
                            continue

                        metadata = {
                            "answer": answer,
                            "timestamp": timestamp,
                            "source": filename
                        }
                        documents.append(Document(page_content=question, metadata=metadata))
                        total += 1
                except Exception as e:
                    print(f"âš ï¸ {filename} ì½ê¸° ì‹¤íŒ¨: {e}")
    return documents, total, skipped

def main():
    print("ğŸ”„ ëŒ€í™” ë¡œê·¸ ì¸ê²ŒìŠ¤íŠ¸ ì‹œì‘...")

    records, total, skipped = load_chat_records()
    if not records:
        print("âš ï¸ ì¸ê²ŒìŠ¤íŠ¸í•  ìœ íš¨í•œ ëŒ€í™” ì—†ìŒ.")
        return

    # âœ… í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.split_documents(records)

    # âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì €ì¥
    db = Chroma.from_documents(
        documents=docs,
        embedding=OpenAIEmbeddings(),
        persist_directory=VECTOR_DB_DIR
    )
    db.persist()

    print(f"âœ… ì¸ê²ŒìŠ¤íŠ¸ ì™„ë£Œ: ì´ {total}ê±´ (ë¬´ì‹œëœ {skipped}ê±´) â†’ {VECTOR_DB_DIR}")

if __name__ == "__main__":
    main()
