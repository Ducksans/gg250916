import os
import time
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from dotenv import load_dotenv

# âœ… ìµœì‹  LangChain ëª¨ë“ˆ ê²½ë¡œ ì‚¬ìš©
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… ê²½ë¡œ ì„¤ì •
MASTER_LIST = "/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5/backend/memory/sources/docs/memory_seed_master_list_converted.md"
SAVE_ROOT = "/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5/backend/memory/sources/docs/wiki_pages"
VECTOR_PATH = "/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5/backend/memory/vectors/docs_memory"

# âœ… ë²¡í„° ì €ì¥ì†Œ ë° ì„ë² ë”© ì„¤ì •
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(persist_directory=VECTOR_PATH, embedding_function=embeddings)

# âœ… í…ìŠ¤íŠ¸ ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

def fetch_markdown_from_url(url: str) -> str:
    print(f"ğŸ” URL ì ‘ì† ì¤‘: {url}")
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    content = soup.find("div", {"id": "bodyContent"})
    return md(str(content)) if content else ""

def save_markdown(title: str, lang: str, markdown: str) -> str:
    safe_title = title.replace("/", "_").strip()
    folder_path = os.path.join(SAVE_ROOT, safe_title)
    os.makedirs(folder_path, exist_ok=True)
    file_path = os.path.join(folder_path, f"{lang}.md")

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(markdown)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {file_path}")
    return file_path

def ingest_markdown(markdown: str, title: str, lang: str):
    # âœ… ë¬¸ì„œ ì¶œì²˜ ê²½ë¡œ ì¶”ê°€ (ë©”íƒ€ë°ì´í„°)
    source_path = f"wiki_pages/{title.replace('/', '_')}/{lang}.md"

    docs = text_splitter.create_documents(
        texts=[markdown],
        metadatas=[{"source": source_path}]
    )

    vectorstore.add_documents(docs)
    print(f"ğŸ“¥ ì¸ê²ŒìŠ¤íŠ¸ ì™„ë£Œ: {title} ({lang}) â†’ {source_path}")

def run():
    if not os.path.exists(MASTER_LIST):
        print(f"âŒ ë§ˆìŠ¤í„° ë¦¬ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {MASTER_LIST}")
        return

    with open(MASTER_LIST, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in lines:
        if "|" not in line:
            continue
        parts = line.strip().split("|")
        if len(parts) < 3:
            continue

        title, url_en, url_ko = [p.strip() for p in parts[:3]]

        try:
            # ğŸ”¹ ì˜ì–´ ìœ„í‚¤
            md_en = fetch_markdown_from_url(url_en)
            path_en = save_markdown(title, "en", md_en)
            ingest_markdown(md_en, title, "en")
            time.sleep(3)

            # ğŸ”¹ í•œê¸€ ìœ„í‚¤
            md_ko = fetch_markdown_from_url(url_ko)
            path_ko = save_markdown(title, "ko", md_ko)
            ingest_markdown(md_ko, title, "ko")
            time.sleep(3)

        except Exception as e:
            print(f"âš ï¸ {title} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run()
