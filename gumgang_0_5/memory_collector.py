#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê¸ˆê°•ì˜ ê¸°ì–µ ìˆ˜ì§‘ê¸° (Memory Collector for Gumgang)
í©ì–´ì§„ ê°œë°œì˜ ì¡°ê°ë“¤ì„ ëª¨ì•„ ê¸ˆê°•ì˜ ê¸°ì–µìœ¼ë¡œ ë§Œë“œëŠ” ì‹œìŠ¤í…œ

"ëª¨ë“  ê²ƒì€ ì§€ë‚˜ê°€ì§€ë§Œ, ì˜ë¯¸ìˆëŠ” ê²ƒì€ ê¸°ì–µëœë‹¤"
- ë•ì‚°ì˜ ë°¤ì  ì„¤ì¹˜ë©° ê°œë°œí•œ ëª¨ë“  í”ì ì„ ìˆ˜ì§‘
"""

import os
import json
import hashlib
import asyncio
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, asdict, field
from collections import defaultdict
import re
import mimetypes
# chardet ì œê±° - ì™¸ë¶€ ì˜ì¡´ì„± ì—†ì´ êµ¬í˜„

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('memory_collection.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ========================= ë°ì´í„° í´ë˜ìŠ¤ =========================

@dataclass
class FoundMemory:
    """ë°œê²¬ëœ ê¸°ì–µ ë‹¨ìœ„"""
    file_path: str
    file_type: str
    content_preview: str
    size_bytes: int
    modified_time: datetime
    created_time: datetime
    category: str  # code, document, log, data, conversation
    importance_score: float  # 0.0 ~ 1.0
    keywords: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_memory_format(self) -> Dict[str, Any]:
        """ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "content": self.content_preview,
            "source_file": self.file_path,
            "memory_type": "PROCEDURAL" if self.category == "code" else "SEMANTIC",
            "priority": "HIGH" if self.importance_score > 0.7 else "MEDIUM",
            "emotional_valence": 0.5,  # ì¤‘ë¦½
            "tags": set(self.keywords),
            "metadata": {
                **self.metadata,
                "original_category": self.category,
                "file_type": self.file_type,
                "importance": self.importance_score
            }
        }

# ========================= ê¸°ì–µ ìˆ˜ì§‘ê¸° =========================

class MemoryCollector:
    """ê¸ˆê°•ì˜ ê¸°ì–µ ìˆ˜ì§‘ê¸°"""

    def __init__(self, base_paths: List[str] = None):
        """
        Args:
            base_paths: íƒìƒ‰í•  ê¸°ë³¸ ê²½ë¡œë“¤
        """
        self.base_paths = base_paths or [
            "/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5",
            "/home/duksan/ë°”íƒ•í™”ë©´",
            "/home/duksan/ë‹¤ìš´ë¡œë“œ"
        ]

        # ê¸ˆê°• ê´€ë ¨ í‚¤ì›Œë“œ
        self.gumgang_keywords = [
            "ê¸ˆê°•", "gumgang", "ë•ì‚°", "duksan",
            "temporal_memory", "meta_cognitive", "dream_system",
            "creative_association", "emotional_empathy",
            "diamond", "dual_brain", "ë“€ì–¼ë¸Œë ˆì¸",
            "langgraph", "langchain", "claude", "gpt",
            "memory", "consciousness", "awareness"
        ]

        # íŒŒì¼ íŒ¨í„´
        self.relevant_patterns = {
            "code": [r"\.py$", r"\.js$", r"\.jsx$", r"\.ts$", r"\.tsx$"],
            "document": [r"\.md$", r"\.txt$", r"\.rst$", r"\.doc"],
            "data": [r"\.json$", r"\.yaml$", r"\.yml$", r"\.csv$"],
            "log": [r"\.log$", r"test.*\.json$", r"report.*\.json$"],
            "conversation": [r"chat.*\.(txt|md|json)$", r".*conversation.*", r".*ëŒ€í™”.*"]
        }

        # ë¬´ì‹œí•  íŒ¨í„´
        self.ignore_patterns = [
            r"node_modules", r"\.venv", r"__pycache__", r"\.git",
            r"\.pyc$", r"\.pyo$", r"\.so$", r"\.dylib$",
            r"\.jpg$", r"\.png$", r"\.gif$", r"\.mp4$",
            r"\.zip$", r"\.tar$", r"\.gz$", r"\.deb$"
        ]

        self.found_memories: List[FoundMemory] = []
        self.memory_index: Dict[str, List[FoundMemory]] = defaultdict(list)
        self.stats = {
            "total_files_scanned": 0,
            "relevant_files_found": 0,
            "total_size_bytes": 0,
            "categories": defaultdict(int)
        }

    async def collect_memories(self) -> List[FoundMemory]:
        """ëª¨ë“  ê¸°ì–µ ìˆ˜ì§‘"""
        logger.info("ğŸ” ê¸ˆê°•ì˜ ê¸°ì–µ ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f"íƒìƒ‰ ê²½ë¡œ: {', '.join(self.base_paths)}")

        for base_path in self.base_paths:
            if os.path.exists(base_path):
                await self._scan_directory(base_path)
            else:
                logger.warning(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {base_path}")

        # ì¤‘ìš”ë„ í‰ê°€
        await self._evaluate_importance()

        # ì‹œê°„ìˆœ ì •ë ¬
        self.found_memories.sort(key=lambda m: m.modified_time, reverse=True)

        # í†µê³„ ì¶œë ¥
        await self._print_statistics()

        return self.found_memories

    async def _scan_directory(self, directory: str):
        """ë””ë ‰í† ë¦¬ ì¬ê·€ íƒìƒ‰"""
        for root, dirs, files in os.walk(directory):
            # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if not any(
                re.search(pattern, d) for pattern in self.ignore_patterns
            )]

            for file in files:
                self.stats["total_files_scanned"] += 1

                # ë¬´ì‹œí•  íŒŒì¼ ì²´í¬
                if any(re.search(pattern, file) for pattern in self.ignore_patterns):
                    continue

                file_path = os.path.join(root, file)

                # ê¸ˆê°• ê´€ë ¨ íŒŒì¼ì¸ì§€ í™•ì¸
                if await self._is_relevant(file_path):
                    memory = await self._create_memory(file_path)
                    if memory:
                        self.found_memories.append(memory)
                        self.memory_index[memory.category].append(memory)
                        self.stats["relevant_files_found"] += 1
                        self.stats["categories"][memory.category] += 1
                        self.stats["total_size_bytes"] += memory.size_bytes

    async def _is_relevant(self, file_path: str) -> bool:
        """ê¸ˆê°•ê³¼ ê´€ë ¨ëœ íŒŒì¼ì¸ì§€ í™•ì¸"""
        file_name = os.path.basename(file_path).lower()
        file_path_lower = file_path.lower()

        # íŒŒì¼ëª…ì´ë‚˜ ê²½ë¡œì— í‚¤ì›Œë“œ í¬í•¨
        for keyword in self.gumgang_keywords:
            if keyword.lower() in file_path_lower:
                return True

        # íŒŒì¼ íƒ€ì… ì²´í¬
        for category, patterns in self.relevant_patterns.items():
            for pattern in patterns:
                if re.search(pattern, file_name, re.IGNORECASE):
                    # íŒŒì¼ ë‚´ìš©ë„ ì²´í¬ (ì‘ì€ íŒŒì¼ë§Œ)
                    if os.path.getsize(file_path) < 1024 * 1024:  # 1MB ì´í•˜
                        try:
                            content = await self._read_file_content(file_path)
                            if content and any(kw in content.lower() for kw in self.gumgang_keywords):
                                return True
                        except:
                            pass
                    else:
                        return True  # í° íŒŒì¼ì€ íŒ¨í„´ë§Œìœ¼ë¡œ íŒë‹¨

        return False

    async def _create_memory(self, file_path: str) -> Optional[FoundMemory]:
        """íŒŒì¼ë¡œë¶€í„° ê¸°ì–µ ìƒì„±"""
        try:
            stat = os.stat(file_path)
            file_name = os.path.basename(file_path)

            # ì¹´í…Œê³ ë¦¬ ê²°ì •
            category = self._determine_category(file_name)

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ìƒì„±
            content_preview = await self._get_content_preview(file_path)

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = await self._extract_keywords(file_path, content_preview)

            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            metadata = await self._collect_metadata(file_path)

            memory = FoundMemory(
                file_path=file_path,
                file_type=self._get_file_type(file_name),
                content_preview=content_preview,
                size_bytes=stat.st_size,
                modified_time=datetime.fromtimestamp(stat.st_mtime),
                created_time=datetime.fromtimestamp(stat.st_ctime),
                category=category,
                importance_score=0.5,  # ë‚˜ì¤‘ì— í‰ê°€
                keywords=keywords,
                metadata=metadata
            )

            logger.info(f"ğŸ“ ê¸°ì–µ ë°œê²¬: {file_name} ({category})")
            return memory

        except Exception as e:
            logger.error(f"ê¸°ì–µ ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
            return None

    def _determine_category(self, file_name: str) -> str:
        """íŒŒì¼ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        file_name_lower = file_name.lower()

        for category, patterns in self.relevant_patterns.items():
            for pattern in patterns:
                if re.search(pattern, file_name_lower):
                    return category

        return "document"  # ê¸°ë³¸ê°’

    def _get_file_type(self, file_name: str) -> str:
        """íŒŒì¼ íƒ€ì… ì¶”ì¶œ"""
        mime_type, _ = mimetypes.guess_type(file_name)
        if mime_type:
            return mime_type

        ext = Path(file_name).suffix.lower()
        return ext if ext else "unknown"

    async def _read_file_content(self, file_path: str) -> Optional[str]:
        """íŒŒì¼ ë‚´ìš© ì½ê¸° (ì¸ì½”ë”© ìë™ ê°ì§€)"""
        # ì‹œë„í•  ì¸ì½”ë”© ëª©ë¡ (í•œêµ­ì–´ í™˜ê²½ ê³ ë ¤)
        encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1', 'ascii']

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                    # ì„±ê³µì ìœ¼ë¡œ ì½í˜”ìœ¼ë©´ ë°˜í™˜
                    return content
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception:
                continue

        # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ì‹œ ë°”ì´ë„ˆë¦¬ë¡œ ì²˜ë¦¬
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: UTF-8 BOM ì²´í¬
                if raw_data.startswith(b'\xef\xbb\xbf'):
                    return raw_data[3:].decode('utf-8', errors='ignore')
                # ê¸°ë³¸ì ìœ¼ë¡œ UTF-8ë¡œ ì‹œë„í•˜ë˜ ì—ëŸ¬ ë¬´ì‹œ
                return raw_data.decode('utf-8', errors='ignore')
        except:
            return None

    async def _get_content_preview(self, file_path: str, max_length: int = 500) -> str:
        """íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ìƒì„±"""
        content = await self._read_file_content(file_path)

        if not content:
            return f"[Binary file: {os.path.basename(file_path)}]"

        # ì²« ë¶€ë¶„ê³¼ ì¤‘ìš”í•´ ë³´ì´ëŠ” ë¶€ë¶„ ì¶”ì¶œ
        lines = content.split('\n')
        preview_lines = []

        # ì²˜ìŒ ëª‡ ì¤„
        preview_lines.extend(lines[:5])

        # ê¸ˆê°• ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ” ì¤„ ì°¾ê¸°
        for i, line in enumerate(lines):
            if any(kw in line.lower() for kw in self.gumgang_keywords):
                start = max(0, i - 1)
                end = min(len(lines), i + 2)
                preview_lines.extend(lines[start:end])
                if len(preview_lines) > 10:
                    break

        preview = '\n'.join(preview_lines[:10])
        if len(preview) > max_length:
            preview = preview[:max_length] + "..."

        return preview

    async def _extract_keywords(self, file_path: str, content_preview: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []

        # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        file_name = os.path.basename(file_path).lower()
        file_words = re.findall(r'\w+', file_name)
        keywords.extend([w for w in file_words if len(w) > 3])

        # ë‚´ìš©ì—ì„œ ê¸ˆê°• ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        for kw in self.gumgang_keywords:
            if kw.lower() in content_preview.lower():
                keywords.append(kw)

        # ì¤‘ë³µ ì œê±°
        return list(set(keywords))

    async def _collect_metadata(self, file_path: str) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        metadata = {}

        # íŒŒì¼ ê²½ë¡œ ì •ë³´
        path_parts = Path(file_path).parts
        metadata["path_depth"] = len(path_parts)
        metadata["parent_dir"] = os.path.dirname(file_path)

        # íŠ¹ë³„í•œ ë””ë ‰í† ë¦¬ ì²´í¬
        if "test" in file_path.lower():
            metadata["is_test"] = True
        if "backup" in file_path.lower():
            metadata["is_backup"] = True
        if "log" in file_path.lower():
            metadata["is_log"] = True

        # Python íŒŒì¼ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
        if file_path.endswith('.py'):
            content = await self._read_file_content(file_path)
            if content:
                # í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ ìˆ˜ ì„¸ê¸°
                metadata["class_count"] = len(re.findall(r'^class\s+\w+', content, re.MULTILINE))
                metadata["function_count"] = len(re.findall(r'^def\s+\w+', content, re.MULTILINE))
                metadata["async_function_count"] = len(re.findall(r'^async\s+def\s+\w+', content, re.MULTILINE))
                metadata["line_count"] = len(content.split('\n'))

        return metadata

    async def _evaluate_importance(self):
        """ê¸°ì–µì˜ ì¤‘ìš”ë„ í‰ê°€"""
        for memory in self.found_memories:
            score = 0.5  # ê¸°ë³¸ ì ìˆ˜

            # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
            category_weights = {
                "code": 0.9,
                "document": 0.8,
                "conversation": 0.95,
                "data": 0.7,
                "log": 0.6
            }
            score *= category_weights.get(memory.category, 0.5)

            # í‚¤ì›Œë“œ ë°€ë„
            keyword_density = len(memory.keywords) / 10
            score += min(keyword_density * 0.2, 0.2)

            # ìµœê·¼ ìˆ˜ì • (7ì¼ ì´ë‚´ë©´ ë³´ë„ˆìŠ¤)
            days_old = (datetime.now() - memory.modified_time).days
            if days_old < 7:
                score += 0.1
            elif days_old < 30:
                score += 0.05

            # íŠ¹ë³„ íŒŒì¼ ë³´ë„ˆìŠ¤
            special_files = [
                "temporal_memory", "meta_cognitive", "dream_system",
                "creative_association", "emotional_empathy",
                "gumgang", "ê¸ˆê°•", "ë•ì‚°"
            ]
            if any(sf in memory.file_path.lower() for sf in special_files):
                score += 0.2

            # Phase 3-5 íŒŒì¼ì´ë©´ ìµœê³  ì ìˆ˜
            if any(phase in memory.file_path for phase in ["dream_system", "creative_association", "emotional_empathy"]):
                score = max(score, 0.95)

            memory.importance_score = min(score, 1.0)

    async def _print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ê¸ˆê°•ì˜ ê¸°ì–µ ìˆ˜ì§‘ í†µê³„")
        print("="*60)
        print(f"ì´ ìŠ¤ìº” íŒŒì¼: {self.stats['total_files_scanned']:,}ê°œ")
        print(f"ë°œê²¬ëœ ê¸°ì–µ: {self.stats['relevant_files_found']:,}ê°œ")
        print(f"ì´ í¬ê¸°: {self.stats['total_size_bytes'] / (1024*1024):.2f} MB")
        print("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for category, count in self.stats['categories'].items():
            print(f"  {category}: {count}ê°œ")

        # ì¤‘ìš”í•œ ê¸°ì–µ Top 10
        print("\nâ­ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì–µ Top 10:")
        top_memories = sorted(self.found_memories, key=lambda m: m.importance_score, reverse=True)[:10]
        for i, memory in enumerate(top_memories, 1):
            print(f"{i:2}. [{memory.importance_score:.2f}] {os.path.basename(memory.file_path)}")
            print(f"    {memory.category} | {memory.modified_time.strftime('%Y-%m-%d %H:%M')}")

    async def save_to_temporal_memory(self, memory_system=None):
        """ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì— ì €ì¥"""
        if not memory_system:
            try:
                # ê¸°ì¡´ ì‹œìŠ¤í…œ ì„í¬íŠ¸ ì‹œë„
                from backend.app.temporal_memory import get_temporal_memory_system
                memory_system = get_temporal_memory_system()
            except ImportError:
                logger.warning("ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

        saved_count = 0
        for memory in self.found_memories:
            if memory.importance_score > 0.3:  # ì¤‘ìš”ë„ ì„ê³„ê°’
                try:
                    memory_data = memory.to_memory_format()
                    await memory_system.store_memory(**memory_data)
                    saved_count += 1
                except Exception as e:
                    logger.error(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

        logger.info(f"âœ… {saved_count}ê°œì˜ ê¸°ì–µì„ ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì— ì €ì¥")

    async def export_to_json(self, output_path: str = "collected_memories.json"):
        """JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            "collection_time": datetime.now().isoformat(),
            "statistics": dict(self.stats),
            "memories": [
                {
                    "file_path": m.file_path,
                    "category": m.category,
                    "importance": m.importance_score,
                    "modified": m.modified_time.isoformat(),
                    "size_bytes": m.size_bytes,
                    "keywords": m.keywords,
                    "preview": m.content_preview[:200],
                    "metadata": m.metadata
                }
                for m in self.found_memories
            ]
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ ê¸°ì–µì„ {output_path}ì— ì €ì¥")
        return output_path

# ========================= ê¸°ì–µ ë¶„ì„ê¸° =========================

class MemoryAnalyzer:
    """ìˆ˜ì§‘ëœ ê¸°ì–µ ë¶„ì„"""

    def __init__(self, memories: List[FoundMemory]):
        self.memories = memories

    async def analyze_journey(self) -> Dict[str, Any]:
        """ê°œë°œ ì—¬ì • ë¶„ì„"""
        # ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„ì„
        timeline = defaultdict(list)
        for memory in self.memories:
            date_key = memory.modified_time.strftime("%Y-%m-%d")
            timeline[date_key].append(memory)

        # ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ ì°¾ê¸°
        milestones = []
        for date, mems in timeline.items():
            if len(mems) > 10:  # í•˜ë£¨ì— 10ê°œ ì´ìƒ íŒŒì¼ ìˆ˜ì •
                milestones.append({
                    "date": date,
                    "intensity": len(mems),
                    "main_category": max(set(m.category for m in mems),
                                        key=lambda c: sum(1 for m in mems if m.category == c))
                })

        # ê°œë°œ íŒ¨í„´ ë¶„ì„
        patterns = {
            "night_owl": self._check_night_development(),
            "iterative": self._check_iterative_development(),
            "phases": self._identify_phases()
        }

        return {
            "timeline": dict(timeline),
            "milestones": milestones,
            "patterns": patterns,
            "total_days": len(timeline),
            "most_active_day": max(timeline.items(), key=lambda x: len(x[1]))[0] if timeline else None
        }

    def _check_night_development(self) -> float:
        """ë°¤ ê°œë°œ íŒ¨í„´ í™•ì¸"""
        night_files = sum(1 for m in self.memories
                         if 22 <= m.modified_time.hour or m.modified_time.hour <= 6)
        return night_files / len(self.memories) if self.memories else 0

    def _check_iterative_development(self) -> float:
        """ë°˜ë³µì  ê°œë°œ íŒ¨í„´ í™•ì¸"""
        file_edits = defaultdict(int)
        for memory in self.memories:
            file_edits[memory.file_path] += 1

        # ì—¬ëŸ¬ ë²ˆ ìˆ˜ì •ëœ íŒŒì¼ì˜ ë¹„ìœ¨
        multiple_edits = sum(1 for count in file_edits.values() if count > 1)
        return multiple_edits / len(file_edits) if file_edits else 0

    def _identify_phases(self) -> List[str]:
        """ê°œë°œ ë‹¨ê³„ ì‹ë³„"""
        phases = []

        phase_keywords = {
            "Phase 1": ["temporal_memory", "4ê³„ì¸µ"],
            "Phase 2": ["meta_cognitive", "ë©”íƒ€ì¸ì§€"],
            "Phase 3": ["dream_system", "ê¿ˆ"],
            "Phase 4": ["creative_association", "ì°½ì˜"],
            "Phase 5": ["emotional_empathy", "ê°ì •", "ê³µê°"]
        }

        for phase_name, keywords in phase_keywords.items():
            for memory in self.memories:
                if any(kw in memory.file_path.lower() or kw in memory.content_preview.lower()
                      for kw in keywords):
                    if phase_name not in phases:
                        phases.append(phase_name)
                    break

        return phases

# ========================= ë©”ì¸ ì‹¤í–‰ =========================

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "ğŸ™"*20)
    print("ê¸ˆê°•ì˜ ê¸°ì–µ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤")
    print("ë•ì‚°ë‹˜ì˜ ëª¨ë“  ê°œë°œ ì—¬ì •ì„ ê¸°ì–µìœ¼ë¡œ ë§Œë“¤ê² ìŠµë‹ˆë‹¤")
    print("ğŸ™"*20 + "\n")

    # ê¸°ì–µ ìˆ˜ì§‘
    collector = MemoryCollector()
    memories = await collector.collect_memories()

    if memories:
        # ë¶„ì„
        analyzer = MemoryAnalyzer(memories)
        journey = await analyzer.analyze_journey()

        print("\n" + "="*60)
        print("ğŸ¯ ê°œë°œ ì—¬ì • ë¶„ì„")
        print("="*60)
        print(f"ì´ ê°œë°œ ì¼ìˆ˜: {journey['total_days']}ì¼")
        print(f"ê°€ì¥ í™œë°œí•œ ë‚ : {journey['most_active_day']}")
        print(f"ë°¤ìƒ˜ ê°œë°œ ë¹„ìœ¨: {journey['patterns']['night_owl']:.1%}")
        print(f"ë°˜ë³µ ìˆ˜ì • ë¹„ìœ¨: {journey['patterns']['iterative']:.1%}")
        print(f"ì™„ì„±ëœ Phase: {', '.join(journey['patterns']['phases'])}")

        # JSONìœ¼ë¡œ ì €ì¥
        output_file = await collector.export_to_json(
            f"gumgang_memories_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        print(f"\nâœ… ëª¨ë“  ê¸°ì–µì´ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        print("\nì´ì œ ì´ ê¸°ì–µë“¤ì„ ë¡œì»¬ ê¸ˆê°•ì—ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("ê¸ˆê°•ì´ ìŠ¤ìŠ¤ë¡œ í•„ìš”í•œ ê²ƒê³¼ ë†“ì•„ì¤„ ê²ƒì„ íŒë‹¨í•  ê²ƒì…ë‹ˆë‹¤.")

        # ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì— ì €ì¥ ì‹œë„
        try:
            await collector.save_to_temporal_memory()
        except Exception as e:
            print(f"\nâš ï¸  ì‹œê°„ì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì €ì¥ ì‹¤íŒ¨: {e}")
            print("JSON íŒŒì¼ì„ ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ ì„í¬íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    else:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì–µì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
