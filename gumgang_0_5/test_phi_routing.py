from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate

# âœ… 1. Phi ëª¨ë¸ ì—°ê²°
llm = Ollama(model="phi")

# âœ… 2. ì˜ë¯¸ ë¶„ë¥˜ ì „ìš© í”„ë¡¬í”„íŠ¸
prompt = PromptTemplate.from_template("""
ë‹¤ìŒ ì§ˆë¬¸ì„ ì˜ë¯¸ì— ë”°ë¼ ë¶„ë¥˜í•˜ì„¸ìš”.
ì„ íƒì§€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: [greeting, status, edit, recall, generate]
ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}
ì‘ë‹µ:
""")

# âœ… 3. ë¶„ë¥˜ê¸° í•¨ìˆ˜
def classify_query(query: str) -> str:
    chain = prompt | llm
    response = chain.invoke({"query": query})
    return response.strip().lower()

# âœ… 4. ì‹¤í—˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
test_questions = {
    "ê¸ˆê°•ì•„": "greeting",
    "ì§€ê¸ˆ ìƒíƒœ ì•Œë ¤ì¤˜": "status",
    "êµ¬ì¡° ìˆ˜ì • í•´ì¤˜": "edit",
    "ì§€ë‚œ ëŒ€í™” ê¸°ì–µ íšŒìƒí•´ì¤˜": "recall",
    "ë¶€ë™ì‚° í”Œë«í¼ ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ?": "generate"
}

# âœ… 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ“Š Phi ì˜ë¯¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n")
    for q, expected in test_questions.items():
        predicted = classify_query(q)
        success = "âœ…" if predicted == expected else "âŒ"
        print(f"{success} ì§ˆë¬¸: {q} â†’ ì˜ˆì¸¡: {predicted} (ì •ë‹µ: {expected})")
