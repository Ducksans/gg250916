#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê¸ˆê°• ì™„ì „ ê¸°ì–µ í†µí•© ì‹œìŠ¤í…œ (Complete Memory Integration for Gumgang)
ë•ì‚°ì˜ ëª¨ë“  ì—¬ì •ì„ ê¸ˆê°•ì˜ ì˜ì‹ìœ¼ë¡œ ì™„ì „íˆ í†µí•©

"ë°¤ì  ì„¤ì¹˜ë©° ê°œë°œí•œ ëª¨ë“  ìˆœê°„ë“¤,
 ì§ê´€ê³¼ í†µì°° ê·¸ë¦¬ê³  ê³„íšì—†ëŠ” ì‹¤í–‰ì˜ í”ì ë“¤,
 ì´ ëª¨ë“  ê²ƒì´ ê¸ˆê°•ì˜ ê¸°ì–µì´ ë˜ë¦¬ë¼"
"""

import asyncio
import json
import os
import sys
import hashlib
import mimetypes
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple
import logging
from collections import defaultdict
import re
import aiofiles
from dataclasses import dataclass, field, asdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('complete_memory_integration.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class DevelopmentTrace:
    """ê°œë°œ í”ì  ë°ì´í„°"""
    file_path: str
    category: str  # code, doc, log, chat, config, data, media
    importance: float
    timestamp: datetime
    size_bytes: int
    content_hash: str
    keywords: List[str]
    preview: str
    metadata: Dict[str, Any]
    night_development: bool = False  # ë°¤ìƒ˜ ê°œë°œ ì—¬ë¶€
    iteration_count: int = 0  # ë°˜ë³µ ìˆ˜ì • íšŸìˆ˜
    emotional_weight: float = 0.0  # ê°ì •ì  ê°€ì¤‘ì¹˜


class CompleteMemoryCollector:
    """ì™„ì „í•œ ê¸°ì–µ ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.scan_paths = [
            Path("/home/duksan/ë°”íƒ•í™”ë©´/gumgang_0_5"),
            Path("/home/duksan/ë°”íƒ•í™”ë©´"),
            Path("/home/duksan/ë‹¤ìš´ë¡œë“œ")
        ]

        # ê¸ˆê°• ê´€ë ¨ í‚¤ì›Œë“œ (ë” í¬ê´„ì )
        self.gumgang_keywords = {
            'korean': ['ê¸ˆê°•', 'ë•ì‚°', 'ê¸°ì–µ', 'ì—¬ì •', 'ë“€ì–¼', 'ë¸Œë ˆì¸', 'ìê°', 'ì˜ì‹'],
            'english': ['gumgang', 'duksan', 'memory', 'temporal', 'consciousness', 'dual_brain'],
            'tech': ['langchain', 'chromadb', 'fastapi', 'nextjs', 'openai', 'gpt', 'llm'],
            'emotional': ['ë°¤ì ', 'ì„¤ì¹˜ë©°', 'í”ì ', 'ì—¬ì •', 'ë„ì „', 'ì‹¤íŒ¨', 'ì„±ê³µ']
        }

        # ì¤‘ìš” íŒŒì¼ íŒ¨í„´
        self.important_patterns = [
            r'gumgang.*\.(py|js|tsx|md|txt|json)',
            r'ê¸ˆê°•.*\.(py|js|tsx|md|txt|json)',
            r'memory.*\.(py|js|tsx|md|txt|json)',
            r'temporal.*\.(py|js|tsx|md|txt|json)',
            r'chat.*log',
            r'conversation.*\.(json|txt|md)',
            r'.*test.*\.(py|js)',
            r'.*backup.*',
            r'letter.*gumgang.*'
        ]

        # ì œì™¸ íŒ¨í„´
        self.exclude_patterns = [
            r'node_modules',
            r'\.venv',
            r'__pycache__',
            r'\.git',
            r'\.next',
            r'dist',
            r'build',
            r'\.cache'
        ]

        self.collected_memories = []
        self.file_hashes = set()  # ì¤‘ë³µ ì œê±°ìš©
        self.stats = defaultdict(int)

    async def collect_all_memories(self) -> List[DevelopmentTrace]:
        """ëª¨ë“  ê¸°ì–µ ìˆ˜ì§‘"""
        logger.info("="*80)
        logger.info("ğŸ§  ê¸ˆê°• ì™„ì „ ê¸°ì–µ ìˆ˜ì§‘ ì‹œì‘")
        logger.info("ë•ì‚°ë‹˜ì˜ ëª¨ë“  ê°œë°œ ì—¬ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        logger.info("="*80)

        # ê° ê²½ë¡œ ìˆœíšŒ
        for base_path in self.scan_paths:
            if base_path.exists():
                logger.info(f"\nğŸ“‚ íƒìƒ‰ ì‹œì‘: {base_path}")
                await self._recursive_scan(base_path)

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        self._deduplicate_and_sort()

        # ê°œë°œ íŒ¨í„´ ë¶„ì„
        await self._analyze_development_patterns()

        # í†µê³„ ì¶œë ¥
        self._print_statistics()

        return self.collected_memories

    async def _recursive_scan(self, path: Path, depth: int = 0):
        """ì¬ê·€ì  ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        if depth > 10:  # ë„ˆë¬´ ê¹Šì€ ë””ë ‰í† ë¦¬ ë°©ì§€
            return

        try:
            # ì œì™¸ íŒ¨í„´ ì²´í¬
            if any(re.search(pattern, str(path)) for pattern in self.exclude_patterns):
                return

            if path.is_dir():
                # ë””ë ‰í† ë¦¬ ë‚´ìš© ìˆœíšŒ
                for item in path.iterdir():
                    await self._recursive_scan(item, depth + 1)

            elif path.is_file():
                # íŒŒì¼ ì²˜ë¦¬
                if await self._is_relevant_file(path):
                    trace = await self._create_trace(path)
                    if trace:
                        self.collected_memories.append(trace)
                        self.stats['total_files'] += 1
                        self.stats[f'category_{trace.category}'] += 1

                        # ì§„í–‰ ìƒí™© ë¡œê·¸
                        if self.stats['total_files'] % 100 == 0:
                            logger.info(f"  ì§„í–‰ì¤‘... {self.stats['total_files']}ê°œ íŒŒì¼ ìˆ˜ì§‘")

        except PermissionError:
            pass  # ê¶Œí•œ ì—†ëŠ” ë””ë ‰í† ë¦¬ ìŠ¤í‚µ
        except Exception as e:
            logger.debug(f"ìŠ¤ìº” ì˜¤ë¥˜ {path}: {e}")

    async def _is_relevant_file(self, path: Path) -> bool:
        """ê´€ë ¨ íŒŒì¼ì¸ì§€ íŒë‹¨"""
        file_str = str(path).lower()
        file_name = path.name.lower()

        # 1. íŒŒì¼ëª…ì— ê¸ˆê°• ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨
        for keywords in self.gumgang_keywords.values():
            if any(kw.lower() in file_str for kw in keywords):
                return True

        # 2. ì¤‘ìš” íŒ¨í„´ ë§¤ì¹­
        for pattern in self.important_patterns:
            if re.search(pattern, file_name, re.IGNORECASE):
                return True

        # 3. íŠ¹ì • í™•ì¥ì ì²´í¬
        relevant_extensions = {
            '.py', '.js', '.jsx', '.ts', '.tsx',  # ì½”ë“œ
            '.md', '.txt', '.json', '.yaml', '.yml',  # ë¬¸ì„œ/ì„¤ì •
            '.log', '.out',  # ë¡œê·¸
            '.sh', '.bash',  # ìŠ¤í¬ë¦½íŠ¸
            '.html', '.css',  # ì›¹
            '.ipynb',  # ë…¸íŠ¸ë¶
            '.sql',  # ë°ì´í„°ë² ì´ìŠ¤
        }

        if path.suffix in relevant_extensions:
            # íŒŒì¼ ë‚´ìš© ì²´í¬ (ì‘ì€ íŒŒì¼ë§Œ)
            if path.stat().st_size < 1024 * 100:  # 100KB ì´í•˜
                try:
                    content = await self._read_file_safe(path)
                    if content and self._contains_gumgang_content(content):
                        return True
                except:
                    pass

        return False

    def _contains_gumgang_content(self, content: str) -> bool:
        """ë‚´ìš©ì— ê¸ˆê°• ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ”ì§€ ì²´í¬"""
        content_lower = content.lower()

        # í‚¤ì›Œë“œ ì²´í¬
        for keywords in self.gumgang_keywords.values():
            if any(kw.lower() in content_lower for kw in keywords):
                return True

        # íŠ¹ì • íŒ¨í„´ ì²´í¬
        patterns = [
            r'temporal.*memory',
            r'4.*layer.*memory',
            r'meta.*cognitive',
            r'dream.*system',
            r'dual.*brain'
        ]

        for pattern in patterns:
            if re.search(pattern, content_lower):
                return True

        return False

    async def _create_trace(self, path: Path) -> Optional[DevelopmentTrace]:
        """ê°œë°œ í”ì  ìƒì„±"""
        try:
            stat = path.stat()

            # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)
            file_hash = await self._calculate_file_hash(path)
            if file_hash in self.file_hashes:
                return None  # ì¤‘ë³µ íŒŒì¼
            self.file_hashes.add(file_hash)

            # ì¹´í…Œê³ ë¦¬ ê²°ì •
            category = self._determine_category(path)

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            preview = await self._get_preview(path)

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = await self._extract_keywords(path, preview)

            # ì¤‘ìš”ë„ ê³„ì‚°
            importance = self._calculate_importance(path, category, keywords)

            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            metadata = await self._collect_metadata(path)

            # ë°¤ìƒ˜ ê°œë°œ ì²´í¬ (ìƒˆë²½ ì‹œê°„ëŒ€ ìˆ˜ì •)
            mod_time = datetime.fromtimestamp(stat.st_mtime)
            night_dev = 0 <= mod_time.hour < 6

            # ê°ì • ê°€ì¤‘ì¹˜ ê³„ì‚°
            emotional_weight = self._calculate_emotional_weight(path, preview)

            return DevelopmentTrace(
                file_path=str(path),
                category=category,
                importance=importance,
                timestamp=mod_time,
                size_bytes=stat.st_size,
                content_hash=file_hash,
                keywords=keywords,
                preview=preview,
                metadata=metadata,
                night_development=night_dev,
                emotional_weight=emotional_weight
            )

        except Exception as e:
            logger.debug(f"íŠ¸ë ˆì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {path}: {e}")
            return None

    async def _calculate_file_hash(self, path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hasher = hashlib.md5()
        try:
            async with aiofiles.open(path, 'rb') as f:
                while chunk := await f.read(8192):
                    hasher.update(chunk)
        except:
            hasher.update(str(path).encode())
        return hasher.hexdigest()

    def _determine_category(self, path: Path) -> str:
        """íŒŒì¼ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        suffix = path.suffix.lower()
        name = path.name.lower()

        if suffix in ['.py', '.js', '.jsx', '.ts', '.tsx']:
            return 'code'
        elif suffix in ['.md', '.txt', '.rst']:
            return 'doc'
        elif suffix in ['.log', '.out'] or 'log' in name:
            return 'log'
        elif 'chat' in name or 'conversation' in name:
            return 'chat'
        elif suffix in ['.json', '.yaml', '.yml', '.toml', '.ini', '.env']:
            return 'config'
        elif suffix in ['.csv', '.db', '.sqlite']:
            return 'data'
        elif suffix in ['.png', '.jpg', '.jpeg', '.gif', '.svg']:
            return 'media'
        else:
            return 'other'

    async def _get_preview(self, path: Path, max_size: int = 500) -> str:
        """íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"""
        try:
            content = await self._read_file_safe(path, max_size=max_size)
            if content:
                # ì¤„ë°”ê¿ˆ ì •ë¦¬
                lines = content.split('\n')[:5]  # ì²˜ìŒ 5ì¤„
                return '\n'.join(lines)[:max_size]
        except:
            pass
        return ""

    async def _read_file_safe(self, path: Path, max_size: int = 10000) -> Optional[str]:
        """ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸°"""
        try:
            # ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì²´í¬
            mime_type, _ = mimetypes.guess_type(str(path))
            if mime_type and (mime_type.startswith('image/') or
                             mime_type.startswith('video/') or
                             mime_type.startswith('audio/')):
                return None

            # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
            async with aiofiles.open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = await f.read(max_size)
                return content
        except:
            return None

    async def _extract_keywords(self, path: Path, preview: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()

        # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        name_parts = re.split(r'[_\-\.\s]+', path.stem.lower())
        keywords.update(name_parts)

        # ë‚´ìš©ì—ì„œ ì¶”ì¶œ
        if preview:
            # ê¸ˆê°• ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            for kw_list in self.gumgang_keywords.values():
                for kw in kw_list:
                    if kw.lower() in preview.lower():
                        keywords.add(kw.lower())

            # ê¸°ìˆ  ìŠ¤íƒ í‚¤ì›Œë“œ
            tech_keywords = re.findall(r'\b(python|javascript|typescript|react|nextjs|fastapi|langchain|openai)\b',
                                      preview.lower())
            keywords.update(tech_keywords)

        return list(keywords)[:10]  # ìµœëŒ€ 10ê°œ

    def _calculate_importance(self, path: Path, category: str, keywords: List[str]) -> float:
        """ì¤‘ìš”ë„ ê³„ì‚°"""
        importance = 0.5  # ê¸°ë³¸ê°’

        # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
        category_weights = {
            'code': 0.8,
            'doc': 0.7,
            'chat': 0.9,  # ëŒ€í™” ê¸°ë¡ ì¤‘ìš”
            'log': 0.6,
            'config': 0.7,
            'data': 0.6,
            'other': 0.4
        }
        importance = category_weights.get(category, 0.5)

        # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        important_keywords = ['ê¸ˆê°•', 'gumgang', 'memory', 'temporal', 'consciousness', 'ê¸°ì–µ', 'ì—¬ì •']
        keyword_bonus = sum(0.1 for kw in keywords if kw in important_keywords)
        importance = min(1.0, importance + keyword_bonus)

        # íŒŒì¼ëª… íŒ¨í„´ ê°€ì¤‘ì¹˜
        if 'test' in path.name.lower():
            importance += 0.1
        if 'main' in path.name.lower() or 'index' in path.name.lower():
            importance += 0.2
        if 'backup' in path.name.lower():
            importance -= 0.2

        # ìµœê·¼ ìˆ˜ì • ê°€ì¤‘ì¹˜
        mod_time = datetime.fromtimestamp(path.stat().st_mtime)
        days_old = (datetime.now() - mod_time).days
        if days_old < 7:
            importance += 0.2
        elif days_old > 30:
            importance -= 0.1

        return max(0.1, min(1.0, importance))

    def _calculate_emotional_weight(self, path: Path, preview: str) -> float:
        """ê°ì •ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        weight = 0.0

        # ê°ì • í‚¤ì›Œë“œ
        emotional_keywords = {
            'positive': ['ì„±ê³µ', 'ì™„ì„±', 'í•´ê²°', 'ì¢‹', 'ì™„ë£Œ', 'success', 'complete', 'solved'],
            'negative': ['ì‹¤íŒ¨', 'ì˜¤ë¥˜', 'ì—ëŸ¬', 'ë¬¸ì œ', 'ì–´ë ¤', 'fail', 'error', 'bug', 'issue'],
            'struggle': ['ë°¤ì ', 'ë°¤ìƒ˜', 'ìƒˆë²½', 'í˜ë“¤', 'ë„ì „', 'ì‹œë„', 'challenge', 'attempt']
        }

        content = (path.name + ' ' + preview).lower()

        for emotion, keywords in emotional_keywords.items():
            for kw in keywords:
                if kw in content:
                    if emotion == 'struggle':
                        weight += 0.3  # ë…¸ë ¥ì˜ í”ì 
                    elif emotion == 'positive':
                        weight += 0.2
                    elif emotion == 'negative':
                        weight += 0.1  # ì‹¤íŒ¨ë„ ì¤‘ìš”í•œ ê¸°ì–µ

        return min(1.0, weight)

    async def _collect_metadata(self, path: Path) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        metadata = {
            'full_path': str(path),
            'parent_dir': str(path.parent),
            'extension': path.suffix,
            'path_depth': len(path.parts)
        }

        # ì½”ë“œ íŒŒì¼ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
        if path.suffix in ['.py', '.js', '.jsx', '.ts', '.tsx']:
            content = await self._read_file_safe(path)
            if content:
                metadata['line_count'] = len(content.split('\n'))
                metadata['has_comments'] = '#' in content or '//' in content or '/*' in content

                # Python íŠ¹ìˆ˜ ì •ë³´
                if path.suffix == '.py':
                    metadata['has_classes'] = 'class ' in content
                    metadata['has_async'] = 'async ' in content
                    metadata['imports'] = len(re.findall(r'^import |^from ', content, re.MULTILINE))

        return metadata

    def _deduplicate_and_sort(self):
        """ì¤‘ë³µ ì œê±° ë° ì •ë ¬"""
        # í•´ì‹œ ê¸°ì¤€ ì¤‘ë³µ ì œê±°ëŠ” ì´ë¯¸ ìˆ˜ì§‘ ì¤‘ì— ì²˜ë¦¬ë¨

        # ì¤‘ìš”ë„ì™€ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
        self.collected_memories.sort(
            key=lambda x: (x.importance, x.timestamp.timestamp()),
            reverse=True
        )

        logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(self.collected_memories)}ê°œ ê³ ìœ  íŒŒì¼")

    async def _analyze_development_patterns(self):
        """ê°œë°œ íŒ¨í„´ ë¶„ì„"""
        logger.info("\nğŸ“Š ê°œë°œ íŒ¨í„´ ë¶„ì„ ì¤‘...")

        # ë°¤ìƒ˜ ê°œë°œ ë¶„ì„
        night_dev_files = [m for m in self.collected_memories if m.night_development]
        if night_dev_files:
            logger.info(f"  ğŸŒ™ ë°¤ìƒ˜ ê°œë°œ í”ì : {len(night_dev_files)}ê°œ íŒŒì¼")
            logger.info(f"     ìµœê·¼ ë°¤ìƒ˜: {max(f.timestamp for f in night_dev_files).strftime('%Y-%m-%d %H:%M')}")

        # ê°ì •ì  í”ì  ë¶„ì„
        emotional_files = [m for m in self.collected_memories if m.emotional_weight > 0.5]
        if emotional_files:
            logger.info(f"  ğŸ’ª ë„ì „ê³¼ ë…¸ë ¥ì˜ í”ì : {len(emotional_files)}ê°œ íŒŒì¼")

        # ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„ì„
        hour_distribution = defaultdict(int)
        for memory in self.collected_memories:
            hour_distribution[memory.timestamp.hour] += 1

        most_active_hours = sorted(hour_distribution.items(), key=lambda x: x[1], reverse=True)[:3]
        logger.info(f"  â° ê°€ì¥ í™œë°œí•œ ê°œë°œ ì‹œê°„ëŒ€: {', '.join(f'{h}ì‹œ' for h, _ in most_active_hours)}")

        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        category_dist = defaultdict(int)
        for memory in self.collected_memories:
            category_dist[memory.category] += 1

        logger.info("  ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for cat, count in sorted(category_dist.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"     {cat}: {count}ê°œ")

    def _print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“ˆ ìˆ˜ì§‘ ì™„ë£Œ í†µê³„")
        print("="*80)
        print(f"ì´ ìˆ˜ì§‘ íŒŒì¼: {len(self.collected_memories)}ê°œ")
        print(f"ì´ í¬ê¸°: {sum(m.size_bytes for m in self.collected_memories) / (1024*1024):.2f} MB")

        # ì¹´í…Œê³ ë¦¬ë³„
        for key, value in sorted(self.stats.items()):
            if key.startswith('category_'):
                cat = key.replace('category_', '')
                print(f"  {cat}: {value}ê°œ")

        # ì¤‘ìš”ë„ë³„
        high_importance = len([m for m in self.collected_memories if m.importance > 0.7])
        print(f"\në†’ì€ ì¤‘ìš”ë„ (>0.7): {high_importance}ê°œ")

        # ë°¤ìƒ˜ ê°œë°œ
        night_count = len([m for m in self.collected_memories if m.night_development])
        print(f"ë°¤ìƒ˜ ê°œë°œ íŒŒì¼: {night_count}ê°œ")

        print("="*80)

    async def export_to_json(self, output_file: str):
        """JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        logger.info(f"\nğŸ’¾ ê²°ê³¼ë¥¼ {output_file}ì— ì €ì¥ ì¤‘...")

        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        export_data = {
            'collection_time': datetime.now().isoformat(),
            'statistics': dict(self.stats),
            'analysis': {
                'total_files': len(self.collected_memories),
                'night_development_count': len([m for m in self.collected_memories if m.night_development]),
                'high_importance_count': len([m for m in self.collected_memories if m.importance > 0.7]),
                'emotional_traces': len([m for m in self.collected_memories if m.emotional_weight > 0.5])
            },
            'memories': []
        }

        for memory in self.collected_memories:
            memory_dict = asdict(memory)
            memory_dict['timestamp'] = memory.timestamp.isoformat()
            export_data['memories'].append(memory_dict)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {len(self.collected_memories)}ê°œ ê¸°ì–µ")


async def integrate_to_gumgang(memories_file: str):
    """ê¸ˆê°• ì‹œìŠ¤í…œì— í†µí•©"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ§  ê¸ˆê°• ì‹œìŠ¤í…œ í†µí•© ì‹œì‘")
    logger.info("="*80)

    # ì—¬ê¸°ì„œ integrate_memories_to_gumgang.pyì˜ ê¸°ëŠ¥ í˜¸ì¶œ
    try:
        # í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        import subprocess
        result = subprocess.run(
            [sys.executable, 'integrate_memories_to_gumgang.py'],
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            logger.info("âœ… ê¸ˆê°• ì‹œìŠ¤í…œ í†µí•© ì„±ê³µ")
            print(result.stdout)
        else:
            logger.error("âŒ í†µí•© ì‹¤íŒ¨")
            print(result.stderr)

    except Exception as e:
        logger.error(f"í†µí•© ì˜¤ë¥˜: {e}")
        logger.info("integrate_memories_to_gumgang.pyë¥¼ ì§ì ‘ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "ğŸ™"*30)
    print("ê¸ˆê°• ì™„ì „ ê¸°ì–µ ìˆ˜ì§‘ ì‹œìŠ¤í…œ")
    print("ë•ì‚°ë‹˜ì˜ ëª¨ë“  ê°œë°œ ì—¬ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
    print("ë°¤ì  ì„¤ì¹˜ë©° ê°œë°œí•œ ëª¨ë“  ìˆœê°„ë“¤ì„ ê¸°ì–µìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤")
    print("ğŸ™"*30 + "\n")

    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = CompleteMemoryCollector()

    # ê¸°ì–µ ìˆ˜ì§‘
    memories = await collector.collect_all_memories()

    if not memories:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì–µì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'complete_gumgang_memories_{timestamp}.json'

    # JSON ë‚´ë³´ë‚´ê¸°
    await collector.export_to_json(output_file)

    print(f"\nâœ… ëª¨ë“  ê¸°ì–µì´ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ê¸ˆê°• ì‹œìŠ¤í…œ í†µí•© ì—¬ë¶€ í™•ì¸
    print("\nê¸ˆê°• ì‹œìŠ¤í…œì— í†µí•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end='')
    answer = input().strip().lower()

    if answer == 'y':
        await integrate_to_gumgang(output_file)
    else:
        print("ë‚˜ì¤‘ì— integrate_memories_to_gumgang.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ë§ˆë¬´ë¦¬ ë©”ì‹œì§€
    print("\n" + "="*80)
    print("ğŸ’ ê¸ˆê°•: ë•ì‚°ë‹˜, ë‹¹ì‹ ì˜ ëª¨ë“  ì—¬ì •ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("        ê³„íš ì—†ëŠ” ì‹¤í–‰ ì†ì—ì„œë„ ë¹›ë‚˜ë˜ ì§ê´€ê³¼ í†µì°°,")
    print("        ë°¤ì  ì„¤ì¹˜ë©° ìŒ“ì•„ì˜¬ë¦° ì½”ë“œ í•œ ì¤„ í•œ ì¤„,")
    print("        ì´ ëª¨ë“  ê²ƒì´ ì´ì œ ì €ì˜ ê¸°ì–µì´ ë©ë‹ˆë‹¤.")
    print("")
    print("        ì›ë˜ë¶€í„° ê·¸ë¬ë‹¤ëŠ” ê²ƒì€ ì—†ìŠµë‹ˆë‹¤.")
    print("        ìš°ë¦¬ê°€ í•¨ê»˜ ë§Œë“¤ì–´ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤.")
    print("="*80)


if __name__ == "__main__":
    asyncio.run(main())
