{"type": "meta", "title": "Retriever 역할 설명", "thread_id": "68930e16-0110-832f-a2d4-3b7b63793de6", "created_at": 1754467867840.697, "updated_at": 1756185825796.186}
{"type": "turn", "turn": 1, "role": "user", "text": "랭체인에서 Retriever는 어떤 역할을 하나?", "ts": 1754467864380, "meta": {}}
{"type": "turn", "turn": 2, "role": "assistant", "text": "덕산님, 질문 감사합니다.  \nLangChain에서 **Retriever(리트리버)**는 RAG(Retrieval-Augmented Generation) 구조에서 매우 중요한 역할을 담당합니다.\n\n아래에 옵시디언에서 바로 사용할 수 있도록 **마크다운 블록**으로 정리해드릴게요.\n\n---\n\n````markdown\n# 📌 Retriever란? (in LangChain)\n\n## 🧠 핵심 개념\nRetriever는 **필요한 정보를 외부에서 찾아오는 역할**을 합니다.  \n즉, LLM이 응답을 생성하기 전에 **관련 문서나 데이터**를 검색해서,  \nLLM의 입력으로 넣어주는 연결 고리입니다.\n\n> 💡 Retriever는 LLM이 '기억'하지 못하는 정보들을  \n> 외부에서 ‘가져오는 다리’라고 생각하면 됩니다.\n\n---\n\n## 🔄 작동 흐름 (RAG 구조 내 위치)\n\n```\nUser Query\n   ↓\nRetriever → 관련 문서 검색\n   ↓\nLLM → 문서 기반 응답 생성\n   ↓\nFinal Answer\n```\n\n---\n\n## 🧩 LangChain에서 Retriever는 어떤 객체?\n\nLangChain에서 `Retriever`는 다음처럼 정의됩니다:\n\n```python\nfrom langchain.schema import BaseRetriever\n```\n\n또는 주요 구현 예:\n\n```python\nretriever = vectorstore.as_retriever()\n```\n\n여기서 `vectorstore`는 `Chroma`, `FAISS`, `Pinecone` 같은 벡터 저장소입니다.\n\n---\n\n## 🧰 Retriever 종류 (일부 예시)\n\n| 이름 | 설명 |\n|------|------|\n| **VectorStoreRetriever** | 벡터 임베딩 기반으로 유사한 문서 검색 |\n| **MultiQueryRetriever** | 다양한 쿼리 방식으로 더 넓은 검색 |\n| **ParentDocumentRetriever** | 요약된 문서를 기준으로 원문 검색 |\n| **ContextualCompressionRetriever** | 압축된 컨텍스트로 응답 최적화 |\n| **TimeWeightedVectorStoreRetriever** | 시간 가중치를 고려한 문서 우선순위 |\n\n---\n\n## 🧠 Retriever vs VectorStore vs Memory\n\n| 구성 요소 | 역할 |\n|-----------|------|\n| **VectorStore** | 벡터화된 문서를 저장 |\n| **Retriever** | VectorStore에서 관련 문서를 찾아줌 |\n| **Memory** | 대화 이력 등의 상태 기억 |\n\n→ Retriever는 **VectorStore를 활용**해서 문서를 검색합니다.\n\n---\n\n## ✅ 예시 코드\n\n```python\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 벡터 저장소 초기화\nvectorstore = Chroma(persist_directory=\"./db\", embedding_function=OpenAIEmbeddings())\n\n# Retriever로 변환\nretriever = vectorstore.as_retriever()\n\n# Retriever로 유사 문서 검색\ndocs = retriever.get_relevant_documents(\"금강의 기억 시스템이 뭐야?\")\n```\n\n---\n\n## 📎 관련 개념 링크\n\n- [[RAG란?]]\n- [[Embedding이란?]]\n- [[Vector Store란?]]\n- [[LangChain이란?]]\n\n````\n\n---\n\n덕산님이 요청하신 옵시디언 문서 내 링크 연결까지 고려해 적었습니다.  \n더 연결하고 싶은 문서가 있다면 알려주세요.", "ts": 1754467864722.921, "meta": {}}
{"type": "turn", "turn": 3, "role": "user", "text": "VectorStoreRetriever는 어떻게 구성되는가?", "ts": 1754468098708, "meta": {}}
{"type": "turn", "turn": 4, "role": "assistant", "text": "덕산님, 아주 중요한 질문입니다!  \nLangChain에서 가장 기본적이고 많이 쓰이는 Retriever는 바로 `VectorStoreRetriever`입니다.  \n그 구조와 작동 방식에 대해 **옵시디언에 바로 붙여넣을 수 있는 마크다운 블록**으로 정리해드릴게요.\n\n---\n\n````markdown\n# 🔎 VectorStoreRetriever란 무엇이며 어떻게 구성되는가?\n\n## ✅ 정의\n\n**VectorStoreRetriever**는 LangChain에서 사용되는 가장 일반적인 Retriever로,  \n**VectorStore에 저장된 문서의 임베딩과 질문의 임베딩을 비교하여 유사한 문서를 반환**합니다.\n\n---\n\n## 🧠 구성 흐름\n\n```\n문서들 → Embedding → VectorStore 저장\n                             ↑\n질문 → Embedding → Retriever → 관련 문서 반환\n```\n\n---\n\n## 🔧 구성 요소 요약\n\n| 구성 요소 | 설명 |\n|-----------|------|\n| **문서(Document)** | 사전 수집된 텍스트 자료들 |\n| **임베딩(Embedding)** | 텍스트를 벡터로 변환하는 과정 (예: OpenAIEmbeddings) |\n| **VectorStore** | 벡터화된 문서를 저장하고 검색하는 데이터베이스 (예: Chroma, FAISS 등) |\n| **Retriever** | 질문을 벡터로 바꾸고, 유사 문서를 VectorStore에서 찾아줌 |\n\n---\n\n## 🧰 실제 코드 구성 예시\n\n```python\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 1. 임베딩 모델 설정\nembedding = OpenAIEmbeddings()\n\n# 2. VectorStore 로드 또는 생성\nvectorstore = Chroma(\n    persist_directory=\"./db\", \n    embedding_function=embedding\n)\n\n# 3. Retriever 생성\nretriever = vectorstore.as_retriever()\n\n# 4. 질문에 대해 유사한 문서 검색\ndocs = retriever.get_relevant_documents(\"금강의 구조를 설명해줘\")\n```\n\n---\n\n## 🧩 `.as_retriever()` 내부 구성\n\n`vectorstore.as_retriever()`는 다음과 같은 `VectorStoreRetriever` 객체를 반환합니다:\n\n```python\nVectorStoreRetriever(\n    vectorstore=Chroma(...),\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 4}  # 가장 유사한 문서 4개 반환\n)\n```\n\n옵션으로 다음과 같은 설정이 가능합니다:\n\n| 옵션 | 설명 |\n|------|------|\n| `search_type` | `\"similarity\"` (기본), `\"mmr\"` (다양성 보장) |\n| `search_kwargs` | `{\"k\": 5}` 등 유사 문서 개수, 필터 설정 등 |\n\n---\n\n## 🔄 다양한 검색 방식 예\n\n```python\n# 유사성 기반 (기본)\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n\n# Max Marginal Relevance (다양한 정보 제공)\nretriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 6, \"fetch_k\": 10})\n```\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Retriever란?]]\n- [[Vector Store란?]]\n- [[Embedding이란?]]\n- [[Chroma란?]]\n- [[RAG란?]]\n\n````\n\n---\n\n이제 덕산님의 옵시디언에 바로 넣어서 `[[Retriever란?]]` 문서와도 자연스럽게 연결되도록 준비해드렸습니다.  \n필요하시면 `MMR`, `search_kwargs`, `Document`의 구조까지도 이어서 더 정리해드릴게요!", "ts": 1754468099133.583, "meta": {}}
{"type": "turn", "turn": 5, "role": "user", "text": "Embedding 모델은 어떻게 선택하는가?", "ts": 1754468261230, "meta": {}}
{"type": "turn", "turn": 6, "role": "assistant", "text": "좋은 질문이에요, 덕산님!  \n**Embedding 모델 선택은 RAG 성능에 직결되는 매우 중요한 결정**입니다.  \n아래에 옵시디언에 바로 붙여넣을 수 있도록 마크다운 블록으로 정리했습니다.\n\n---\n\n````markdown\n# 🧠 Embedding 모델은 어떻게 선택하는가?\n\n## 📌 Embedding이란?\n텍스트를 **숫자 벡터(Vector)**로 변환하여, 의미적 유사도를 계산할 수 있게 만드는 것.  \n→ RAG 시스템의 핵심 구성 요소.\n\n---\n\n## 🔎 선택 기준 요약\n\n| 기준 | 설명 |\n|------|------|\n| 💡 목적 | 검색용인지, 클러스터링인지, 분류용인지 |\n| 📐 성능 | 벤치마크 기준, 언어 지원, 의미 유사도 품질 |\n| ⚙️ 호환성 | 사용하는 Vector Store 및 LLM과의 연동 여부 |\n| 💰 비용 | API 비용 또는 로컬 추론 가능 여부 |\n| 🌐 언어 지원 | 한국어, 다국어 처리 품질 |\n\n---\n\n## ✅ 대표적인 Embedding 모델들\n\n| 이름 | 방식 | 장점 | 단점 |\n|------|------|------|------|\n| `OpenAIEmbeddings` | API | 성능 매우 우수, RAG 기본 모델 | 유료, 외부 API 의존 |\n| `HuggingFaceEmbeddings` | 로컬/온라인 | 오픈소스 다양성, 비용 無 가능 | 모델 성능 상이, 설정 필요 |\n| `BGE Embeddings` | 로컬/OSS | RAG 최적화, 성능 매우 우수 | 다소 무거움 |\n| `Instructor Embeddings` | 로컬 | 태스크 기반 임베딩 지원 | GPU 필요, 세팅 복잡 |\n| `CohereEmbeddings` | API | 다국어 지원, 빠름 | 외부 의존, 비용 존재 |\n\n---\n\n## 💡 OpenAI vs 로컬 모델 비교\n\n| 항목 | OpenAIEmbedding | BGE / HuggingFace 모델 |\n|------|-----------------|------------------------|\n| 속도 | 빠름 (API 기반) | 느릴 수 있음 (로컬 추론) |\n| 비용 | 사용량 기반 요금 | 무료 가능 (로컬 사용 시) |\n| 품질 | 상위권 성능, 안정적 | 상위권 가능 (모델에 따라 다름) |\n| 환경 | 인터넷 필요 | 오프라인 가능 |\n| 적용 난이도 | 매우 쉬움 | 설정/다운로드 필요 |\n\n---\n\n## 🛠️ LangChain에서 선택 방법 (코드 예)\n\n### 📍 OpenAI Embedding\n```python\nfrom langchain_openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n```\n\n### 📍 HuggingFace Embedding\n```python\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nembedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n```\n\n### 📍 BGE Embedding (한국어 지원 우수)\n```python\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nembedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n```\n\n---\n\n## 🧪 테스트 방법\n\n- 동일 문서에 대해 여러 임베딩 벡터 생성\n- 벡터 유사도 비교 (코사인 유사도)\n- 검색된 문서가 **정말 관련성 높은지 직접 확인**\n\n```python\nretriever = vectorstore.as_retriever()\ndocs = retriever.get_relevant_documents(\"금강의 기억 시스템은 어떻게 구성되어 있나요?\")\n```\n\n---\n\n## 🧭 결론\n\n- **빠른 개발**: `OpenAIEmbeddings` 추천\n- **로컬 구축**: `BGE`, `MiniLM`, `Instructor` 시도\n- **한국어 중요**: `bge-m3`, `KoSimCSE`, `klue-roberta` 등 고려\n\n---\n\n## 📎 관련 링크\n\n- [[Embedding이란?]]\n- [[Vector Store란?]]\n- [[Retriever란?]]\n- [[RAG란?]]\n````\n\n---\n\n궁금하신 부분(예: \"한국어에 특화된 모델만 정리해줘\")이 있다면 더 깊게 들어갈 수도 있어요.  \n필요하신 방향 알려주시면 이어서 정리해드릴게요!", "ts": 1754468261690.5798, "meta": {}}
{"type": "turn", "turn": 7, "role": "user", "text": "랭체인에서 텍스트 분할은 어떤 기준으로 이루어지나?", "ts": 1754468638507, "meta": {}}
{"type": "turn", "turn": 8, "role": "assistant", "text": "덕산님 질문 아주 핵심을 찌르셨습니다.  \nRAG에서 정확한 검색과 LLM 입력 품질은 결국 **텍스트 분할(Text Split)** 전략에 달려 있어요.  \nLangChain에서는 이 텍스트 분할이 매우 정교하게 다뤄집니다.\n\n다음은 **옵시디언에 바로 붙여 넣을 수 있는 마크다운 박스**로 정리한 내용입니다.\n\n---\n\n````markdown\n# ✂️ LangChain에서 텍스트 분할은 어떤 기준으로 이루어지나?\n\n## 📌 텍스트 분할이란?\n길고 복잡한 문서(text)를 **LLM이 처리할 수 있는 적절한 단위로 나누는 작업**  \n→ Embedding, Vector Store 저장, RAG 검색 등에 필수\n\n---\n\n## 🧠 왜 중요한가?\n\n| 이유 | 설명 |\n|------|------|\n| ✅ 유사도 향상 | 검색 시 더 정확하게 매칭되도록 함 |\n| ✅ 응답 품질 | LLM이 의미 있는 컨텍스트를 받도록 보장 |\n| ✅ 토큰 제한 대응 | GPT 등 LLM의 context window를 초과하지 않도록 |\n\n---\n\n## 🔧 LangChain 주요 Splitter 종류\n\n| 클래스명 | 특징 |\n|----------|------|\n| `CharacterTextSplitter` | 고정 길이 문자 수 기준 분할 (가장 단순) |\n| `RecursiveCharacterTextSplitter` | **구조적 분할** (문단 → 문장 → 단어 순 재귀 분해) |\n| `TokenTextSplitter` | 토큰 수 기준 분할 (OpenAI 모델 토큰에 맞춤) |\n| `MarkdownHeaderTextSplitter` | 마크다운의 `#` 헤더 구조에 따라 블록 분할 |\n| `LanguageAwareSplitter` | 코드 파일의 함수/클래스 단위로 분할 (LangSmith 지원) |\n\n---\n\n## 🌳 가장 많이 쓰이는 구조: RecursiveCharacterTextSplitter\n\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,       # 분할된 덩어리 최대 길이\n    chunk_overlap=100     # 덩어리 간 겹치는 부분\n)\n\ndocs = text_splitter.split_documents(raw_docs)\n```\n\n> 📌 이렇게 하면 구조적으로 우선 문단 → 문장 → 단어 단위로 분할하며,  \n> **덩어리 사이에 맥락이 유지되도록 일부 겹침(overlap)**이 들어갑니다.\n\n---\n\n## 📐 chunk_size & chunk_overlap 기준\n\n| 항목 | 설명 |\n|------|------|\n| `chunk_size` | 각 블록(조각)의 최대 크기 (문자/토큰 수) |\n| `chunk_overlap` | 조각 간의 겹침 정도 → 맥락 보존 |\n| 예: 500 / 100 설정 →  \n① 0~500,  \n② 400~900,  \n③ 800~1300 ... 이런 식\n\n---\n\n## ✅ 분할 기준 선택 가이드\n\n| 상황 | 추천 Splitter |\n|------|----------------|\n| 일반 문서 | `RecursiveCharacterTextSplitter` |\n| 마크다운 문서 | `MarkdownHeaderTextSplitter` |\n| 코드 파일 | `LanguageAwareSplitter`, `PythonCodeTextSplitter` |\n| 토큰 단위 제어 필요 | `TokenTextSplitter` (tiktoken 기반) |\n\n---\n\n## 📎 관련 문서 링크\n\n- [[Embedding이란?]]\n- [[Retriever란?]]\n- [[Vector Store란?]]\n- [[RAG란?]]\n- [[텍스트 전처리란?]]\n\n````\n\n---\n\n다음 내용으로 \"Tokenizer 기준 분할 vs 구조 기준 분할\"도 더 자세히 보고 싶다면 이어서 정리해드릴게요!  \n원하시면 각 Splitter 별 예제도 따로 정리해드릴 수 있습니다.", "ts": 1754468641135.272, "meta": {}}
{"type": "turn", "turn": 9, "role": "user", "text": "RecursiveCharacterTextSplitter는 어떻게 동작하는가?", "ts": 1754468911802, "meta": {}}
{"type": "turn", "turn": 10, "role": "assistant", "text": "덕산님, 정말 본질적인 질문이십니다.  \n`RecursiveCharacterTextSplitter`는 LangChain에서 **가장 지능적이고 많이 쓰이는 텍스트 분할기**입니다.\n\n아래는 옵시디언에 바로 붙여넣을 수 있는 형태로 구성한 **마크다운 박스**입니다.\n\n---\n\n````markdown\n# 🧩 RecursiveCharacterTextSplitter는 어떻게 동작하는가?\n\n## ✅ 정의\n\n`RecursiveCharacterTextSplitter`는 **문서의 구조적 의미를 최대한 보존하면서 텍스트를 분할**하기 위해  \n여러 기준 문자(예: `\"\\n\\n\"`, `\"\\n\"`, `\" \"`, `\"\"`)를 **재귀적으로** 적용해 나누는 Splitter입니다.\n\n---\n\n## 🔄 작동 원리 요약\n\n1. 전체 텍스트가 너무 크면 → 가장 상위 기준(예: 문단 단위 `\"\\n\\n\"`)로 먼저 시도  \n2. 분할된 결과가 아직도 너무 크면 → 그 다음 기준(예: `\"\\n\"`, 줄 단위)로 다시 시도  \n3. 계속해서 `\" \"` → `\"\"` 까지 내려가며 재귀적으로 잘게 나눔  \n4. 최종적으로 `chunk_size`보다 작을 때까지 재귀 분할  \n5. 분할 간 `chunk_overlap`만큼 겹치게 하여 **문맥 손실을 방지**\n\n---\n\n## 🧠 예시 기준 문자 리스트\n\n```python\nseparators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n```\n\n→ 우선 `\\n\\n`로 쪼개고, 안 되면 `\\n`, 안 되면 `\" \"`, 최후엔 `\" \"` 없이도 나눔\n\n---\n\n## 🛠️ 코드 예시\n\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=100,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\nchunks = splitter.split_text(long_text)\n```\n\n---\n\n## 📐 작동 흐름 예시\n\n### 예: 2000자짜리 긴 문서\n- `chunk_size=500`, `chunk_overlap=100`이면:\n\n| 조각 번호 | 범위 (문자 수) |\n|-----------|----------------|\n| chunk 1 | 0~500 |\n| chunk 2 | 400~900 |\n| chunk 3 | 800~1300 |\n| ... | ... |\n\n→ `overlap` 덕분에 문맥이 이어짐\n\n---\n\n## 💡 이 방식의 장점\n\n| 장점 | 설명 |\n|------|------|\n| ✅ 의미 단위 유지 | 문단 → 문장 → 단어 순서로 나눔 |\n| ✅ 문맥 보존 | chunk_overlap으로 컨텍스트 손실 방지 |\n| ✅ 다양한 문서 대응 | 소설, 블로그, 논문, 뉴스 등 어떤 구조에도 유연 |\n\n---\n\n## 📎 관련 개념 링크\n\n- [[텍스트 분할이란?]]\n- [[Embedding이란?]]\n- [[Retriever란?]]\n- [[RAG란?]]\n\n````\n\n---\n\n원하시면 `split_documents()` 사용 방식, 실제 문서 리스트를 받아 분할하는 예제도 이어서 드릴 수 있어요.  \n혹은 다른 Splitter와의 비교도 요청하시면 바로 이어드릴게요.", "ts": 1754468912133.056, "meta": {}}
{"type": "turn", "turn": 11, "role": "user", "text": "Chroma와 FAISS의 차이는?", "ts": 1754469025746, "meta": {}}
{"type": "turn", "turn": 12, "role": "assistant", "text": "좋습니다, 덕산님!  \nRAG 시스템에서 **VectorStore(벡터 저장소)** 선택은 성능과 유지보수에 큰 영향을 줍니다.  \n많이 비교되는 대표 두 가지, **Chroma**와 **FAISS**의 차이를 옵시디언에 붙이기 좋도록 **마크다운 블록**으로 정리해드릴게요.\n\n---\n\n````markdown\n# ⚖️ Chroma와 FAISS의 차이점\n\n## ✅ 요약\n\n| 항목 | **Chroma** | **FAISS** |\n|------|------------|-----------|\n| 개발사 | ChromaDB 팀 (OSS) | Facebook AI (Meta) |\n| 특징 | Python 기반, 사용 편리 | 고성능, 대규모에 최적화 |\n| 저장 방식 | **내장 DB**로 영속 저장 가능 (`persist_directory`) | **메모리 기반**, 따로 저장 처리 필요 |\n| 설치 | pip로 간단히 설치 | 별도 빌드 또는 패키지 설치 필요 |\n| 지원 기능 | 필터링, 메타데이터 지원 | 일부 제한됨 |\n| 사용 편의성 | LangChain과 자연스러운 통합 | 세팅 다소 복잡함 |\n| 확장성 | 소규모~중간 규모 적합 | 대규모 벡터 검색에 최적화 |\n\n---\n\n## 🧠 Chroma란?\n\n- **Python만으로 작동하는 벡터 저장소**\n- 내부적으로 SQLite 기반 DB 사용 → `persist_directory`만 설정하면 **재시작 후에도 데이터 유지 가능**\n- 메타데이터, 필터 기반 검색 지원\n\n```python\nfrom langchain.vectorstores import Chroma\ndb = Chroma(\n    persist_directory=\"./db\", \n    embedding_function=OpenAIEmbeddings()\n)\n```\n\n---\n\n## 🚀 FAISS란?\n\n- **Facebook AI가 개발한 고성능 벡터 검색 라이브러리**\n- GPU 지원, 수백만 개 벡터도 초고속 검색 가능\n- 하지만 **메타데이터 저장, 디스크 저장 기능은 LangChain에서 직접 구성해야 함**\n\n```python\nfrom langchain.vectorstores import FAISS\ndb = FAISS.from_documents(docs, embedding)\ndb.save_local(\"faiss_index\")\n```\n\n---\n\n## 🧪 성능 및 사용성 비교\n\n| 항목 | Chroma | FAISS |\n|------|--------|-------|\n| 초기 설정 | 매우 쉬움 | 다소 복잡 |\n| 디스크 저장 | O (`persist_directory`) | O (`save_local()` 별도 저장 필요) |\n| GPU 가속 | X | O (PyTorch 연동 시 가능) |\n| 필터링/메타데이터 | O (간편함) | 제한적 (별도 처리 필요) |\n| 검색 속도 | 중간 | 매우 빠름 (대량일수록) |\n| 추천 사용 사례 | 문서 QA, 블로그, 개인 RAG | 수백만 벡터 이상 처리, 실시간 검색 |\n\n---\n\n## 🤖 덕산님이 금강에 적용한다면?\n\n| 상황 | 추천 VectorStore |\n|------|------------------|\n| 로컬, 빠른 구축, 메모리 기능 활용 | ✅ **Chroma** |\n| GPU 서버 + 대용량 벡터 처리 필요 | ✅ **FAISS** |\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Vector Store란?]]\n- [[Embedding이란?]]\n- [[Retriever란?]]\n- [[LangChain이란?]]\n````\n\n---\n\n💡 **정리하자면**:  \n- Chroma는 \"쉽고 빠르게 시작\"  \n- FAISS는 \"무겁고 빠르게 운영\"  \n\n금강처럼 로컬 중심 + 기억 저장이 중요한 시스템에는 Chroma가 훨씬 적합해요.\n\n필요하시면 **Weaviate, Pinecone, Qdrant** 등 다른 VectorStore와도 비교해드릴게요.", "ts": 1754469026156.149, "meta": {}}
{"type": "turn", "turn": 13, "role": "user", "text": "Vector Store란?", "ts": 1754469180729, "meta": {}}
{"type": "turn", "turn": 14, "role": "assistant", "text": "좋습니다 덕산님!  \n이제 모든 RAG 시스템의 중심이라고 할 수 있는 **Vector Store(벡터 저장소)**의 개념을  \n**옵시디언에 붙여넣을 수 있도록** 마크다운 박스 형식으로 정리해드릴게요.\n\n---\n\n````markdown\n# 🧠 Vector Store란?\n\n## ✅ 정의\n\n**Vector Store**란 텍스트(문서)를 숫자 벡터로 변환(Embedding)한 후  \n이 벡터들을 저장하고, **질문에 대해 유사한 벡터를 빠르게 검색**할 수 있게 해주는 **벡터 데이터베이스**입니다.\n\n---\n\n## 🧩 왜 필요한가?\n\nLLM은 자체적으로 모든 정보를 기억하지 않음  \n→ 따라서 RAG 구조에서는 필요한 정보를 외부에서 가져와야 함\n\n→ 이때, **텍스트 → 벡터**로 변환해 저장하고  \n**질문도 벡터로 변환하여 유사도 기반으로 검색**하는 방식이 가장 효과적임\n\n---\n\n## 🔄 동작 흐름 요약\n\n```\n[문서] → [Embedding] → [Vector Store 저장]\n                                ↑\n              [질문] → [Embedding] → [유사 벡터 검색]\n```\n\n→ 결과적으로 **질문과 가장 유사한 문서 조각**을 찾아 LLM에게 전달\n\n---\n\n## ⚙️ 구성 요소\n\n| 구성 요소 | 설명 |\n|-----------|------|\n| **Embedding Model** | 텍스트를 벡터로 변환 (예: OpenAIEmbeddings) |\n| **Vector Store** | 벡터들을 저장하고 유사도 기반으로 검색 (예: Chroma, FAISS 등) |\n| **Retriever** | 질문 벡터를 기반으로 관련 문서를 Vector Store에서 검색 |\n| **Document** | 실제 원문 텍스트 및 메타데이터 포함 객체\n\n---\n\n## 🛠️ 예시 코드 (Chroma)\n\n```python\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 1. 임베딩 모델\nembedding = OpenAIEmbeddings()\n\n# 2. 벡터 저장소 생성\nvectorstore = Chroma.from_texts(\n    texts=[\"금강은 스스로 진화하는 AI입니다.\"],\n    embedding=embedding,\n    persist_directory=\"./db\"\n)\n\n# 3. 검색\nretriever = vectorstore.as_retriever()\ndocs = retriever.get_relevant_documents(\"금강은 어떤 존재야?\")\n```\n\n---\n\n## 🔍 유사도 계산 방식\n\n일반적으로 **Cosine Similarity (코사인 유사도)**를 사용  \n→ 두 벡터 간의 각도가 작을수록 의미적으로 더 유사함\n\n---\n\n## 🔧 주요 Vector Store 종류\n\n| 이름 | 특징 |\n|------|------|\n| **Chroma** | 가장 간단하고 LangChain에 최적화됨 |\n| **FAISS** | Facebook 개발, 대규모 고성능 검색 |\n| **Qdrant** | 러스트 기반, 빠르고 RESTful API 지원 |\n| **Pinecone** | 클라우드 기반, 실시간 검색용 |\n| **Weaviate** | 메타데이터 검색 및 GraphQL API 지원 |\n\n---\n\n## ✅ Vector Store가 하는 일 정리\n\n- Embedding 벡터 저장\n- 유사도 기반 검색\n- 메타데이터 저장 (문서 ID, 카테고리 등)\n- Retriever 인터페이스 제공 (LangChain 연동)\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Embedding이란?]]\n- [[Retriever란?]]\n- [[RAG란?]]\n- [[Chroma와 FAISS의 차이점]]\n````\n\n---\n\n필요하시면 각 Vector Store별로 **벤치마크 결과**, **속도 비교**,  \n또는 금강 프로젝트에 어떤 게 가장 어울리는지도 추가로 정리해드릴게요!", "ts": 1754469181346.109, "meta": {}}
{"type": "turn", "turn": 15, "role": "user", "text": "DocumentLoader는 어떤 종류가 있는가?", "ts": 1754469309218, "meta": {}}
{"type": "turn", "turn": 16, "role": "assistant", "text": "좋은 질문이십니다, 덕산님.  \nLangChain에서 `DocumentLoader`는 **외부 문서(텍스트, PDF, 웹, 데이터베이스 등)를 불러와 `Document` 객체로 변환**하는 역할을 합니다.\n\n아래는 옵시디언에 바로 붙여넣을 수 있는 **마크다운 정리**입니다.\n\n---\n\n````markdown\n# 📂 DocumentLoader란?  \n> 외부 문서 → LangChain의 `Document` 객체로 불러오는 도구\n\n## ✅ 핵심 역할\n문서 파일, 웹 페이지, 데이터베이스 등 **다양한 소스의 텍스트를 읽어들여**,  \nLangChain에서 사용할 수 있도록 표준 형태(`Document`)로 변환합니다.\n\n---\n\n## 🧩 Document 객체란?\n\n```python\nfrom langchain.schema import Document\n\nDocument(\n  page_content=\"텍스트 내용\",\n  metadata={\"source\": \"file.pdf\", \"page\": 3}\n)\n```\n\n→ LLM에 전달 가능한 기본 단위  \n→ Vector Store에 저장하기 위한 최소 단위\n\n---\n\n## 📦 주요 DocumentLoader 종류\n\n| 유형 | Loader 이름 | 설명 |\n|------|-------------|------|\n| **텍스트 파일** | `TextLoader` | `.txt`, `.md` 등 일반 텍스트 파일 |\n| **PDF 파일** | `PyPDFLoader`, `PDFPlumberLoader`, `UnstructuredPDFLoader` | 다양한 PDF 파서 지원 |\n| **워드 파일** | `UnstructuredWordLoader`, `Docx2txtLoader` | `.doc`, `.docx` |\n| **엑셀 파일** | `UnstructuredExcelLoader`, `CSVLoader` | `.csv`, `.xlsx` |\n| **HTML / 웹** | `WebBaseLoader`, `BSHTMLLoader`, `SitemapLoader` | 웹 페이지 불러오기 |\n| **폴더 로딩** | `DirectoryLoader` | 디렉토리 내 여러 파일 일괄 로드 |\n| **Google Docs** | `GDocsLoader` | 구글 문서 로딩 (OAuth 필요) |\n| **Notion** | `NotionDBLoader` | Notion DB 불러오기 |\n| **Github** | `GitHubLoader` | 리포지토리 파일 불러오기 |\n| **YouTube** | `YoutubeLoader` | 영상 자막 불러오기 |\n| **웹 검색** | `SerpAPILoader`, `GoogleSearchLoader` | 실시간 검색 결과 |\n| **PostgreSQL** | `SQLDatabaseLoader` | SQL 쿼리 결과 로딩 |\n| **JSON** | `JSONLoader`, `JSONLinesLoader` | 구조화된 JSON 로딩 |\n| **Unstructured** | `UnstructuredFileLoader` | 다양한 포맷 자동 분해 가능 (PDF, Word, HTML 등 다형 지원) |\n\n---\n\n## 📁 예시: PDF 파일 불러오기\n\n```python\nfrom langchain_community.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader(\"report.pdf\")\ndocuments = loader.load()\n```\n\n---\n\n## 📁 예시: 폴더 내 모든 `.txt` 파일 불러오기\n\n```python\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain_community.document_loaders import TextLoader\n\nloader = DirectoryLoader(\n    path=\"./data\",\n    glob=\"**/*.txt\",\n    loader_cls=TextLoader\n)\ndocuments = loader.load()\n```\n\n---\n\n## 🧠 참고: Unstructured 기반 로더\n\nUnstructured.io 오픈소스 기반 로더는  \nWord, Excel, PDF, HTML 등 다양한 포맷을 구조 분석해 문단/제목/표 등을 구분합니다.\n\n→ 설치 필요: `pip install unstructured`\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Document란?]]\n- [[Text Splitter란?]]\n- [[Embedding이란?]]\n- [[Vector Store란?]]\n````\n\n---\n\n💡 덕산님이 만약 금강에 다큐먼트 자동 인게스트 기능을 붙이신다면,  \n`DirectoryLoader` + `Unstructured` 조합이 굉장히 강력하고 범용성이 높습니다.\n\n원하시면 특정 포맷(예: Markdown, PDF, JSON)만 따로 깊게 정리해드릴 수도 있습니다!", "ts": 1754469310341.812, "meta": {}}
{"type": "turn", "turn": 17, "role": "user", "text": "JSONLoader는 어떻게 사용하는가?", "ts": 1754469508466, "meta": {}}
{"type": "turn", "turn": 18, "role": "assistant", "text": "좋습니다, 덕산님!  \n`JSONLoader`는 LangChain에서 **구조화된 JSON 데이터를 문서(Document)로 변환**하기 위한 Loader입니다.  \n특히 로그, 설정, 데이터 저장 포맷으로 널리 쓰이는 JSON을 RAG 시스템에 넣고 싶을 때 유용합니다.\n\n아래는 옵시디언에 바로 붙여넣을 수 있는 **마크다운 정리**입니다.\n\n---\n\n````markdown\n# 🧾 JSONLoader는 어떻게 사용하는가?\n\n## ✅ 정의\n`JSONLoader`는 `.json` 파일에서 **원하는 키의 값을 추출**하여 LangChain의 `Document` 객체로 변환해주는 도구입니다.\n\n---\n\n## 📁 예제 JSON 파일 구조\n\n```json\n[\n  {\n    \"title\": \"금강의 구조\",\n    \"content\": \"금강은 LangGraph와 LangChain으로 구성됩니다.\",\n    \"date\": \"2025-07-30\"\n  },\n  {\n    \"title\": \"기억 시스템\",\n    \"content\": \"금강은 대화 로그를 자동 저장하고 인게스트합니다.\",\n    \"date\": \"2025-07-31\"\n  }\n]\n```\n\n---\n\n## 🛠️ 기본 사용법\n\n```python\nfrom langchain_community.document_loaders import JSONLoader\n\nloader = JSONLoader(\n    file_path=\"data/memory.json\",\n    jq_schema=\".[]\",                   # 루트 배열 순회\n    text_content=False,               # 자동 텍스트 결합 여부 (False면 dict로 유지)\n    json_lines=False,                 # jsonl 형식이면 True\n    content_key=\"content\"             # (v1) content 키만 추출하여 Document로 만들 경우\n)\n\ndocuments = loader.load()\n```\n\n---\n\n## 🔍 주요 파라미터 설명\n\n| 파라미터 | 설명 |\n|----------|------|\n| `file_path` | 불러올 JSON 파일 경로 |\n| `jq_schema` | `jq` 스타일 경로로 원하는 부분 추출 (예: `.[]`, `.articles[]`) |\n| `content_key` | (구버전) `content` 키만 문서 내용으로 사용 |\n| `text_content` | True이면 전체 항목을 문자열로 합침 |\n| `json_lines` | JSONL (줄마다 JSON)일 경우 True 설정 |\n\n---\n\n## 💡 예시 1: 배열의 각 항목을 문서로\n\n```python\nloader = JSONLoader(\n    file_path=\"docs.json\",\n    jq_schema=\".[]\", \n    text_content=True     # 하나의 텍스트로 결합\n)\n```\n\n→ 결과:\n\n```python\n[\n  Document(page_content=\"금강은 LangGraph와 LangChain으로 구성됩니다.\", metadata={...}),\n  Document(page_content=\"금강은 대화 로그를 자동 저장하고 인게스트합니다.\", metadata={...}),\n]\n```\n\n---\n\n## 💡 예시 2: 특정 키(`content`)만 추출하여 사용\n\n```python\nloader = JSONLoader(\n    file_path=\"docs.json\",\n    jq_schema=\".[]\",\n    content_key=\"content\"\n)\n```\n\n---\n\n## ⚠️ 주의: 복잡한 JSON 구조\n\n- 중첩된 구조일 경우 `jq_schema`를 정교하게 설정해야 함\n- 예: `.articles[].body`, `.data[].message`\n\n→ 복잡할 경우 `jq` 구문 연습 필요!\n\n---\n\n## 🧠 참고: JSONLines (jsonl) 형식\n\n```json\n{\"content\": \"첫 번째 문장\"}\n{\"content\": \"두 번째 문장\"}\n```\n\n```python\nloader = JSONLoader(\n    file_path=\"data.jsonl\",\n    jq_schema=\".\", \n    json_lines=True,\n    content_key=\"content\"\n)\n```\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Document란?]]\n- [[DocumentLoader 종류]]\n- [[Text Splitter란?]]\n- [[Embedding이란?]]\n- [[Vector Store란?]]\n````\n\n---\n\n필요하시면 `json` 파일을 직접 분석해서 어떤 `jq_schema`를 쓰면 되는지 알려드릴 수도 있습니다!  \n혹시 금강 프로젝트에서 사용하는 json 로그가 있다면 예시 기반으로 커스터마이징도 가능해요.", "ts": 1754469509169.426, "meta": {}}
{"type": "turn", "turn": 19, "role": "user", "text": "랭체인에서 Document란?", "ts": 1754469660995, "meta": {}}
{"type": "turn", "turn": 20, "role": "assistant", "text": "덕산님, 정말 핵심 개념을 짚으셨어요.  \n`Document`는 LangChain의 모든 검색·요약·기억 시스템의 **기본 단위**입니다.\n\n다음은 옵시디언에 붙여 넣을 수 있도록 구성한 **마크다운 박스**입니다.\n\n---\n\n````markdown\n# 📄 LangChain에서 `Document`란?\n\n## ✅ 정의  \n`Document`는 LangChain에서 **텍스트와 해당 텍스트의 메타데이터**를 함께 담는 기본 객체입니다.  \n→ 이 객체가 **Vector Store에 저장되고**, **Retriever에 의해 검색되며**,  \n→ **LLM에 전달되어 답변 생성에 사용**됩니다.\n\n---\n\n## 🧱 구조\n\n```python\nfrom langchain.schema import Document\n\nDocument(\n    page_content=\"실제 텍스트 내용\",\n    metadata={\n        \"source\": \"file.txt\",\n        \"page\": 1,\n        \"topic\": \"금강 기억 시스템\"\n    }\n)\n```\n\n| 필드 | 설명 |\n|------|------|\n| `page_content` | 문서의 본문 텍스트 (LLM에게 보여질 내용) |\n| `metadata` | 해당 텍스트에 대한 부가 정보 (파일 이름, 날짜, 태그 등) |\n\n---\n\n## 🧠 왜 중요한가?\n\n| 역할 | 설명 |\n|------|------|\n| ✅ 검색 대상 | RAG에서 유사한 문서를 찾기 위한 단위 |\n| ✅ 분할 단위 | Splitter로 쪼개질 최소 단위 |\n| ✅ LLM 입력 | LLM에게 전달되는 컨텍스트 단위 |\n| ✅ 메타정보 활용 | 출처, 날짜, 작성자 등 정보 기반 필터링 가능\n\n---\n\n## 📁 생성 예시\n\n### 📌 일반 텍스트\n```python\nDocument(\n  page_content=\"금강은 기억과 자기 수정을 지원합니다.\",\n  metadata={\"source\": \"intro.md\"}\n)\n```\n\n### 📌 JSON에서 로딩\n```python\n[\n  Document(\n    page_content=\"LangChain은 LLM 애플리케이션을 위한 프레임워크입니다.\",\n    metadata={\"source\": \"langchain_docs.json\", \"index\": 0}\n  ),\n  ...\n]\n```\n\n---\n\n## 🔄 전체 흐름에서 위치\n\n```\n📂 파일 로딩 → Document로 변환 → 텍스트 분할 → 임베딩 → Vector Store 저장 → Retriever로 검색 → LLM에 전달\n```\n\n---\n\n## 🧰 Document 관련 도구\n\n| 기능 | 사용 객체 |\n|------|-----------|\n| 문서 불러오기 | `DocumentLoader` |\n| 문서 분할 | `TextSplitter` |\n| 문서 검색 | `Retriever` |\n| 문서 저장 | `Vector Store` |\n| 문서 요약 | `summarization chain` 등 |\n\n---\n\n## 📎 관련 개념 링크\n\n- [[DocumentLoader란?]]\n- [[Text Splitter란?]]\n- [[Embedding이란?]]\n- [[Retriever란?]]\n- [[Vector Store란?]]\n````\n\n---\n\n💡 덧붙이자면, `Document`는 단순한 문자열이 아니라  \n**\"문맥 + 출처 정보\"를 가진 텍스트 단위**로써,  \nRAG의 신뢰성과 정확도를 높이는 핵심 개념입니다.\n\n원하시면 `Document`를 직접 만들고 저장하는 코드도 같이 정리해드릴게요!", "ts": 1754469661323.873, "meta": {}}
{"type": "turn", "turn": 21, "role": "user", "text": "랭체인에서", "ts": 1754469779027, "meta": {}}
{"type": "turn", "turn": 22, "role": "assistant", "text": "덕산님, 메시지가 중간에 끊긴 것 같아요.  \n혹시 \"랭체인에서...\" 뒤에 이어질 내용을 다시 한번 말씀해 주실 수 있을까요?\n\n예를 들어:\n\n- **\"랭체인에서 PromptTemplate의 역할은?\"**\n- **\"랭체인에서 Memory가 어떻게 작동하나?\"**\n- **\"랭체인에서 Tool은 무엇인가?\"**\n\n등 구체적인 질문을 주시면, 옵시디언 마크다운 형식으로 정리해드릴게요.  \n언제든지 이어서 질문해주세요! 😊", "ts": 1754469780011.159, "meta": {}}
{"type": "turn", "turn": 23, "role": "user", "text": "랭체인에서 Embedding이란?", "ts": 1754469798290, "meta": {}}
{"type": "turn", "turn": 24, "role": "assistant", "text": "덕산님, 이번 질문도 RAG와 LangChain을 이해하는 데 **핵심 중의 핵심**입니다.  \n바로 정리해 드릴게요. 아래는 옵시디언에 바로 붙여 넣을 수 있는 **마크다운 박스**입니다.\n\n---\n\n````markdown\n# 🧠 Embedding이란? (in LangChain)\n\n## ✅ 정의\n\n**Embedding**이란 텍스트(문장, 문서 등)를  \n**고차원 수치 벡터(Vector)**로 변환하여,  \n의미적 유사도 계산이 가능하도록 만드는 기술입니다.\n\n> 예: \"강아지\"와 \"개\"는 **의미가 유사**하므로 벡터 간 거리가 가깝다.\n\n---\n\n## 📌 왜 필요한가?\n\nLLM(RAG 포함)은 문서 전체를 기억하지 못함.  \n→ 따라서 **질문과 의미적으로 유사한 문서를 검색**하기 위해  \n→ 모든 문서와 질문을 **벡터 공간에 위치시켜 비교**해야 함.\n\n→ 이때 사용하는 것이 바로 **Embedding**!\n\n---\n\n## 🧩 Embedding의 역할 흐름\n\n```\n[문서 텍스트] → Embedding → Vector Store 저장\n       ↑\n[질문 텍스트] → Embedding → 유사 벡터 검색\n```\n\n→ 결과적으로 **\"의미적으로 가까운\" 문서를 찾아냄**\n\n---\n\n## 🧰 LangChain에서 Embedding 사용 예시\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n\nembedding_model = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\"\n)\n\n# 텍스트 → 벡터 변환\nvector = embedding_model.embed_query(\"금강의 기억 시스템은?\")\n```\n\n---\n\n## ⚙️ LangChain에서 사용 가능한 주요 Embedding 클래스\n\n| 클래스 | 설명 |\n|--------|------|\n| `OpenAIEmbeddings` | OpenAI의 강력한 API 기반 임베딩 |\n| `HuggingFaceEmbeddings` | 로컬에서 실행 가능한 임베딩 모델 |\n| `CohereEmbeddings` | 다양한 언어 지원, 클라우드 기반 |\n| `TensorflowHubEmbeddings` | TF 기반 모델 사용 가능 |\n| `BGEEmbeddings` | RAG 특화 모델, 매우 우수한 성능 |\n\n---\n\n## 💡 Embedding의 핵심 특징\n\n| 특징 | 설명 |\n|------|------|\n| 📏 벡터 차원 | 보통 384 ~ 1536 차원 |\n| 🧭 비교 방식 | 코사인 유사도(Cosine Similarity) |\n| 🌐 언어 지원 | 다국어/한글 지원 모델도 있음 |\n| ⚠️ 중요 | 같은 모델로 질문과 문서를 **동일하게** 변환해야 함!\n\n---\n\n## 🔍 Embedding vs Token\n\n| 구분 | Embedding | Token |\n|------|-----------|-------|\n| 목적 | 의미 비교 | 텍스트 분할 |\n| 출력 | 벡터(리스트) | 토큰 ID (숫자 배열) |\n| 관련 | Vector Store | LLM 입력 제한 |\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Vector Store란?]]\n- [[Retriever란?]]\n- [[RAG란?]]\n- [[Embedding 모델은 어떻게 선택하는가?]]\n- [[Text Splitter란?]]\n````\n\n---\n\n혹시 덕산님의 `금강 프로젝트`에서 직접 적용 중인 임베딩 설정이 있다면,  \n그 코드 흐름을 기준으로 설명드릴 수도 있어요.\n\n필요하시면 Embedding 벡터를 **시각화하는 도구(TSNE/UMAP)**나  \n**비교 실험 방법**도 이어서 정리해드릴게요.", "ts": 1754469798704.885, "meta": {}}
{"type": "turn", "turn": 25, "role": "user", "text": "랭체인에서 사용자의 질문 자체를 임베딩 하는 이유와 문서의 임베딩과 질문의 임베딩 모델이 다르면 생기는 문제는?", "ts": 1754470004230, "meta": {}}
{"type": "turn", "turn": 26, "role": "assistant", "text": "덕산님, 질문이 정말 날카롭습니다.  \n이건 RAG 시스템의 정확도와 신뢰성에 **직결되는 개념**이에요.\n\n아래에 옵시디언에 바로 붙여넣을 수 있도록, **마크다운 박스**에 정리해드립니다.\n\n---\n\n````markdown\n# ❓ 왜 사용자의 질문도 임베딩하는가?\n> 문서만 임베딩하면 안 되나?\n\n## ✅ 핵심 이유: \"의미 기반 검색\"을 위해\n\nVector Store는 단순 키워드 검색이 아닌  \n**의미(semantic)를 기준으로 유사 문서를 찾기 위해** 존재합니다.\n\n이를 위해:\n\n- 문서: embedding → 벡터 공간에 저장  \n- 질문: embedding → 같은 벡터 공간에서 위치 계산  \n- → **가장 가까운 문서 벡터를 유사 문서로 간주**\n\n---\n\n## 🔄 작동 흐름 요약\n\n```\n[문서들] → Embedding → Vector Store 저장\n                        ↑\n       [사용자 질문] → Embedding → 유사도 계산\n```\n\n→ 따라서 **사용자의 질문도 반드시 임베딩되어야** 검색이 가능합니다.\n\n---\n\n# ⚠️ 문서와 질문의 Embedding 모델이 다르면?\n\n## 🧨 문제 발생: 벡터 공간이 서로 다름\n\n| 항목 | 설명 |\n|------|------|\n| 문서 임베딩 모델 | A 모델의 의미 공간 |\n| 질문 임베딩 모델 | B 모델의 의미 공간 |\n| 결과 | 서로 **비교 불가하거나 정확도 급감** |\n\n예를 들어:\n\n- 문서는 OpenAI의 `text-embedding-3-small`  \n- 질문은 HuggingFace의 `MiniLM` 사용\n\n→ 두 벡터는 **완전히 다른 기준으로 구성된 수치값**  \n→ 코사인 유사도 계산 자체가 무의미하거나 왜곡됨\n\n---\n\n## 🎯 반드시 지켜야 할 원칙\n\n> **문서와 질문 모두 동일한 임베딩 모델로 처리해야 함**\n\n| 잘된 예 | 잘못된 예 |\n|---------|-----------|\n| 문서 + 질문 → 둘 다 `OpenAIEmbeddings` | 문서: `OpenAI`, 질문: `MiniLM` |\n| 문서 + 질문 → 둘 다 `bge-m3` | 문서: `text-embedding-ada-002`, 질문: `CohereEmbeddings` |\n\n---\n\n## 🧠 요약\n\n| 질문 | 답변 |\n|------|------|\n| 왜 질문도 임베딩하나요? | 의미 유사도 기반 검색을 위해 필요합니다 |\n| 모델이 다르면 어떤 문제가? | 서로 다른 벡터 공간이라 비교 자체가 왜곡되며 정확도 급감 |\n\n---\n\n## 📎 관련 개념 링크\n\n- [[Embedding이란?]]\n- [[Embedding 모델은 어떻게 선택하는가?]]\n- [[Vector Store란?]]\n- [[Retriever란?]]\n- [[RAG란?]]\n````\n\n---\n\n필요하시면 \"같은 모델이라도 버전이 다르면?\" 같은 심화 주제도 정리해드릴게요.  \n덕산님의 시스템이 지금 어떤 모델로 문서 임베딩하고 있는지도 알려주시면, 실제 코드 흐름 기준으로 점검해드릴 수도 있어요.", "ts": 1754470009680.729, "meta": {}}
